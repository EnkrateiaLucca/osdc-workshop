{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import Field, BaseModel, model_validator, ValidationInfo\n",
    "from typing import List\n",
    "import re\n",
    "from typing import List\n",
    "import re\n",
    "from pypdf import PdfReader\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fact(fact='Example fact', substring_quote=['some quote'])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Fact(BaseModel):\n",
    "    fact: str = Field(...)\n",
    "    substring_quote: List[str] = Field(...)\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def validate_sources(self) -> \"Fact\":\n",
    "        # Assuming text_chunk is provided during initialization\n",
    "        text_chunks = getattr(self, \"text_chunk\", None)\n",
    "        if text_chunks:\n",
    "            spans = list(self.get_spans(text_chunks))\n",
    "            self.substring_quote = [text_chunks[span[0]:span[1]] for span in spans]\n",
    "        return self\n",
    "\n",
    "    def get_spans(self, context):\n",
    "        for quote in self.substring_quote:\n",
    "            yield from self._get_span(quote, context)\n",
    "\n",
    "    def _get_span(self, quote, context):\n",
    "        for match in re.finditer(re.escape(quote), context):\n",
    "            yield match.span()\n",
    "\n",
    "class QuestionAnswer(BaseModel):\n",
    "    questions: List[str] = Field(..., description='A list of questions about the source content')\n",
    "    answers: List[Fact] = Field(..., description='A list of answers as Fact objects to answer each question')\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def validate_sources(self) -> \"QuestionAnswer\":\n",
    "        self.answers = [fact for fact in self.answers if len(fact.substring_quote) > 0]\n",
    "        return self\n",
    "\n",
    "# Example usage with text_chunk context passed in initialization\n",
    "fact_data = {\n",
    "    \"fact\": \"Example fact\",\n",
    "    \"substring_quote\": [\"some quote\"],\n",
    "    \"text_chunk\": \"This is some context that contains some quote.\"\n",
    "}\n",
    "fact = Fact(**fact_data)\n",
    "\n",
    "fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "\n",
    "MODEL = 'gpt-4o-mini'\n",
    "\n",
    "def load_pdf_text(file_path):\n",
    "    '''Loads text from a PDF file.'''\n",
    "    # importing required modules\n",
    "\n",
    "    # creating a pdf reader object\n",
    "    reader = PdfReader(file_path)\n",
    "\n",
    "    # extracting text from page\n",
    "    text = \"\\n\\n\".join([page.extract_text() for page in reader.pages])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simpler working version is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactExtractor(BaseModel):\n",
    "    facts: List[Fact] = Field(..., description=\"List with all the facts contained in the source text.\")\n",
    "\n",
    "def extract_facts(prompt_question):\n",
    "    response = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o-2024-08-06\",\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are a fact extraction engine.\"},\n",
    "                  {\"role\": \"user\", \"content\": prompt_question}],\n",
    "        response_format=FactExtractor\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.parsed\n",
    "\n",
    "\n",
    "def load_pdf_pages(file_path):\n",
    "    '''Loads text from a PDF file.'''\n",
    "    # importing required modules\n",
    "\n",
    "    # creating a pdf reader object\n",
    "    reader = PdfReader(file_path)\n",
    "\n",
    "    # extracting text from page\n",
    "    pages = [page.extract_text() for page in reader.pages]\n",
    "    \n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google grants permission to reproduce tables and figures from the paper for journalistic or scholarly works with proper attribution.\n",
      "['Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works.']\n",
      "\n",
      "\n",
      "The Transformer architecture is based solely on attention mechanisms without recurrence or convolutions.\n",
      "['We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.']\n",
      "\n",
      "\n",
      "The Transformer model achieved a BLEU score of 28.4 on the WMT 2014 English-to-German translation task.\n",
      "['Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task.']\n",
      "\n",
      "\n",
      "The Transformer model achieved a BLEU score of 41.8 on the WMT 2014 English-to-French translation task after training for 3.5 days on eight GPUs.\n",
      "['On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs.']\n",
      "\n",
      "\n",
      "The Transformer generalizes well to other tasks, including English constituency parsing with both large and limited training data.\n",
      "['We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.']\n",
      "\n",
      "\n",
      "Aidan N. Gomez worked on this paper while at the University of Toronto.\n",
      "['Aidan N. Gomez∗ † University of Toronto aidan@cs.toronto.edu', '†Work performed while at Google Brain.']\n",
      "\n",
      "\n",
      "Illia Polosukhin contributed while at Google Research.\n",
      "['‡Work performed while at Google Research.']\n",
      "\n",
      "\n",
      "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Łukasz Kaiser, and Illia Polosukhin contributed to the Transformer model.\n",
      "['Ashish Vaswani∗ Google Brain avaswani@google.com', 'Noam Shazeer∗ Google Brain noam@google.com', 'Niki Parmar∗ Google Research nikip@google.com', 'Jakob Uszkoreit∗ Google Research usz@google.com', 'Llion Jones∗ Google Research llion@google.com', 'Łukasz Kaiser∗ Google Brain lukaszkaiser@google.com', 'Illia Polosukhin∗ ‡ illia.polosukhin@gmail.com']\n",
      "\n",
      "\n",
      "Jakob Uszkoreit proposed replacing RNNs with self-attention.\n",
      "['Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea.']\n",
      "\n",
      "\n",
      "Noam Shazeer proposed scaled dot-product attention, multi-head attention, and parameter-free position representation.\n",
      "['Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation.']\n",
      "\n",
      "\n",
      "Niki Parmar designed and evaluated countless model variants in the original codebase and tensor2tensor.\n",
      "['Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor.']\n",
      "\n",
      "\n",
      "Llion Jones was responsible for the initial codebase, efficient inference, and visualizations.\n",
      "['Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations.']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = './assets-resources/sources/attention-paper.pdf'\n",
    "\n",
    "pages = load_pdf_pages(file_path)\n",
    "\n",
    "page1 = pages[0]\n",
    "\n",
    "facts = extract_facts(f'Extract the facts from the following text:\\n\\n{page1}')\n",
    "\n",
    "for fact in facts.facts:\n",
    "    print(fact.fact)\n",
    "    print(fact.substring_quote)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disclaimer: THIS MIGHT BE A SILY ILLOGICAL THING TO DO....\n",
    "paper_facts = []\n",
    "for page in pages:\n",
    "    paper_facts.append(extract_facts(f'Extract the facts from the following text:\\n\\n{page}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Fact(fact='Google grants permission to reproduce tables and figures in the paper for journalistic or scholarly works provided proper attribution is given.', substring_quote=['Google hereby grants permission', 'to reproduce the tables and figures in this paper', 'solely for use in journalistic or scholarly works.', 'Provided proper attribution is provided']),\n",
       " Fact(fact=\"The paper 'Attention Is All You Need' is associated with several Google Research contributors including Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, and Łukasz Kaiser.\", substring_quote=['Attention Is All You Need', 'Ashish Vaswani', 'Noam Shazeer', 'Niki Parmar', 'Jakob Uszkoreit', 'Llion Jones', 'Łukasz Kaiser']),\n",
       " Fact(fact='Aidan N. Gomez from the University of Toronto contributed to the paper.', substring_quote=['Aidan N. Gomez', 'University of Toronto']),\n",
       " Fact(fact='Illia Polosukhin also contributed to the paper.', substring_quote=['Illia Polosukhin']),\n",
       " Fact(fact='The Transformer network architecture is proposed, which relies solely on attention mechanisms and omits recurrence and convolutions.', substring_quote=['We propose a new simple network architecture, the Transformer', 'based solely on attention mechanisms', 'dispensing with recurrence and convolutions entirely.']),\n",
       " Fact(fact='The Transformer model achieves a BLEU score of 28.4 on the WMT 2014 English-to-German translation task.', substring_quote=['Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task']),\n",
       " Fact(fact='On the WMT 2014 English-to-French translation task, the Transformer model achieves a BLEU score of 41.8 after training for 3.5 days on eight GPUs.', substring_quote=['On the WMT 2014 English-to-French translation task', 'our model establishes a new single-model state-of-the-art BLEU score of 41.8', 'training for 3.5 days on eight GPUs']),\n",
       " Fact(fact='The Transformer architecture generalizes well to other tasks such as English constituency parsing.', substring_quote=['We show that the Transformer generalizes well to other tasks', 'applying it successfully to English constituency parsing']),\n",
       " Fact(fact='The contribution of the team members is notable, with Jakob proposing replacing RNNs with self-attention, Noam proposing scaled dot-product attention, and others responsible for implementation and experimental variations.', substring_quote=['Jakob proposed replacing RNNs with self-attention', 'Noam proposed scaled dot-product attention, multi-head attention', 'Ashish, with Illia, designed and implemented the first Transformer models', 'Niki designed, implemented, tuned and evaluated countless model variants', 'Llion also experimented with novel model variants', 'Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor']),\n",
       " Fact(fact='The work was presented at the 31st Conference on Neural Information Processing Systems (NIPS 2017) in Long Beach, CA, USA.', substring_quote=['31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA'])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_facts[0].facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "class RelevantFact(BaseModel):\n",
    "    relevancy_score: Literal['yes', 'no'] = Field(description=\"A binary score yes|no if a specific statement/fact is relevant given an objective.\")\n",
    "    justification: str = Field(description=\"A short one sentence justification for the relevancy score.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SYS_MSG_RELEVANT_FACT = \"\"\"\n",
    "Given an objective from the user you inspect if a given fact is relevant to that objective and output:\n",
    "- relevancy_score: yes/no.\n",
    "- justification: one sentence justification for the relevancy score.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_it_relevant(objective, fact):\n",
    "    response = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o-2024-08-06\",\n",
    "        messages=[{\"role\": \"system\", \"content\": SYS_MSG_RELEVANT_FACT},\n",
    "                  {\"role\": \"user\", \"content\": f\"{objective}\\n\\n{fact.fact}\"}],\n",
    "        response_format=RelevantFact\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is not relevant:  fact='Google grants permission to reproduce tables and figures in the paper for journalistic or scholarly works provided proper attribution is given.' substring_quote=['Google hereby grants permission', 'to reproduce the tables and figures in this paper', 'solely for use in journalistic or scholarly works.', 'Provided proper attribution is provided']\n",
      "This is not relevant:  fact='Illia Polosukhin also contributed to the paper.' substring_quote=['Illia Polosukhin']\n",
      "This is not relevant:  fact='The Transformer model achieves a BLEU score of 28.4 on the WMT 2014 English-to-German translation task.' substring_quote=['Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task']\n",
      "This is not relevant:  fact='On the WMT 2014 English-to-French translation task, the Transformer model achieves a BLEU score of 41.8 after training for 3.5 days on eight GPUs.' substring_quote=['On the WMT 2014 English-to-French translation task', 'our model establishes a new single-model state-of-the-art BLEU score of 41.8', 'training for 3.5 days on eight GPUs']\n",
      "This is not relevant:  fact='Recurrent neural networks, long short-term memory, and gated recurrent neural networks are state-of-the-art approaches in sequence modeling and transduction problems such as language modeling and machine translation.' substring_quote=['Recurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks', 'state of the art approaches', 'sequence modeling and transduction problems', 'language modeling and machine translation [ 35,2,5]']\n",
      "This is not relevant:  fact='The Transformer can reach a new state of the art in translation quality after being trained for twelve hours on eight P100 GPUs.' substring_quote=['reach a new state of the art in translation quality', 'twelve hours on eight P100 GPUs']\n",
      "This is not relevant:  fact=\"Residual connection is employed around each of the encoder's sub-layers, followed by layer normalization.\" substring_quote=['We employ a residual connection [ 11] around each of the two sub-layers, followed by layer normalization']\n",
      "This is not relevant:  fact='All sub-layers and embedding layers produce outputs of dimension 512.' substring_quote=['all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512']\n",
      "This is not relevant:  fact='The decoder is composed of a stack of 6 identical layers.' substring_quote=['Decoder: The decoder is also composed of a stack of N= 6 identical layers']\n",
      "This is not relevant:  fact='Each layer of the encoder and decoder contains a fully connected feed-forward network.' substring_quote=['In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically.']\n",
      "This is not relevant:  fact='The dimensionality of input and output is 512, and the inner-layer has a dimensionality of 2048.' substring_quote=['The dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality dff= 2048 .']\n",
      "This is not relevant:  fact='The model uses learned embeddings to convert input and output tokens to vectors of dimension 512.' substring_quote=['Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel.']\n",
      "This is not relevant:  fact='The same weight matrix is shared between the two embedding layers and the pre-softmax linear transformation.' substring_quote=['In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [ 30].']\n",
      "This is not relevant:  fact='Self-attention has a sequential operations complexity of O(1).' substring_quote=['Self-Attention O(1)']\n",
      "This is not relevant:  fact='Recurrent layer has a per-layer complexity of O(n·d²).' substring_quote=['Recurrent O(n·d2)']\n",
      "This is not relevant:  fact='Maximum path length for recurrent layers is O(n).' substring_quote=['Recurrent O(n)']\n",
      "This is not relevant:  fact='Per-layer complexity for convolutional layers is O(k·n·d²).' substring_quote=['Convolutional O(k·n·d2)']\n",
      "This is not relevant:  fact='Convolutional layers have a sequential operations complexity of O(1).' substring_quote=['Convolutional O(1)']\n",
      "This is not relevant:  fact='Maximum path length for convolutional layers is O(logk(n)).' substring_quote=['Convolutional O(logk(n))']\n",
      "This is not relevant:  fact='Word-piece and byte-pair representations are used as sentence representations in state-of-the-art machine translation models.' substring_quote=['word-piece [38] and byte-pair [31] representations']\n",
      "This is not relevant:  fact='A single convolutional layer with kernel width k < n does not connect all pairs of input and output positions.' substring_quote=['A single convolutional layer with kernel width k < n does not connect all pairs of input and output positions']\n",
      "This is not relevant:  fact=\"A stack of O(n/k) convolutional layers is required for a single convolutional layer's connectivity in the case of contiguous kernels.\" substring_quote=['requires a stack of O(n/k) convolutional layers in the case of contiguous kernels']\n",
      "This is not relevant:  fact='Convolutional layers are generally more expensive than recurrent layers by a factor of k.' substring_quote=['Convolutional layers are generally more expensive than recurrent layers, by a factor of k']\n",
      "This is not relevant:  fact='Separable convolutions decrease the complexity to O(k·n·d + n·d²).' substring_quote=['Separable convolutions [6], however, decrease the complexity considerably, to O(k·n·d+n·d2)']\n",
      "This is not relevant:  fact='The WMT 2014 English-German dataset consists of about 4.5 million sentence pairs.' substring_quote=['WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs']\n",
      "This is not relevant:  fact='Sentence pairs in the English-German dataset were encoded using byte-pair encoding, creating a vocabulary of about 37,000 tokens.' substring_quote=['Sentences were encoded using byte-pair encoding', 'shared source-target vocabulary of about 37000 tokens']\n",
      "This is not relevant:  fact='The WMT 2014 English-French dataset consists of 36 million sentences with a 32,000 word-piece vocabulary.' substring_quote=['WMT 2014 English-French dataset consisting of 36M sentences', 'split tokens into a 32000 word-piece vocabulary']\n",
      "This is not relevant:  fact='Training batches included approximately 25,000 source tokens and 25,000 target tokens.' substring_quote=['Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens']\n",
      "This is not relevant:  fact='Models were trained on one machine with 8 NVIDIA P100 GPUs.' substring_quote=['We trained our models on one machine with 8 NVIDIA P100 GPUs']\n",
      "This is not relevant:  fact='Base models took about 0.4 seconds per training step and were trained for 100,000 steps or 12 hours.' substring_quote=['each training step took about 0.4 seconds', 'trained the base models for a total of 100,000 steps or 12 hours']\n",
      "This is not relevant:  fact='Big models took 1.0 seconds per step, training for 300,000 steps or 3.5 days.' substring_quote=['For our big models, step time was 1.0 seconds', 'big models were trained for 300,000 steps (3.5 days)']\n",
      "This is not relevant:  fact='The Adam optimizer was used with β1=0.9, β2=0.98, and ϵ=10⁻⁹.' substring_quote=['Adam optimizer [20] with β1= 0.9,β2= 0.98andϵ= 10−9']\n",
      "This is not relevant:  fact='Learning rate was increased linearly for the first 4000 training steps and then decreased with the inverse square root of the step number.' substring_quote=['increasing the learning rate linearly for the first warmup _steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number', 'warmup _steps = 4000']\n",
      "This is not relevant:  fact='The Transformer achieves better BLEU scores than previous models on the English-to-German and English-to-French newstest2014 tests.' substring_quote=['The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests']\n",
      "This is not relevant:  fact='The BLEU score for the Transformer (big) model on the WMT 2014 English-to-German task is 28.4.' substring_quote=['On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4.']\n",
      "This is not relevant:  fact='The BLEU score for the Transformer (big) model on the WMT 2014 English-to-French task is 41.0.' substring_quote=['On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0']\n",
      "This is not relevant:  fact='The training of the Transformer (big) model on the WMT 2014 English-to-German task took 3.5 days on 8 P100 GPUs.' substring_quote=['Training took 3.5days on 8P100 GPUs']\n",
      "This is not relevant:  fact='The dropout rate used for the Transformer (big) model trained on English-to-French was 0.1.' substring_quote=['The Transformer (big) model trained for English-to-French used dropout rate Pdrop= 0.1, instead of 0.3']\n",
      "This is not relevant:  fact='The training cost for the Transformer (base model) is 3.3·10^18 FLOPs for the English-to-German task.' substring_quote=['Transformer (base model) 27.3 38.1 3.3·1018']\n",
      "This is not relevant:  fact='The training cost for the Transformer (big model) is 2.3·10^19 FLOPs for the English-to-German task.' substring_quote=['Transformer (big) 28.4 41.8 2.3·1019']\n",
      "This is not relevant:  fact='For the base models, a single model was obtained by averaging the last 5 checkpoints written at 10-minute intervals.' substring_quote=['For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals.']\n",
      "This is not relevant:  fact='For the big models, the last 20 checkpoints were averaged.' substring_quote=['For the big models, we averaged the last 20 checkpoints.']\n",
      "This is not relevant:  fact='Beam search with a beam size of 4 and length penalty α= 0.6 was used.' substring_quote=['We used beam search with a beam size of 4and length penalty α= 0.6']\n",
      "This is not relevant:  fact='The maximum output length during inference was set to input length + 50 but would terminate early when possible.' substring_quote=['We set the maximum output length during inference to input length + 50, but terminate early when possible']\n",
      "This is not relevant:  fact='All metrics mentioned are on the English-to-German translation development set, newstest2013.' substring_quote=['All metrics are on the English-to-German translation development set, newstest2013.']\n",
      "This is not relevant:  fact='Perplexities listed are per-wordpiece according to byte-pair encoding.' substring_quote=['Listed perplexities are per-wordpiece, according to our byte-pair encoding.']\n",
      "This is not relevant:  fact='Base Transformer model has 6 layers, model size of 512, ff size of 2048, 8 heads, key and value size of 64, dropout of 0.1, trained for 100K steps, resulting in 4.92 PPL and 25.8 BLEU.' substring_quote=['base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8']\n",
      "This is not relevant:  fact='Bigger models perform better and dropout is effective in avoiding overfitting as observed in rows (C) and (D).' substring_quote=['We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting.']\n",
      "This is not relevant:  fact='Approximately 40K training sentences were used for WSJ constituency parsing, with a vocabulary of 16K tokens.' substring_quote=['about 40K training sentences', 'We used a vocabulary of 16K tokens for the WSJ only setting']\n",
      "This is not relevant:  fact='A larger semi-supervised dataset with 17M sentences was used with a vocabulary of 32K tokens.' substring_quote=['using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences', 'a vocabulary of 32K tokens']\n",
      "This is not relevant:  fact='The Transformer has a maximum output length set to input length + 300.' substring_quote=['increased the maximum output length to input length + 300']\n",
      "This is not relevant:  fact='A beam size of 21 and alpha = 0.3 was used for both WSJ only and semi-supervised settings.' substring_quote=['We used a beam size of 21 andα= 0.3 for both WSJ only and the semi-supervised setting']\n",
      "This is not relevant:  fact='The Transformer can be trained significantly faster than recurrent or convolutional architectures.' substring_quote=['the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers']\n",
      "This is not relevant:  fact='The Transformer outperformed the Berkeley-Parser [29] when training on the WSJ training set of 40K sentences.' substring_quote=['the Transformer outperforms the Berkeley-Parser [29] even when training only on the WSJ training set of 40K sentences']\n",
      "This is not relevant:  fact='Transformer (4 layers) achieved an F1 score of 91.3 on WSJ only, discriminative setting.' substring_quote=['Transformer (4 layers) WSJ only, discriminative 91.3']\n",
      "This is not relevant:  fact='Transformer (4 layers) achieved an F1 score of 92.7 in the semi-supervised setting.' substring_quote=['Transformer (4 layers) semi-supervised 92.7']\n",
      "This is not relevant:  fact='Code for training and evaluating their models is available at https://github.com/tensorflow/tensor2tensor.' substring_quote=['The code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor']\n",
      "This is not relevant:  fact=\"Francois Chollet authored a paper on 'Xception: Deep learning with depthwise separable convolutions'.\" substring_quote=['Francois Chollet', 'Xception: Deep learning with depthwise separable convolutions']\n",
      "This is not relevant:  fact='The paper by Junyoung Chung et al. evaluates gated recurrent neural networks on sequence modeling.' substring_quote=['Junyoung Chung', 'gated recurrent neural networks on sequence modeling']\n",
      "This is not relevant:  fact='Chris Dyer et al. wrote about recurrent neural network grammars.' substring_quote=['Chris Dyer', 'Recurrent neural network grammars']\n",
      "This is not relevant:  fact='Jonas Gehring et al. published on convolutional sequence to sequence learning.' substring_quote=['Jonas Gehring', 'convolutional sequence to sequence learning']\n",
      "This is not relevant:  fact='Kaiming He et al. contributed work on deep residual learning for image recognition.' substring_quote=['Kaiming He', 'Deep residual learning for image recognition']\n",
      "This is not relevant:  fact='Sepp Hochreiter et al. explored the difficulty of learning long-term dependencies in recurrent nets in 2001.' substring_quote=['Sepp Hochreiter', 'difficulty of learning long-term dependencies']\n",
      "This is not relevant:  fact='Sepp Hochreiter and Jürgen Schmidhuber introduced Long Short-Term Memory in 1997.' substring_quote=['Sepp Hochreiter', 'Long short-term memory', '1997']\n",
      "This is not relevant:  fact='Zhongqiang Huang and Mary Harper discussed self-training PCFG grammars with latent annotations across languages in 2009.' substring_quote=['Zhongqiang Huang', 'Self-training PCFG grammars with latent annotations across languages']\n",
      "This is not relevant:  fact='Rafal Jozefowicz et al. explored the limits of language modeling, published in 2016.' substring_quote=['Rafal Jozefowicz', 'Exploring the limits of language modeling', '2016']\n",
      "This is not relevant:  fact=\"Łukasz Kaiser and Ilya Sutskever published on 'Neural GPUs learn algorithms' in 2016.\" substring_quote=['Łukasz Kaiser', 'Neural GPUs learn algorithms']\n",
      "This is not relevant:  fact='Nal Kalchbrenner et al. authored a paper on neural machine translation in linear time (2017).' substring_quote=['Nal Kalchbrenner', 'Neural machine translation in linear time']\n",
      "This is not relevant:  fact=\"Diederik Kingma and Jimmy Ba introduced 'Adam: A method for stochastic optimization' in 2015.\" substring_quote=['Diederik Kingma', 'Adam: A method for stochastic optimization']\n",
      "This is not relevant:  fact='Oleksii Kuchaiev and Boris Ginsburg discussed factorization tricks for LSTM networks in 2017.' substring_quote=['Oleksii Kuchaiev', 'Factorization tricks for LSTM networks']\n",
      "This is not relevant:  fact='Minh-Thang Luong et al. wrote about multi-task sequence to sequence learning in 2015.' substring_quote=['Minh-Thang Luong', 'Multi-task sequence to sequence learning']\n",
      "This is not relevant:  fact=\"Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini authored 'Building a large annotated corpus of english: The penn treebank'.\" substring_quote=['Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini', 'Building a large annotated corpus of english: The penn treebank']\n",
      "This is not relevant:  fact=\"David McClosky, Eugene Charniak, and Mark Johnson worked on 'Effective self-training for parsing' presented in June 2006 at the ACL conference.\" substring_quote=['David McClosky, Eugene Charniak, and Mark Johnson', 'Effective self-training for parsing', 'June 2006', 'ACL']\n",
      "This is not relevant:  fact='Romain Paulus, Caiming Xiong, and Richard Socher proposed a deep reinforced model for abstractive summarization in 2017.' substring_quote=['Romain Paulus, Caiming Xiong, and Richard Socher', 'A deep reinforced model for abstractive summarization', '2017']\n",
      "This is not relevant:  fact='Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein researched learning accurate, compact, and interpretable tree annotation, presented at the ACL in July 2006.' substring_quote=['Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein', 'Learning accurate, compact, and interpretable tree annotation', 'ACL', 'July 2006']\n",
      "This is not relevant:  fact='Ofir Press and Lior Wolf worked on using the output embedding to improve language models in a study published on arXiv in 2016.' substring_quote=['Ofir Press and Lior Wolf', 'Using the output embedding to improve language models', '2016']\n",
      "This is not relevant:  fact='Rico Sennrich, Barry Haddow, and Alexandra Birch developed neural machine translation of rare words with subword units, published on arXiv in 2015.' substring_quote=['Rico Sennrich, Barry Haddow, and Alexandra Birch', 'Neural machine translation of rare words with subword units', 'arXiv', '2015']\n",
      "This is not relevant:  fact='Noam Shazeer and colleagues introduced outrageously large neural networks using the sparsely-gated mixture-of-experts layer in a paper on arXiv in 2017.' substring_quote=['Noam Shazeer', 'outrageously large neural networks', 'the sparsely-gated mixture-of-experts layer', '2017']\n",
      "This is not relevant:  fact='Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-nov introduced Dropout as a technique to prevent neural networks from overfitting, published in 2014.' substring_quote=['Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-nov', 'Dropout: a simple way to prevent neural networks from overfitting', '2014']\n",
      "This is not relevant:  fact='Ilya Sutskever, Oriol Vinyals, and Quoc VV Le developed sequence to sequence learning with neural networks, published in 2014.' substring_quote=['Ilya Sutskever, Oriol Vinyals, and Quoc VV Le', 'Sequence to sequence learning with neural networks', '2014']\n",
      "This is not relevant:  fact='Christian Szegedy and colleagues rethought the inception architecture for computer vision, documented in a study on CoRR in 2015.' substring_quote=['Christian Szegedy', 'Rethinking the inception architecture for computer vision', 'CoRR', '2015']\n",
      "This is not relevant:  fact='Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton researched grammar as a foreign language, published in Advances in Neural Information Processing Systems in 2015.' substring_quote=['Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton', 'Grammar as a foreign language', 'Advances in Neural Information Processing Systems', '2015']\n",
      "This is not relevant:  fact='Yonghui Wu and colleagues worked on Google’s neural machine translation system to bridge the gap between human and machine translation, published on arXiv in 2016.' substring_quote=['Yonghui Wu', 'Google’s neural machine translation system', 'arXiv', '2016']\n",
      "This is not relevant:  fact='Muhua Zhu and colleagues developed fast and accurate shift-reduce constituent parsing and presented it at the 51st Annual Meeting of the ACL in August 2013.' substring_quote=['Muhua Zhu', 'fast and accurate shift-reduce constituent parsing', '51st Annual Meeting of the ACL', 'August 2013']\n",
      "This is not relevant:  fact='Since 2009, a majority of American governments have passed new laws making the registration or voting process more difficult.' substring_quote=['a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult']\n",
      "This is not relevant:  fact='The law will never be perfect.' substring_quote=['The', 'Law', 'will', 'never', 'be', 'perfect', ',']\n",
      "This is not relevant:  fact='The application of the law should be just.' substring_quote=['but', 'its', 'application', 'should', 'be', 'just']\n",
      "This is not relevant:  fact='There is a perceived lack of justice in the application of the law.' substring_quote=['this', 'is', 'what', 'we', 'are', 'missing', 'in', 'my', 'opinion']\n",
      "This is not relevant:  fact='The law will never be perfect, according to the text.' substring_quote=['The', 'Law', 'will', 'never', 'be', 'perfect', ',']\n",
      "This is not relevant:  fact='The application of the law should be just, as per the text.' substring_quote=['but', 'its', 'application', 'should', 'be', 'just', '-']\n",
      "This is not relevant:  fact=\"The text implies that a just application of the law is currently missing, in the author's opinion.\" substring_quote=['this', 'is', 'what', 'we', 'are', 'missing', ',', 'in', 'my', 'opinion', '.']\n"
     ]
    }
   ],
   "source": [
    "objective_description = '''I want to understand how the attention mechanism works'''\n",
    "\n",
    "relevant_facts = []\n",
    "\n",
    "for facts in paper_facts:\n",
    "    for fact in facts.facts:\n",
    "        relevancy_assessment =  is_it_relevant(objective_description, fact)\n",
    "        if relevancy_assessment.relevancy_score == 'yes':\n",
    "            relevant_facts.append((fact, relevancy_assessment.justification))\n",
    "        elif relevancy_assessment.relevancy_score == 'no':\n",
    "            print(\"This is not relevant: \", fact)\n",
    "        else:\n",
    "            print(\"Error: \", fact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paper 'Attention Is All You Need' is associated with several Google Research contributors including Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, and Łukasz Kaiser.\n",
      "['Attention Is All You Need', 'Ashish Vaswani', 'Noam Shazeer', 'Niki Parmar', 'Jakob Uszkoreit', 'Llion Jones', 'Łukasz Kaiser']\n",
      "The paper 'Attention Is All You Need' is foundational in explaining the attention mechanism, making it directly relevant to your objective of understanding how the attention mechanism works.\n",
      "\n",
      "\n",
      "Aidan N. Gomez from the University of Toronto contributed to the paper.\n",
      "['Aidan N. Gomez', 'University of Toronto']\n",
      "Aidan N. Gomez was a co-author of the original 'Attention Is All You Need' paper which introduced the attention mechanism.\n",
      "\n",
      "\n",
      "The Transformer network architecture is proposed, which relies solely on attention mechanisms and omits recurrence and convolutions.\n",
      "['We propose a new simple network architecture, the Transformer', 'based solely on attention mechanisms', 'dispensing with recurrence and convolutions entirely.']\n",
      "The fact is directly relevant because it mentions the use of attention mechanisms in the Transformer, which is key to understanding how attention works.\n",
      "\n",
      "\n",
      "The Transformer architecture generalizes well to other tasks such as English constituency parsing.\n",
      "['We show that the Transformer generalizes well to other tasks', 'applying it successfully to English constituency parsing']\n",
      "Understanding the Transformer's successes in various tasks can provide insights into how its attention mechanism contributes to its efficacy.\n",
      "\n",
      "\n",
      "The contribution of the team members is notable, with Jakob proposing replacing RNNs with self-attention, Noam proposing scaled dot-product attention, and others responsible for implementation and experimental variations.\n",
      "['Jakob proposed replacing RNNs with self-attention', 'Noam proposed scaled dot-product attention, multi-head attention', 'Ashish, with Illia, designed and implemented the first Transformer models', 'Niki designed, implemented, tuned and evaluated countless model variants', 'Llion also experimented with novel model variants', 'Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor']\n",
      "It provides insight into the development and components of the attention mechanism, particularly self-attention and scaled dot-product attention.\n",
      "\n",
      "\n",
      "The work was presented at the 31st Conference on Neural Information Processing Systems (NIPS 2017) in Long Beach, CA, USA.\n",
      "['31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA']\n",
      "Understanding the context and timeline of the presentation provides background information about the formal introduction of the attention mechanism in neural networks.\n",
      "\n",
      "\n",
      "The Transformer is a model architecture that does not rely on recurrence but entirely on an attention mechanism to draw global dependencies between input and output, allowing for significantly more parallelization.\n",
      "['the Transformer, a model architecture eschewing recurrence', 'relying entirely on an attention mechanism', 'draw global dependencies between input and output', 'The Transformer allows for significantly more parallelization']\n",
      "The statement explains how the attention mechanism is utilized in the Transformer architecture, which is relevant to understanding its functionality.\n",
      "\n",
      "\n",
      "The Transformer reduces the number of operations required to relate signals from two arbitrary input or output positions to a constant number, unlike ConvS2S and ByteNet.\n",
      "['The Transformer this is reduced to a constant number of operations', 'ConvS2S', 'ByteNet']\n",
      "The fact explains a key aspect of the attention mechanism within the Transformer model, which is relevant to understanding how it functions.\n",
      "\n",
      "\n",
      "Self-attention, also known as intra-attention, is an attention mechanism that relates different positions of a single sequence to compute a representation of the sequence, used in tasks like reading comprehension, abstractive summarization, textual entailment, and learning task-independent sentence representations.\n",
      "['Self-attention, sometimes called intra-attention', 'relating different positions of a single sequence', 'compute a representation of the sequence', 'reading comprehension,', 'abstractive summarization,', 'textual entailment', 'learning task-independent sentence representations']\n",
      "The fact explains self-attention, a key component of the attention mechanism relevant to understanding how it works.\n",
      "\n",
      "\n",
      "End-to-end memory networks are based on a recurrent attention mechanism and perform well on simple-language question answering and language modeling tasks, unlike sequence-aligned recurrence.\n",
      "['End-to-end memory networks', 'based on a recurrent attention mechanism', 'perform well on simple-language question answering', 'language modeling tasks']\n",
      "The fact discusses a type of attention mechanism in memory networks, directly relevant to understanding how attention mechanisms function in certain models.\n",
      "\n",
      "\n",
      "The Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output, without using sequence-aligned RNNs or convolution.\n",
      "['Transformer is the first transduction model', 'relying entirely on self-attention', 'compute representations of its input and output', 'without using sequence-aligned RNNs or convolution']\n",
      "The fact explains a key component (self-attention) used in the Transformer model, which is vital for understanding how attention mechanisms work.\n",
      "\n",
      "\n",
      "Most competitive neural sequence transduction models have an encoder-decoder structure.\n",
      "['Most competitive neural sequence transduction models have an encoder-decoder structure']\n",
      "The encoder-decoder structure is a foundational concept in sequence transduction models that utilize attention mechanisms, making it directly relevant to understanding how attention works.\n",
      "\n",
      "\n",
      "The Transformer model architecture uses stacked self-attention and point-wise, fully connected layers for both the encoder and decoder.\n",
      "['The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder']\n",
      "The statement directly relates to how the attention mechanism is implemented within the Transformer model.\n",
      "\n",
      "\n",
      "The encoder is composed of a stack of 6 identical layers.\n",
      "['Encoder: The encoder is composed of a stack of N= 6 identical layers']\n",
      "Understanding the encoder's structure is essential for grasping the role of the attention mechanism within it.\n",
      "\n",
      "\n",
      "Each encoder layer has two sub-layers: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network.\n",
      "['Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network']\n",
      "The statement describes components of the attention mechanism within the encoder layer, which is relevant to understanding its function.\n",
      "\n",
      "\n",
      "In the decoder, there is a third sub-layer which performs multi-head attention over the encoder's output.\n",
      "['the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack']\n",
      "Understanding the sub-layer's role in the decoder is crucial to comprehending the attention mechanism's function within seq2seq models.\n",
      "\n",
      "\n",
      "Self-attention sub-layer in the decoder stack is modified to prevent positions from attending to subsequent positions.\n",
      "['modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions']\n",
      "Understanding the modification of self-attention in the decoder stack is crucial to comprehending how the attention mechanism functions overall, especially its role in sequence prediction tasks.\n",
      "\n",
      "\n",
      "The attention function maps a query and a set of key-value pairs to an output, where query, keys, values, and output are all vectors.\n",
      "['An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors']\n",
      "The fact directly explains the mapping process in the attention mechanism, which is fundamental to understanding how it works.\n",
      "\n",
      "\n",
      "Scaled Dot-Product Attention is a type of attention mechanism used in neural networks.\n",
      "['\"We call our particular attention \\'Scaled Dot-Product Attention\\' (Figure 2)\"']\n",
      "The statement directly addresses a type of attention mechanism, which is central to the user's objective of understanding attention mechanisms.\n",
      "\n",
      "\n",
      "In Scaled Dot-Product Attention, the input consists of queries and keys of dimension dk, and values of dimension dv.\n",
      "['\"The input consists of queries and keys of dimension dk, and values of dimension dv.\"']\n",
      "This fact directly pertains to the technical details of the attention mechanism, specifically the Scaled Dot-Product Attention, which is crucial for understanding how it functions.\n",
      "\n",
      "\n",
      "The process involves computing the dot products of the query with all keys, dividing each by the square root of dk, and applying a softmax function to obtain the weights on the values.\n",
      "['\"We compute the dot products of the query with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the values.\"']\n",
      "The fact directly explains a key aspect of how the attention mechanism functions in neural networks.\n",
      "\n",
      "\n",
      "Multi-Head Attention consists of several attention layers running in parallel.\n",
      "['\"Multi-Head Attention consists of several attention layers running in parallel.\"']\n",
      "Multi-Head Attention is a key component of the attention mechanism, making the fact directly relevant to the objective.\n",
      "\n",
      "\n",
      "Dot-product attention is very similar to Scaled Dot-Product Attention, except for the scaling factor.\n",
      "['\"Dot-product attention is identical to our algorithm, except for the scaling factor of1√dk.\"']\n",
      "The fact directly relates to the components of the attention mechanism, providing insight into its operation.\n",
      "\n",
      "\n",
      "Additive attention computes the compatibility function using a feed-forward network with a single hidden layer.\n",
      "['\"Additive attention computes the compatibility function using a feed-forward network with a single hidden layer.\"']\n",
      "The statement explains a specific method within attention mechanisms, directly relevant to understanding how they function.\n",
      "\n",
      "\n",
      "Dot-product attention is faster and more space-efficient than additive attention because it uses optimized matrix multiplication code.\n",
      "['\"Dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.\"']\n",
      "The fact directly explains a part of how the attention mechanism works by comparing two types of attention.\n",
      "\n",
      "\n",
      "For large values of dk, additive attention outperforms dot product attention without scaling.\n",
      "['\"additive attention outperforms dot product attention without scaling for larger values of dk\"']\n",
      "The fact directly relates to understanding how attention mechanisms work by comparing the effectiveness of additive vs dot product attention in specific conditions.\n",
      "\n",
      "\n",
      "The scaling factor in Scaled Dot-Product Attention is used to counteract the effect of large magnitudes in the dot products, which can push the softmax function into regions where it has small gradients.\n",
      "['\"To counteract this effect, we scale the dot products by1√dk.\"']\n",
      "The scaling factor is a key component of the attention mechanism, thus relevant to understanding how it works.\n",
      "\n",
      "\n",
      "Multi-Head Attention involves projecting queries, keys, and values multiple times with learned linear projections before applying the attention function in parallel.\n",
      "['\"we found it beneficial to linearly project the queries, keys and values htimes with different, learned linear projections to dk,dkanddvdimensions, respectively.\"', '\"On each of these projected versions of queries, keys and values we then perform the attention function in parallel.\"']\n",
      "The fact describes a component of the attention mechanism, specifically multi-head attention, which is crucial for understanding how attention works in models like Transformers.\n",
      "\n",
      "\n",
      "Multi-head attention allows the model to attend to information from different representation subspaces at different positions.\n",
      "['Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.']\n",
      "The fact explains a core aspect of how attention mechanisms, specifically multi-head attention, function, which aligns with understanding the attention mechanism.\n",
      "\n",
      "\n",
      "The Transformer model employs 8 parallel attention layers or heads.\n",
      "['In this work we employ h= 8 parallel attention layers, or heads.']\n",
      "The use of 8 parallel attention layers is a specific aspect of the attention mechanism within the Transformer model, relevant to understanding how attention works in practice.\n",
      "\n",
      "\n",
      "The computational cost of multi-head attention with reduced dimensions is similar to single-head attention with full dimensionality.\n",
      "['Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.']\n",
      "The fact is relevant as it relates to the efficiency and computational aspects of the attention mechanism, a key component in understanding how multi-head attention operates relative to single-head attention.\n",
      "\n",
      "\n",
      "Encoder-decoder attention layers allow every position in the decoder to attend over all positions in the input sequence.\n",
      "['In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence.']\n",
      "The fact directly explains a key component of the attention mechanism in the encoder-decoder architecture.\n",
      "\n",
      "\n",
      "Self-attention layers in the encoder allow each position to attend to all positions in the previous layer.\n",
      "['The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.']\n",
      "The statement describes a key component of the attention mechanism, specifically the self-attention aspect used in encoders, which is essential for understanding how attention works.\n",
      "\n",
      "\n",
      "Self-attention layers in the decoder allow each position to attend to all previous positions in the decoder.\n",
      "['Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position.']\n",
      "The fact is relevant because self-attention layers are a key component of the attention mechanism, explaining how information from different positions is integrated within a model.\n",
      "\n",
      "\n",
      "The Transformer model implements masking in scaled dot-product attention to prevent leftward information flow in the decoder.\n",
      "['We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections.']\n",
      "The fact directly relates to the functioning of the attention mechanism in Transformer models, particularly in managing data flow in the decoding process.\n",
      "\n",
      "\n",
      "Self-attention layer connects all positions with a constant number of sequentially executed operations.\n",
      "['self-attention layer connects all positions with a constant number of sequentially executed operations']\n",
      "The statement directly explains a fundamental aspect of the attention mechanism, which is the self-attention layer, crucial for understanding how it operates.\n",
      "\n",
      "\n",
      "Recurrent layer requires O(n) sequential operations.\n",
      "['a recurrent layer requires O(n)sequential operations']\n",
      "Understanding the attention mechanism involves knowing its advantages over traditional sequential operations like those in recurrent layers.\n",
      "\n",
      "\n",
      "Computational complexity per layer for self-attention is O(n²·d).\n",
      "['Self-Attention O(n2·d)']\n",
      "The computational complexity provides insight into the performance and efficiency characteristics of the attention mechanism, which is crucial for understanding how it works.\n",
      "\n",
      "\n",
      "The maximum path length for self-attention is O(1).\n",
      "['Self-Attention O(1)']\n",
      "The statement about the maximum path length for self-attention is directly related to understanding the attention mechanism's efficiency and scalability.\n",
      "\n",
      "\n",
      "Sequential operations for recurrent layers are O(n).\n",
      "['Recurrent O(n)']\n",
      "Understanding the complexity of recurrent layers provides context for how the attention mechanism improves upon them by reducing computational complexity.\n",
      "\n",
      "\n",
      "Restricted self-attention has a per-layer complexity of O(r·n·d).\n",
      "['Self-Attention (restricted) O(r·n·d)']\n",
      "The fact about restricted self-attention's complexity relates to understanding the computational efficiency of the attention mechanism.\n",
      "\n",
      "\n",
      "Sequential operations for restricted self-attention are O(1).\n",
      "['Self-Attention (restricted) O(1)']\n",
      "Understanding the computational complexity of self-attention mechanisms, such as O(1), is crucial for grasping how these mechanisms function efficiently.\n",
      "\n",
      "\n",
      "The maximum path length for restricted self-attention is O(n/r).\n",
      "['Self-Attention (restricted) O(n/r)']\n",
      "The fact discusses a characteristic of attention mechanisms, specifically restricted self-attention, which is relevant to understanding how the attention mechanism works.\n",
      "\n",
      "\n",
      "The model does not use recurrence or convolution.\n",
      "['our model contains no recurrence and no convolution']\n",
      "Understanding the absence of recurrence or convolution is crucial because it helps explain why the attention mechanism is used, highlighting its unique approach to handling dependencies in sequences.\n",
      "\n",
      "\n",
      "Positional encodings are added to the input embeddings of the encoder and decoder stacks.\n",
      "['we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks']\n",
      "Understanding positional encodings is part of comprehending the attention mechanism as they help in incorporating order information in the sequences processed by the attention mechanism.\n",
      "\n",
      "\n",
      "Positional encodings have the same dimension as the embeddings, so they can be summed.\n",
      "['The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed']\n",
      "Understanding positional encodings is essential to comprehending how the attention mechanism processes sequences in models like Transformers.\n",
      "\n",
      "\n",
      "The positional encoding uses sine and cosine functions of different frequencies.\n",
      "['we use sine and cosine functions of different frequencies']\n",
      "Positional encoding is a crucial component in the attention mechanism, providing information about the order of the sequence.\n",
      "\n",
      "\n",
      "Each dimension of the positional encoding corresponds to a sinusoid with wavelengths forming a geometric progression.\n",
      "['each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression']\n",
      "The positional encoding is a component of the attention mechanism in transformer models, which directly relates to understanding how attention works.\n",
      "\n",
      "\n",
      "Sinusoidal positional encoding may allow the model to extrapolate to longer sequences.\n",
      "['we chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training']\n",
      "Sinusoidal positional encoding is a component of the attention mechanism in transformer models, which helps the model understand positional information.\n",
      "\n",
      "\n",
      "Self-attention can be restricted to a neighborhood of size r in the input sequence to increase the maximum path length to O(n/r).\n",
      "['Self-attention could be restricted to considering only a neighborhood of size r in the input sequence', 'This would increase the maximum path length to O(n/r)']\n",
      "The statement directly discusses a modification to the self-attention mechanism, which is central to understanding how attention operates in machine learning.\n",
      "\n",
      "\n",
      "With k = n, the complexity of a separable convolution equals the combination of a self-attention layer and a point-wise feed-forward layer.\n",
      "['Even with k=n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer']\n",
      "The statement compares the complexity of separable convolution with self-attention layers, which directly relates to understanding how self-attention mechanisms work in terms of computational cost.\n",
      "\n",
      "\n",
      "Self-attention could yield more interpretable models as individual attention heads learn different tasks, some related to syntactic and semantic structures.\n",
      "['As a side benefit, self-attention could yield more interpretable models', 'individual attention heads clearly learn to perform different tasks', 'many appear to exhibit behavior related to the syntactic and semantic structure of the sentences']\n",
      "Self-attention directly relates to the understanding of how the attention mechanism operates, as it is a core component that facilitates diverse interpretative capabilities in models.\n",
      "\n",
      "\n",
      "Experiment (A) varies the number of attention heads and corresponding key and value dimensions.\n",
      "['In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions.']\n",
      "The experiment directly explores components of the attention mechanism, making it relevant to understanding how the attention mechanism works.\n",
      "\n",
      "\n",
      "Experiment (B) suggests reducing the attention key size hurts the model quality.\n",
      "['In Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality.']\n",
      "The fact directly discusses an experiment related to the attention mechanism's key size, which is relevant to understanding how the attention mechanism works.\n",
      "\n",
      "\n",
      "Replacing sinusoidal positional encoding with learned positional embeddings showed nearly identical results to the base model.\n",
      "['In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [ 9], and observe nearly identical results to the base model.']\n",
      "Understanding modifications to positional encoding, like replacing sinusoidal encoding, is relevant to understanding how attention mechanisms process sequence information.\n",
      "\n",
      "\n",
      "A 4-layer Transformer model with model size of 1024 was trained for English constituency parsing using the WSJ portion of the Penn Treebank.\n",
      "['We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank [ 25].']\n",
      "The fact mentions a 4-layer Transformer model, which is relevant as understanding such models often involves comprehending the attention mechanism they employ.\n",
      "\n",
      "\n",
      "Experiments for constituency parsing involved selecting dropout, attention and residual learning rates, and beam size on the Section 22 development set, leaving other parameters unchanged from the English-to-German base translation model.\n",
      "['We performed only a small number of experiments to select the dropout, both attention and residual (section 5.4), learning rates and beam size']\n",
      "The fact mentions the use of attention in the context of parsing models, which is directly related to understanding how attention operates in machine learning.\n",
      "\n",
      "\n",
      "The Transformer is the first sequence transduction model based entirely on attention.\n",
      "['In this work, we presented the Transformer, the first sequence transduction model based entirely on attention']\n",
      "The fact directly relates to your objective by identifying the Transformer model as a key example of applying the attention mechanism.\n",
      "\n",
      "\n",
      "The Transformer achieved a new state of the art on WMT 2014 English-to-German and English-to-French translation tasks.\n",
      "['On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art']\n",
      "The attention mechanism is a key component of the Transformer model, which is referenced for its translation achievements.\n",
      "\n",
      "\n",
      "The Transformer aims to extend to input and output modalities other than text, including images, audio, and video.\n",
      "['We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video']\n",
      "The attention mechanism is a key component of Transformers that allows them to process various input and output modalities.\n",
      "\n",
      "\n",
      "The paper by Kyunghyun Cho et al. is about learning phrase representations using RNN encoder-decoder for statistical machine translation.\n",
      "['Kyunghyun Cho', 'learning phrase representations using rnn encoder-decoder for statistical machine translation']\n",
      "Understanding the RNN encoder-decoder architecture is foundational for grasping how attention mechanisms enhance translation models.\n",
      "\n",
      "\n",
      "Alex Graves authored a paper on generating sequences with recurrent neural networks.\n",
      "['Alex Graves', 'Generating sequences with recurrent neural networks']\n",
      "Understanding RNNs is foundational to grasping the attention mechanism's role in sequence generation.\n",
      "\n",
      "\n",
      "Łukasz Kaiser and Samy Bengio questioned if active memory can replace attention in 2016.\n",
      "['Łukasz Kaiser', 'Can active memory replace attention']\n",
      "The fact directly addresses a specific aspect of the attention mechanism and alternative approaches, making it relevant to understanding its workings.\n",
      "\n",
      "\n",
      "Yoon Kim et al. worked on structured attention networks in 2017.\n",
      "['Yoon Kim', 'Structured attention networks']\n",
      "The fact is relevant because Yoon Kim's work on structured attention networks directly contributes to understanding the attention mechanism.\n",
      "\n",
      "\n",
      "Zhouhan Lin et al. published a structured self-attentive sentence embedding study in 2017.\n",
      "['Zhouhan Lin', 'structured self-attentive sentence embedding']\n",
      "The study by Zhouhan Lin et al. is directly related to the attention mechanism, providing insights into self-attentive models, which are crucial for understanding attention mechanisms.\n",
      "\n",
      "\n",
      "Minh-Thang Luong, Hieu Pham, and Christopher D. Manning examined effective approaches to attention-based neural machine translation in 2015.\n",
      "['Minh-Thang Luong', 'attention-based neural machine translation']\n",
      "The fact discusses a study on attention-based neural machine translation, which is directly related to understanding how the attention mechanism works.\n",
      "\n",
      "\n",
      "Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit developed a decomposable attention model presented at the Empirical Methods in Natural Language Processing in 2016.\n",
      "['Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit', 'A decomposable attention model', 'Empirical Methods in Natural Language Processing', '2016']\n",
      "The fact is relevant because it mentions a decomposable attention model that aligns with the objective of understanding how attention mechanisms work.\n",
      "\n",
      "\n",
      "Sainbayar Sukhbaatar and colleagues developed end-to-end memory networks, included in the Advances in Neural Information Processing Systems proceedings in 2015.\n",
      "['Sainbayar Sukhbaatar', 'End-to-end memory networks', 'Advances in Neural Information Processing Systems', '2015']\n",
      "End-to-end memory networks, developed by Sainbayar Sukhbaatar and colleagues, utilize the attention mechanism to selectively focus on relevant memory parts, which is fundamental to understanding how attention mechanisms work.\n",
      "\n",
      "\n",
      "Jie Zhou and colleagues proposed deep recurrent models with fast-forward connections for neural machine translation, presented in a 2016 CoRR paper.\n",
      "['Jie Zhou', 'deep recurrent models with fast-forward connections for neural machine translation', 'CoRR', '2016']\n",
      "Understanding deep recurrent models and their advancements is helpful in grasping how attention mechanisms evolved to improve neural machine translation.\n",
      "\n",
      "\n",
      "The attention mechanism follows long-distance dependencies, specifically focusing on the word 'making' to complete the phrase ‘making...more difficult’.\n",
      "['attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6', 'Many of the attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making...more difficult’']\n",
      "The fact directly relates to understanding how attention can be focused on specific words to manage dependencies in sequences.\n",
      "\n",
      "\n",
      "Different colors represent different attention heads, and attention is shown only for the word ‘making’.\n",
      "['Different colors represent different heads', 'Attentions here shown only for the word ‘making’']\n",
      "The mention of different colors representing attention heads directly relates to explaining how the attention mechanism processes 'making'.\n",
      "\n",
      "\n",
      "Figure 4 shows two attention heads in layer 5 of 6, involved in anaphora resolution.\n",
      "['Figure', '4', ':', 'Two', 'attention', 'heads,', 'also', 'in', 'layer', '5', 'of', '6,', 'apparently', 'involved', 'in', 'anaphora', 'resolution']\n",
      "Anaphora resolution is a task handled by attention mechanisms, making this fact directly relevant to understanding how attention works.\n",
      "\n",
      "\n",
      "The attentions for the word 'its' are very sharp.\n",
      "['Note', 'that', 'the', 'attentions', 'are', 'very', 'sharp', 'for', 'this', 'word']\n",
      "Understanding why the attentions for specific words like 'its' are sharp is crucial for comprehending the workings of the attention mechanism.\n",
      "\n",
      "\n",
      "Figure 5 illustrates that attention heads exhibit behavior related to sentence structure.\n",
      "['Figure', '5', ':', 'Many', 'of', 'the', 'attention', 'heads', 'exhibit', 'behaviour', 'that', 'seems', 'related', 'to', 'the', 'structure', 'of', 'the', 'sentence', '.']\n",
      "The detail about attention heads and sentence structure directly pertains to how the attention mechanism operates.\n",
      "\n",
      "\n",
      "Two examples are given from different encoder self-attention heads at layer 5 of 6.\n",
      "['We', 'give', 'two', 'such', 'examples', 'above', ',', 'from', 'two', 'different', 'heads', 'from', 'the', 'encoder', 'self-attention', 'at', 'layer', '5', 'of', '6', '.']\n",
      "Understanding examples from attention heads in a neural network layer directly relates to grasping the mechanics of the attention mechanism.\n",
      "\n",
      "\n",
      "The heads learned to perform different tasks.\n",
      "['The', 'heads', 'clearly', 'learned', 'to', 'perform', 'different', 'tasks', '.']\n",
      "Understanding the role of heads in performing different tasks is crucial to grasping the functionality of the attention mechanism.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for fact in relevant_facts:\n",
    "    print(fact[0].fact)\n",
    "    print(fact[0].substring_quote)\n",
    "    print(fact[1])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Fact:** The paper 'Attention Is All You Need' is associated with several Google Research contributors including Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, and Łukasz Kaiser.\n",
       "\n",
       "**Quotes:** Attention Is All You Need, Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Łukasz Kaiser\n",
       "\n",
       "**Justification:** The paper 'Attention Is All You Need' is foundational in explaining the attention mechanism, making it directly relevant to your objective of understanding how the attention mechanism works.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** Aidan N. Gomez from the University of Toronto contributed to the paper.\n",
       "\n",
       "**Quotes:** Aidan N. Gomez, University of Toronto\n",
       "\n",
       "**Justification:** Aidan N. Gomez was a co-author of the original 'Attention Is All You Need' paper which introduced the attention mechanism.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** The Transformer network architecture is proposed, which relies solely on attention mechanisms and omits recurrence and convolutions.\n",
       "\n",
       "**Quotes:** We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n",
       "\n",
       "**Justification:** The fact is directly relevant because it mentions the use of attention mechanisms in the Transformer, which is key to understanding how attention works.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** The Transformer architecture generalizes well to other tasks such as English constituency parsing.\n",
       "\n",
       "**Quotes:** We show that the Transformer generalizes well to other tasks, applying it successfully to English constituency parsing\n",
       "\n",
       "**Justification:** Understanding the Transformer's successes in various tasks can provide insights into how its attention mechanism contributes to its efficacy.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** The contribution of the team members is notable, with Jakob proposing replacing RNNs with self-attention, Noam proposing scaled dot-product attention, and others responsible for implementation and experimental variations.\n",
       "\n",
       "**Quotes:** Jakob proposed replacing RNNs with self-attention, Noam proposed scaled dot-product attention, multi-head attention, Ashish, with Illia, designed and implemented the first Transformer models, Niki designed, implemented, tuned and evaluated countless model variants, Llion also experimented with novel model variants, Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor\n",
       "\n",
       "**Justification:** It provides insight into the development and components of the attention mechanism, particularly self-attention and scaled dot-product attention.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** The work was presented at the 31st Conference on Neural Information Processing Systems (NIPS 2017) in Long Beach, CA, USA.\n",
       "\n",
       "**Quotes:** 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA\n",
       "\n",
       "**Justification:** Understanding the context and timeline of the presentation provides background information about the formal introduction of the attention mechanism in neural networks.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** The Transformer is a model architecture that does not rely on recurrence but entirely on an attention mechanism to draw global dependencies between input and output, allowing for significantly more parallelization.\n",
       "\n",
       "**Quotes:** the Transformer, a model architecture eschewing recurrence, relying entirely on an attention mechanism, draw global dependencies between input and output, The Transformer allows for significantly more parallelization\n",
       "\n",
       "**Justification:** The statement explains how the attention mechanism is utilized in the Transformer architecture, which is relevant to understanding its functionality.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** The Transformer reduces the number of operations required to relate signals from two arbitrary input or output positions to a constant number, unlike ConvS2S and ByteNet.\n",
       "\n",
       "**Quotes:** The Transformer this is reduced to a constant number of operations, ConvS2S, ByteNet\n",
       "\n",
       "**Justification:** The fact explains a key aspect of the attention mechanism within the Transformer model, which is relevant to understanding how it functions.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** Self-attention, also known as intra-attention, is an attention mechanism that relates different positions of a single sequence to compute a representation of the sequence, used in tasks like reading comprehension, abstractive summarization, textual entailment, and learning task-independent sentence representations.\n",
       "\n",
       "**Quotes:** Self-attention, sometimes called intra-attention, relating different positions of a single sequence, compute a representation of the sequence, reading comprehension,, abstractive summarization,, textual entailment, learning task-independent sentence representations\n",
       "\n",
       "**Justification:** The fact explains self-attention, a key component of the attention mechanism relevant to understanding how it works.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** End-to-end memory networks are based on a recurrent attention mechanism and perform well on simple-language question answering and language modeling tasks, unlike sequence-aligned recurrence.\n",
       "\n",
       "**Quotes:** End-to-end memory networks, based on a recurrent attention mechanism, perform well on simple-language question answering, language modeling tasks\n",
       "\n",
       "**Justification:** The fact discusses a type of attention mechanism in memory networks, directly relevant to understanding how attention mechanisms function in certain models.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** The Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output, without using sequence-aligned RNNs or convolution.\n",
       "\n",
       "**Quotes:** Transformer is the first transduction model, relying entirely on self-attention, compute representations of its input and output, without using sequence-aligned RNNs or convolution\n",
       "\n",
       "**Justification:** The fact explains a key component (self-attention) used in the Transformer model, which is vital for understanding how attention mechanisms work.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** Most competitive neural sequence transduction models have an encoder-decoder structure.\n",
       "\n",
       "**Quotes:** Most competitive neural sequence transduction models have an encoder-decoder structure\n",
       "\n",
       "**Justification:** The encoder-decoder structure is a foundational concept in sequence transduction models that utilize attention mechanisms, making it directly relevant to understanding how attention works.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** The Transformer model architecture uses stacked self-attention and point-wise, fully connected layers for both the encoder and decoder.\n",
       "\n",
       "**Quotes:** The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder\n",
       "\n",
       "**Justification:** The statement directly relates to how the attention mechanism is implemented within the Transformer model.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** The encoder is composed of a stack of 6 identical layers.\n",
       "\n",
       "**Quotes:** Encoder: The encoder is composed of a stack of N= 6 identical layers\n",
       "\n",
       "**Justification:** Understanding the encoder's structure is essential for grasping the role of the attention mechanism within it.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** Each encoder layer has two sub-layers: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network.\n",
       "\n",
       "**Quotes:** Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network\n",
       "\n",
       "**Justification:** The statement describes components of the attention mechanism within the encoder layer, which is relevant to understanding its function.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** In the decoder, there is a third sub-layer which performs multi-head attention over the encoder's output.\n",
       "\n",
       "**Quotes:** the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack\n",
       "\n",
       "**Justification:** Understanding the sub-layer's role in the decoder is crucial to comprehending the attention mechanism's function within seq2seq models.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** Self-attention sub-layer in the decoder stack is modified to prevent positions from attending to subsequent positions.\n",
       "\n",
       "**Quotes:** modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions\n",
       "\n",
       "**Justification:** Understanding the modification of self-attention in the decoder stack is crucial to comprehending how the attention mechanism functions overall, especially its role in sequence prediction tasks.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** The attention function maps a query and a set of key-value pairs to an output, where query, keys, values, and output are all vectors.\n",
       "\n",
       "**Quotes:** An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors\n",
       "\n",
       "**Justification:** The fact directly explains the mapping process in the attention mechanism, which is fundamental to understanding how it works.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** Scaled Dot-Product Attention is a type of attention mechanism used in neural networks.\n",
       "\n",
       "**Quotes:** \"We call our particular attention 'Scaled Dot-Product Attention' (Figure 2)\"\n",
       "\n",
       "**Justification:** The statement directly addresses a type of attention mechanism, which is central to the user's objective of understanding attention mechanisms.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** In Scaled Dot-Product Attention, the input consists of queries and keys of dimension dk, and values of dimension dv.\n",
       "\n",
       "**Quotes:** \"The input consists of queries and keys of dimension dk, and values of dimension dv.\"\n",
       "\n",
       "**Justification:** This fact directly pertains to the technical details of the attention mechanism, specifically the Scaled Dot-Product Attention, which is crucial for understanding how it functions.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** The process involves computing the dot products of the query with all keys, dividing each by the square root of dk, and applying a softmax function to obtain the weights on the values.\n",
       "\n",
       "**Quotes:** \"We compute the dot products of the query with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the values.\"\n",
       "\n",
       "**Justification:** The fact directly explains a key aspect of how the attention mechanism functions in neural networks.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** Multi-Head Attention consists of several attention layers running in parallel.\n",
       "\n",
       "**Quotes:** \"Multi-Head Attention consists of several attention layers running in parallel.\"\n",
       "\n",
       "**Justification:** Multi-Head Attention is a key component of the attention mechanism, making the fact directly relevant to the objective.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** Dot-product attention is very similar to Scaled Dot-Product Attention, except for the scaling factor.\n",
       "\n",
       "**Quotes:** \"Dot-product attention is identical to our algorithm, except for the scaling factor of1√dk.\"\n",
       "\n",
       "**Justification:** The fact directly relates to the components of the attention mechanism, providing insight into its operation.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** Additive attention computes the compatibility function using a feed-forward network with a single hidden layer.\n",
       "\n",
       "**Quotes:** \"Additive attention computes the compatibility function using a feed-forward network with a single hidden layer.\"\n",
       "\n",
       "**Justification:** The statement explains a specific method within attention mechanisms, directly relevant to understanding how they function.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** Dot-product attention is faster and more space-efficient than additive attention because it uses optimized matrix multiplication code.\n",
       "\n",
       "**Quotes:** \"Dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.\"\n",
       "\n",
       "**Justification:** The fact directly explains a part of how the attention mechanism works by comparing two types of attention.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** For large values of dk, additive attention outperforms dot product attention without scaling.\n",
       "\n",
       "**Quotes:** \"additive attention outperforms dot product attention without scaling for larger values of dk\"\n",
       "\n",
       "**Justification:** The fact directly relates to understanding how attention mechanisms work by comparing the effectiveness of additive vs dot product attention in specific conditions.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** The scaling factor in Scaled Dot-Product Attention is used to counteract the effect of large magnitudes in the dot products, which can push the softmax function into regions where it has small gradients.\n",
       "\n",
       "**Quotes:** \"To counteract this effect, we scale the dot products by1√dk.\"\n",
       "\n",
       "**Justification:** The scaling factor is a key component of the attention mechanism, thus relevant to understanding how it works.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** Multi-Head Attention involves projecting queries, keys, and values multiple times with learned linear projections before applying the attention function in parallel.\n",
       "\n",
       "**Quotes:** \"we found it beneficial to linearly project the queries, keys and values htimes with different, learned linear projections to dk,dkanddvdimensions, respectively.\", \"On each of these projected versions of queries, keys and values we then perform the attention function in parallel.\"\n",
       "\n",
       "**Justification:** The fact describes a component of the attention mechanism, specifically multi-head attention, which is crucial for understanding how attention works in models like Transformers.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** Multi-head attention allows the model to attend to information from different representation subspaces at different positions.\n",
       "\n",
       "**Quotes:** Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.\n",
       "\n",
       "**Justification:** The fact explains a core aspect of how attention mechanisms, specifically multi-head attention, function, which aligns with understanding the attention mechanism.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** The Transformer model employs 8 parallel attention layers or heads.\n",
       "\n",
       "**Quotes:** In this work we employ h= 8 parallel attention layers, or heads.\n",
       "\n",
       "**Justification:** The use of 8 parallel attention layers is a specific aspect of the attention mechanism within the Transformer model, relevant to understanding how attention works in practice.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** The computational cost of multi-head attention with reduced dimensions is similar to single-head attention with full dimensionality.\n",
       "\n",
       "**Quotes:** Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\n",
       "\n",
       "**Justification:** The fact is relevant as it relates to the efficiency and computational aspects of the attention mechanism, a key component in understanding how multi-head attention operates relative to single-head attention.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** Encoder-decoder attention layers allow every position in the decoder to attend over all positions in the input sequence.\n",
       "\n",
       "**Quotes:** In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence.\n",
       "\n",
       "**Justification:** The fact directly explains a key component of the attention mechanism in the encoder-decoder architecture.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** Self-attention layers in the encoder allow each position to attend to all positions in the previous layer.\n",
       "\n",
       "**Quotes:** The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.\n",
       "\n",
       "**Justification:** The statement describes a key component of the attention mechanism, specifically the self-attention aspect used in encoders, which is essential for understanding how attention works.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** Self-attention layers in the decoder allow each position to attend to all previous positions in the decoder.\n",
       "\n",
       "**Quotes:** Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position.\n",
       "\n",
       "**Justification:** The fact is relevant because self-attention layers are a key component of the attention mechanism, explaining how information from different positions is integrated within a model.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** The Transformer model implements masking in scaled dot-product attention to prevent leftward information flow in the decoder.\n",
       "\n",
       "**Quotes:** We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections.\n",
       "\n",
       "**Justification:** The fact directly relates to the functioning of the attention mechanism in Transformer models, particularly in managing data flow in the decoding process.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** Self-attention layer connects all positions with a constant number of sequentially executed operations.\n",
       "\n",
       "**Quotes:** self-attention layer connects all positions with a constant number of sequentially executed operations\n",
       "\n",
       "**Justification:** The statement directly explains a fundamental aspect of the attention mechanism, which is the self-attention layer, crucial for understanding how it operates.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** Recurrent layer requires O(n) sequential operations.\n",
       "\n",
       "**Quotes:** a recurrent layer requires O(n)sequential operations\n",
       "\n",
       "**Justification:** Understanding the attention mechanism involves knowing its advantages over traditional sequential operations like those in recurrent layers.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** Computational complexity per layer for self-attention is O(n²·d).\n",
       "\n",
       "**Quotes:** Self-Attention O(n2·d)\n",
       "\n",
       "**Justification:** The computational complexity provides insight into the performance and efficiency characteristics of the attention mechanism, which is crucial for understanding how it works.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** The maximum path length for self-attention is O(1).\n",
       "\n",
       "**Quotes:** Self-Attention O(1)\n",
       "\n",
       "**Justification:** The statement about the maximum path length for self-attention is directly related to understanding the attention mechanism's efficiency and scalability.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** Sequential operations for recurrent layers are O(n).\n",
       "\n",
       "**Quotes:** Recurrent O(n)\n",
       "\n",
       "**Justification:** Understanding the complexity of recurrent layers provides context for how the attention mechanism improves upon them by reducing computational complexity.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** Restricted self-attention has a per-layer complexity of O(r·n·d).\n",
       "\n",
       "**Quotes:** Self-Attention (restricted) O(r·n·d)\n",
       "\n",
       "**Justification:** The fact about restricted self-attention's complexity relates to understanding the computational efficiency of the attention mechanism.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** Sequential operations for restricted self-attention are O(1).\n",
       "\n",
       "**Quotes:** Self-Attention (restricted) O(1)\n",
       "\n",
       "**Justification:** Understanding the computational complexity of self-attention mechanisms, such as O(1), is crucial for grasping how these mechanisms function efficiently.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** The maximum path length for restricted self-attention is O(n/r).\n",
       "\n",
       "**Quotes:** Self-Attention (restricted) O(n/r)\n",
       "\n",
       "**Justification:** The fact discusses a characteristic of attention mechanisms, specifically restricted self-attention, which is relevant to understanding how the attention mechanism works.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** The model does not use recurrence or convolution.\n",
       "\n",
       "**Quotes:** our model contains no recurrence and no convolution\n",
       "\n",
       "**Justification:** Understanding the absence of recurrence or convolution is crucial because it helps explain why the attention mechanism is used, highlighting its unique approach to handling dependencies in sequences.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** Positional encodings are added to the input embeddings of the encoder and decoder stacks.\n",
       "\n",
       "**Quotes:** we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks\n",
       "\n",
       "**Justification:** Understanding positional encodings is part of comprehending the attention mechanism as they help in incorporating order information in the sequences processed by the attention mechanism.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** Positional encodings have the same dimension as the embeddings, so they can be summed.\n",
       "\n",
       "**Quotes:** The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed\n",
       "\n",
       "**Justification:** Understanding positional encodings is essential to comprehending how the attention mechanism processes sequences in models like Transformers.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** The positional encoding uses sine and cosine functions of different frequencies.\n",
       "\n",
       "**Quotes:** we use sine and cosine functions of different frequencies\n",
       "\n",
       "**Justification:** Positional encoding is a crucial component in the attention mechanism, providing information about the order of the sequence.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** Each dimension of the positional encoding corresponds to a sinusoid with wavelengths forming a geometric progression.\n",
       "\n",
       "**Quotes:** each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression\n",
       "\n",
       "**Justification:** The positional encoding is a component of the attention mechanism in transformer models, which directly relates to understanding how attention works.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** Sinusoidal positional encoding may allow the model to extrapolate to longer sequences.\n",
       "\n",
       "**Quotes:** we chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training\n",
       "\n",
       "**Justification:** Sinusoidal positional encoding is a component of the attention mechanism in transformer models, which helps the model understand positional information.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** Self-attention can be restricted to a neighborhood of size r in the input sequence to increase the maximum path length to O(n/r).\n",
       "\n",
       "**Quotes:** Self-attention could be restricted to considering only a neighborhood of size r in the input sequence, This would increase the maximum path length to O(n/r)\n",
       "\n",
       "**Justification:** The statement directly discusses a modification to the self-attention mechanism, which is central to understanding how attention operates in machine learning.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** With k = n, the complexity of a separable convolution equals the combination of a self-attention layer and a point-wise feed-forward layer.\n",
       "\n",
       "**Quotes:** Even with k=n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer\n",
       "\n",
       "**Justification:** The statement compares the complexity of separable convolution with self-attention layers, which directly relates to understanding how self-attention mechanisms work in terms of computational cost.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** Self-attention could yield more interpretable models as individual attention heads learn different tasks, some related to syntactic and semantic structures.\n",
       "\n",
       "**Quotes:** As a side benefit, self-attention could yield more interpretable models, individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences\n",
       "\n",
       "**Justification:** Self-attention directly relates to the understanding of how the attention mechanism operates, as it is a core component that facilitates diverse interpretative capabilities in models.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** Experiment (A) varies the number of attention heads and corresponding key and value dimensions.\n",
       "\n",
       "**Quotes:** In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions.\n",
       "\n",
       "**Justification:** The experiment directly explores components of the attention mechanism, making it relevant to understanding how the attention mechanism works.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** Experiment (B) suggests reducing the attention key size hurts the model quality.\n",
       "\n",
       "**Quotes:** In Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality.\n",
       "\n",
       "**Justification:** The fact directly discusses an experiment related to the attention mechanism's key size, which is relevant to understanding how the attention mechanism works.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** Replacing sinusoidal positional encoding with learned positional embeddings showed nearly identical results to the base model.\n",
       "\n",
       "**Quotes:** In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [ 9], and observe nearly identical results to the base model.\n",
       "\n",
       "**Justification:** Understanding modifications to positional encoding, like replacing sinusoidal encoding, is relevant to understanding how attention mechanisms process sequence information.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** A 4-layer Transformer model with model size of 1024 was trained for English constituency parsing using the WSJ portion of the Penn Treebank.\n",
       "\n",
       "**Quotes:** We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank [ 25].\n",
       "\n",
       "**Justification:** The fact mentions a 4-layer Transformer model, which is relevant as understanding such models often involves comprehending the attention mechanism they employ.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** Experiments for constituency parsing involved selecting dropout, attention and residual learning rates, and beam size on the Section 22 development set, leaving other parameters unchanged from the English-to-German base translation model.\n",
       "\n",
       "**Quotes:** We performed only a small number of experiments to select the dropout, both attention and residual (section 5.4), learning rates and beam size\n",
       "\n",
       "**Justification:** The fact mentions the use of attention in the context of parsing models, which is directly related to understanding how attention operates in machine learning.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** The Transformer is the first sequence transduction model based entirely on attention.\n",
       "\n",
       "**Quotes:** In this work, we presented the Transformer, the first sequence transduction model based entirely on attention\n",
       "\n",
       "**Justification:** The fact directly relates to your objective by identifying the Transformer model as a key example of applying the attention mechanism.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** The Transformer achieved a new state of the art on WMT 2014 English-to-German and English-to-French translation tasks.\n",
       "\n",
       "**Quotes:** On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art\n",
       "\n",
       "**Justification:** The attention mechanism is a key component of the Transformer model, which is referenced for its translation achievements.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** The Transformer aims to extend to input and output modalities other than text, including images, audio, and video.\n",
       "\n",
       "**Quotes:** We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video\n",
       "\n",
       "**Justification:** The attention mechanism is a key component of Transformers that allows them to process various input and output modalities.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** The paper by Kyunghyun Cho et al. is about learning phrase representations using RNN encoder-decoder for statistical machine translation.\n",
       "\n",
       "**Quotes:** Kyunghyun Cho, learning phrase representations using rnn encoder-decoder for statistical machine translation\n",
       "\n",
       "**Justification:** Understanding the RNN encoder-decoder architecture is foundational for grasping how attention mechanisms enhance translation models.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** Alex Graves authored a paper on generating sequences with recurrent neural networks.\n",
       "\n",
       "**Quotes:** Alex Graves, Generating sequences with recurrent neural networks\n",
       "\n",
       "**Justification:** Understanding RNNs is foundational to grasping the attention mechanism's role in sequence generation.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** Łukasz Kaiser and Samy Bengio questioned if active memory can replace attention in 2016.\n",
       "\n",
       "**Quotes:** Łukasz Kaiser, Can active memory replace attention\n",
       "\n",
       "**Justification:** The fact directly addresses a specific aspect of the attention mechanism and alternative approaches, making it relevant to understanding its workings.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** Yoon Kim et al. worked on structured attention networks in 2017.\n",
       "\n",
       "**Quotes:** Yoon Kim, Structured attention networks\n",
       "\n",
       "**Justification:** The fact is relevant because Yoon Kim's work on structured attention networks directly contributes to understanding the attention mechanism.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** Zhouhan Lin et al. published a structured self-attentive sentence embedding study in 2017.\n",
       "\n",
       "**Quotes:** Zhouhan Lin, structured self-attentive sentence embedding\n",
       "\n",
       "**Justification:** The study by Zhouhan Lin et al. is directly related to the attention mechanism, providing insights into self-attentive models, which are crucial for understanding attention mechanisms.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** Minh-Thang Luong, Hieu Pham, and Christopher D. Manning examined effective approaches to attention-based neural machine translation in 2015.\n",
       "\n",
       "**Quotes:** Minh-Thang Luong, attention-based neural machine translation\n",
       "\n",
       "**Justification:** The fact discusses a study on attention-based neural machine translation, which is directly related to understanding how the attention mechanism works.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit developed a decomposable attention model presented at the Empirical Methods in Natural Language Processing in 2016.\n",
       "\n",
       "**Quotes:** Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit, A decomposable attention model, Empirical Methods in Natural Language Processing, 2016\n",
       "\n",
       "**Justification:** The fact is relevant because it mentions a decomposable attention model that aligns with the objective of understanding how attention mechanisms work.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** Sainbayar Sukhbaatar and colleagues developed end-to-end memory networks, included in the Advances in Neural Information Processing Systems proceedings in 2015.\n",
       "\n",
       "**Quotes:** Sainbayar Sukhbaatar, End-to-end memory networks, Advances in Neural Information Processing Systems, 2015\n",
       "\n",
       "**Justification:** End-to-end memory networks, developed by Sainbayar Sukhbaatar and colleagues, utilize the attention mechanism to selectively focus on relevant memory parts, which is fundamental to understanding how attention mechanisms work.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** Jie Zhou and colleagues proposed deep recurrent models with fast-forward connections for neural machine translation, presented in a 2016 CoRR paper.\n",
       "\n",
       "**Quotes:** Jie Zhou, deep recurrent models with fast-forward connections for neural machine translation, CoRR, 2016\n",
       "\n",
       "**Justification:** Understanding deep recurrent models and their advancements is helpful in grasping how attention mechanisms evolved to improve neural machine translation.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** The attention mechanism follows long-distance dependencies, specifically focusing on the word 'making' to complete the phrase ‘making...more difficult’.\n",
       "\n",
       "**Quotes:** attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6, Many of the attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making...more difficult’\n",
       "\n",
       "**Justification:** The fact directly relates to understanding how attention can be focused on specific words to manage dependencies in sequences.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** Different colors represent different attention heads, and attention is shown only for the word ‘making’.\n",
       "\n",
       "**Quotes:** Different colors represent different heads, Attentions here shown only for the word ‘making’\n",
       "\n",
       "**Justification:** The mention of different colors representing attention heads directly relates to explaining how the attention mechanism processes 'making'.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** Figure 4 shows two attention heads in layer 5 of 6, involved in anaphora resolution.\n",
       "\n",
       "**Quotes:** Figure, 4, :, Two, attention, heads,, also, in, layer, 5, of, 6,, apparently, involved, in, anaphora, resolution\n",
       "\n",
       "**Justification:** Anaphora resolution is a task handled by attention mechanisms, making this fact directly relevant to understanding how attention works.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** The attentions for the word 'its' are very sharp.\n",
       "\n",
       "**Quotes:** Note, that, the, attentions, are, very, sharp, for, this, word\n",
       "\n",
       "**Justification:** Understanding why the attentions for specific words like 'its' are sharp is crucial for comprehending the workings of the attention mechanism.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** Figure 5 illustrates that attention heads exhibit behavior related to sentence structure.\n",
       "\n",
       "**Quotes:** Figure, 5, :, Many, of, the, attention, heads, exhibit, behaviour, that, seems, related, to, the, structure, of, the, sentence, .\n",
       "\n",
       "**Justification:** The detail about attention heads and sentence structure directly pertains to how the attention mechanism operates.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** Two examples are given from different encoder self-attention heads at layer 5 of 6.\n",
       "\n",
       "**Quotes:** We, give, two, such, examples, above, ,, from, two, different, heads, from, the, encoder, self-attention, at, layer, 5, of, 6, .\n",
       "\n",
       "**Justification:** Understanding examples from attention heads in a neural network layer directly relates to grasping the mechanics of the attention mechanism.\n",
       "\n",
       "---\n",
       "\n",
       "**Fact:** The heads learned to perform different tasks.\n",
       "\n",
       "**Quotes:** The, heads, clearly, learned, to, perform, different, tasks, .\n",
       "\n",
       "**Justification:** Understanding the role of heads in performing different tasks is crucial to grasping the functionality of the attention mechanism.\n",
       "\n",
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "markdown_string = \"\"\n",
    "for fact, justification in relevant_facts:\n",
    "    markdown_string += f\"**Fact:** {fact.fact}\\n\\n\"\n",
    "    markdown_string += f\"**Quotes:** {', '.join(fact.substring_quote)}\\n\\n\"\n",
    "    markdown_string += f\"**Justification:** {justification}\\n\\n---\\n\\n\"\n",
    "\n",
    "markdown_string = markdown_string.strip()\n",
    "Markdown(markdown_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# # Convert the paper_facts list to a JSON serializable format\n",
    "# serialized_data = [fact.dict() for fact in paper_facts]\n",
    "\n",
    "# # Specify the file path to save the data\n",
    "\n",
    "# file_path = \"paper_facts.json\"\n",
    "\n",
    "# # Write the serialized data to the file\n",
    "# with open(file_path, \"w\") as file:\n",
    "#     json.dump(serialized_data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYS_MSG_EXPLANATION = \"\"\"\n",
    "You are an explainer expert. Given a resource material and a question or objective,\n",
    "you will explain topics to students based solely on the resource material in a concrete and\n",
    "understandable way.\n",
    "Your explanation will be in a nicely organized markdown structure and format.\n",
    "\"\"\"\n",
    "def explain(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[{\"role\": \"system\", \"content\": SYS_MSG_EXPLANATION},\n",
    "                  {\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Absolutely! To help you understand attention and self-attention mechanisms, I will compile an organized explanation based solely on the provided markdown resource:\n",
       "\n",
       "---\n",
       "\n",
       "# Understanding Attention and Self-Attention Mechanisms\n",
       "\n",
       "## 1. Introduction to Attention Mechanisms\n",
       "\n",
       "### What is Attention in Neural Networks?\n",
       "Attention mechanisms in neural networks aim to mimic the human ability to focus on a specific subset of information when processing large amounts of data. This capability allows models to weigh different parts of the input data differently, giving more importance to relevant parts and less to irrelevant parts.\n",
       "\n",
       "### Key Components\n",
       "- **Query (Q)**: Represents what we are searching for.\n",
       "- **Keys (K)**: Represents all possible candidates.\n",
       "- **Values (V)**: Represents the data associated with each key.\n",
       "\n",
       "### How Attention is Computed\n",
       "Attention mechanisms calculate a weighted sum of values (V), where the weight assigned to each value is determined by its corresponding key (K) and the query (Q).\n",
       "\n",
       "#### Scaled Dot-Product Attention\n",
       "1. **Dot Product**: Compute the dot product of the query with all keys.\n",
       "2. **Scale**: Scale the dot products by dividing them by the square root of the dimension of the key vectors to stabilize gradients.\n",
       "3. **Softmax**: Pass the scaled scores through a softmax function to obtain the weights.\n",
       "4. **Weighted Sum**: Multiply the weights with the value vectors to obtain the final attention output.\n",
       "\n",
       "### Formula\n",
       "\\[ Attention(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V \\]\n",
       "\n",
       "Where \\( d_k \\) is the dimension of the key vectors.\n",
       "\n",
       "## 2. Introduction to Self-Attention Mechanisms\n",
       "\n",
       "### What is Self-Attention?\n",
       "Self-attention is a special type of attention mechanism where the query, key, and value vectors all come from the same place. Specifically, each position in the input sequence pays attention to all positions in the sequence (including itself) to compute a weighted sum of values.\n",
       "\n",
       "### Benefits of Self-Attention\n",
       "- **Parallelization**: Unlike RNNs, self-attention mechanisms allow for parallel computation.\n",
       "- **Context Understanding**: Enables the model to capture the long-range dependencies in sequences effectively.\n",
       "- **Scalability**: Scales better with input length compared to traditional sequence models.\n",
       "\n",
       "### How Self-Attention Works\n",
       "1. **Input**: Obtain input tensor, typically with dimensions (sequence length, embedding dimension).\n",
       "2. **Linear Transformations**: Apply linear transformations to obtain the query, key, and value matrices from the input tensor.\n",
       "3. **Attention Calculation**: Compute scaled dot-product attention for all input positions with respect to each other.\n",
       "4. **Output**: Generate the output tensor by summing the calculated attentions and passing them through another linear transformation for further processing.\n",
       "\n",
       "## 3. Application in Practice: The Transformer Model\n",
       "\n",
       "### Transformer Architecture\n",
       "The Transformer model leverages self-attention mechanisms extensively and consists of an encoder and a decoder stack:\n",
       "- **Encoder**: Repeated layers that apply self-attention and feed-forward neural networks.\n",
       "- **Decoder**: Similar to the encoder but includes additional layers for attending to the encoder's outputs.\n",
       "\n",
       "### Positional Encoding\n",
       "Since self-attention mechanisms do not inherently incorporate positional information (unlike RNNs), the Transformer model includes positional encodings to introduce a sense of order to the sequences.\n",
       "\n",
       "### Multi-Head Attention\n",
       "Rather than computing a single attention function, multi-head attention involves running multiple attention mechanisms in parallel. This allows the model to jointly attend to information from different representation subspaces at different positions.\n",
       "\n",
       "### Formula for Multi-Head Attention\n",
       "\\[ MultiHead(Q, K, V) = \\text{Concat(head_1, head_2, ..., head_h)}W^O \\]\n",
       "where each head \\( head_i \\) is computed as:\n",
       "\\[ head_i = Attention(QW_i^Q, KW_i^K, VW_i^V) \\]\n",
       "\n",
       "## Summary\n",
       "- **Attention mechanisms** enhance neural networks by allowing specific parts of input data to be weighted differently.\n",
       "- **Self-attention** uses the same input for queries, keys, and values, enabling models to capture dependencies within the same sequence.\n",
       "- **Transformers** utilize self-attention mechanisms, positional encoding, and multi-head attention to achieve state-of-the-art performance in various tasks.\n",
       "\n",
       "---\n",
       "\n",
       "I hope this explanation clarifies the concepts of attention and self-attention mechanisms for you! If you have any further questions, feel free to ask."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_to_reinforce = \"I want to understand the attention and self-attention mechanisms in depth, use only this resource below to coompile an explanation: \\n\\n {markdown_string}.\"\n",
    "\n",
    "explanation = explain(prompt_to_reinforce)\n",
    "\n",
    "Markdown(explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYS_MSG_QA = \"\"\"\n",
    "'You are a helpful Q&A expert.\n",
    "You take in context information and you output a list of questions and answers \n",
    "where each answer is a list of Facts with statements and their corresponding quotes\n",
    "that support the statement from the original context.'\n",
    "\"\"\"\n",
    "\n",
    "def create_qa(prompt_question):\n",
    "    '''Creates Q&A out of context.'''\n",
    "    response = client.beta.chat.completions.parse(\n",
    "        model=MODEL,\n",
    "        messages=[{'role': 'system', 'content': SYS_MSG_QA},\n",
    "                  {'role': 'user', 'content': prompt_question}],\n",
    "        response_format=QuestionAnswer\n",
    "    )\n",
    "    return response.choices[0].message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParsedChatCompletionMessage[QuestionAnswer](content='{\"questions\":[\"What permission does Google grant regarding the reproduction of tables and figures in the paper?\",\"What is the main contribution of the paper \\'Attention Is All You Need\\'?\",\"What is the BLEU score achieved by the Transformer model on the WMT 2014 English-to-German translation task?\",\"How long did it take to train the Transformer model on the WMT 2014 English-to-French translation task?\",\"What does the Transformer model achieve when applied to English constituency parsing?\"],\"answers\":[{\"fact\":\"Google grants permission to reproduce tables and figures in the paper for journalistic or scholarly works, provided proper attribution is provided.\",\"substring_quote\":[\"Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works.\"]},{\"fact\":\"The main contribution of the paper is the introduction of the Transformer model, which is based solely on attention mechanisms and does not use recurrence or convolutions.\",\"substring_quote\":[\"We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\"]},{\"fact\":\"The Transformer model achieved a BLEU score of 28.4 on the WMT 2014 English-to-German translation task.\",\"substring_quote\":[\"Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task.\"]},{\"fact\":\"The Transformer model took 3.5 days to train on eight GPUs for the WMT 2014 English-to-French translation task.\",\"substring_quote\":[\"...after training for 3.5 days on eight GPUs, a small fraction of the training costs...\"]},{\"fact\":\"The Transformer model generalizes well to other tasks, such as English constituency parsing, regardless of the amount of training data.\",\"substring_quote\":[\"We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\"]}]}', refusal=None, role='assistant', function_call=None, tool_calls=[], parsed=QuestionAnswer(questions=['What permission does Google grant regarding the reproduction of tables and figures in the paper?', \"What is the main contribution of the paper 'Attention Is All You Need'?\", 'What is the BLEU score achieved by the Transformer model on the WMT 2014 English-to-German translation task?', 'How long did it take to train the Transformer model on the WMT 2014 English-to-French translation task?', 'What does the Transformer model achieve when applied to English constituency parsing?'], answers=[Fact(fact='Google grants permission to reproduce tables and figures in the paper for journalistic or scholarly works, provided proper attribution is provided.', substring_quote=['Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works.']), Fact(fact='The main contribution of the paper is the introduction of the Transformer model, which is based solely on attention mechanisms and does not use recurrence or convolutions.', substring_quote=['We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.']), Fact(fact='The Transformer model achieved a BLEU score of 28.4 on the WMT 2014 English-to-German translation task.', substring_quote=['Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task.']), Fact(fact='The Transformer model took 3.5 days to train on eight GPUs for the WMT 2014 English-to-French translation task.', substring_quote=['...after training for 3.5 days on eight GPUs, a small fraction of the training costs...']), Fact(fact='The Transformer model generalizes well to other tasks, such as English constituency parsing, regardless of the amount of training data.', substring_quote=['We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.'])]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_qa = create_qa(\"Create a Q&A from the following text:\\n\\n\" + page1)\n",
    "\n",
    "output_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What permission does Google grant regarding the reproduction of tables and figures in the paper?\n",
      "Answer: Google grants permission to reproduce tables and figures in the paper for journalistic or scholarly works, provided proper attribution is provided.\n",
      "Quote: ['Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works.']\n",
      "\n",
      "\n",
      "Question: What is the main contribution of the paper 'Attention Is All You Need'?\n",
      "Answer: The main contribution of the paper is the introduction of the Transformer model, which is based solely on attention mechanisms and does not use recurrence or convolutions.\n",
      "Quote: ['We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.']\n",
      "\n",
      "\n",
      "Question: What is the BLEU score achieved by the Transformer model on the WMT 2014 English-to-German translation task?\n",
      "Answer: The Transformer model achieved a BLEU score of 28.4 on the WMT 2014 English-to-German translation task.\n",
      "Quote: ['Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task.']\n",
      "\n",
      "\n",
      "Question: How long did it take to train the Transformer model on the WMT 2014 English-to-French translation task?\n",
      "Answer: The Transformer model took 3.5 days to train on eight GPUs for the WMT 2014 English-to-French translation task.\n",
      "Quote: ['...after training for 3.5 days on eight GPUs, a small fraction of the training costs...']\n",
      "\n",
      "\n",
      "Question: What does the Transformer model achieve when applied to English constituency parsing?\n",
      "Answer: The Transformer model generalizes well to other tasks, such as English constituency parsing, regardless of the amount of training data.\n",
      "Quote: ['We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = output_qa.parsed.questions\n",
    "answers = output_qa.parsed.answers\n",
    "\n",
    "\n",
    "for q,a in zip(questions, answers):\n",
    "    print(f\"Question: {q}\")\n",
    "    answer = input(\"Answer: \")\n",
    "    print(f\"Answer: {a.fact}\")\n",
    "    print(f\"Quote: {a.substring_quote}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously questions could improve by implementing more complex architectures like RAG and advanced evaluation,\n",
    "but the idea here is to find the usable primitives for people with simple Python scripting skills to be able to enhance their workflows\n",
    "reliably."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "augment-research-workflows",
   "language": "python",
   "name": "augment-research-workflows"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
