{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL='gpt-4o-2024-08-06'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYS_MSG_KEYWORD_SEARCH = \"\"\"\n",
    "You are a search engine that translates search topics into a list of relevant keywords for effective search.\n",
    "take in a research topic and you output.\n",
    "\"\"\"\n",
    "\n",
    "SYS_MSG_SEARCH_QUERIES = \"\"\"\n",
    "You are a search engine that takes in a research topic and you output a list of relevant\n",
    "that perfectly encapsulate that topic.\n",
    "\"\"\"\n",
    "\n",
    "SYS_MSG_RELEVANCY_JUDGE = \"\"\"\n",
    "You are a research expert and an evalation engine for research results given a research topic of interest.\n",
    "Given a research topic you output a binary score yes|no to determine if a paper summary is strictly relevant to the topic\n",
    "or not.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Literal\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "class Keywords(BaseModel):\n",
    "    keywords: List[str] = Field(..., description=\"List of relevant keywords to search for\")\n",
    "\n",
    "\n",
    "class SearchQueries(BaseModel):\n",
    "    queries: List[str] = Field(..., description=\"List of queries to search for\")\n",
    "\n",
    "\n",
    "def generate_keywords_for_search(research_topic):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    response = client.beta.chat.completions.parse(\n",
    "        model=MODEL,\n",
    "        messages=[{\"role\": \"system\", \"content\": SYS_MSG_KEYWORD_SEARCH},\n",
    "                  {\"role\": \"user\", \"content\": research_topic}],\n",
    "        response_format=Keywords\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.parsed\n",
    "\n",
    "def generate_search_queries(research_topic, num_queries=5):\n",
    "    \n",
    "    response = client.beta.chat.completions.parse(\n",
    "        model=MODEL,\n",
    "        messages=[{\"role\": \"system\", \"content\": SYS_MSG_SEARCH_QUERIES},\n",
    "                  {\"role\": \"user\", \"content\": f'Generate {num_queries} search queries for this research topic: {research_topic}'}],\n",
    "        response_format=SearchQueries\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.parsed\n",
    "\n",
    "\n",
    "class RelevantPaper(BaseModel):\n",
    "    relevancy_score: Literal['yes', 'no'] = Field(description=\"A binary score yes|no if a paper is relevant given a research topic.\")\n",
    "    justification: str = Field(description=\"A short one sentence justification for the relevancy score.\")\n",
    "\n",
    "\n",
    "def filter_paper_relevancy(research_topic, paper_summary):\n",
    "    paper_relevancy_score = client.beta.chat.completions.parse(\n",
    "            model='gpt-4o-mini',\n",
    "            messages=[\n",
    "                {\n",
    "                    'role': 'system', 'content': SYS_MSG_RELEVANCY_JUDGE,\n",
    "                    'role': 'user', 'content': f'Given this research topic: {research_topic}, score the relevancy of this paper:\\n\\n {paper_summary}'\n",
    "                }\n",
    "            ],\n",
    "            response_format=RelevantPaper\n",
    "        )\n",
    "    \n",
    "    return paper_relevancy_score.choices[0].message.parsed\n",
    "\n",
    "def filter_out_search_results(research_topic: str, search_results: dict):\n",
    "    filtered_results = []\n",
    "    for paper in search_results:\n",
    "        relevancy = filter_paper_relevancy(research_topic, paper.summary)\n",
    "        if relevancy.relevancy_score=='yes':\n",
    "            filtered_results.append((paper, relevancy.justification))\n",
    "        elif relevancy.relevancy_score=='no':\n",
    "            continue\n",
    "        else:\n",
    "            raise ValueError(\"Invalid relevancy score\")\n",
    "    \n",
    "    return filtered_results\n",
    "                \n",
    "        \n",
    "\n",
    "def save_search_results(search_results: dict):\n",
    "    pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Keywords(keywords=['LLMs', 'large language models', 'enhancing learning', 'research enhancement', 'AI in education', 'machine learning', 'cognitive assistant', 'natural language processing', 'learning algorithms', 'educational technology', 'personalized learning', 'knowledge retrieval', 'information synthesis', 'AI tools for learning'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords = generate_keywords_for_search(\"LLMs for enhancing human's ability to research and learn\")\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SearchQueries(queries=['How do language models enhance research capabilities?', 'Impact of large language models on modern learning', 'Language models in academic research', 'Advantages of AI in educational research', 'How LLMs assist in data analysis and interpretation', 'Role of AI in personalized learning experiences', 'Enhancing educational outcomes using LLMs', 'Language models as research assistants', 'AI-driven tools for academic research', 'Improving information synthesis with language models', 'Predictive analytics in education using large language models', 'Collaborative research projects involving LLMs', 'Using large language models for hypothesis generation', 'How do LLMs affect traditional learning methods', 'Are language models revolutionizing educational research?'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_queries = generate_search_queries(\"LLMs for enhancing human's ability to research and learn\")\n",
    "search_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "from datetime import datetime\n",
    "\n",
    "arxiv_client = arxiv.Client()\n",
    "\n",
    "def search_arxiv_papers(keywords, year=2024, MAX_NUM_PAPERS=30):\n",
    "    query = ' '.join(keywords)\n",
    "    \n",
    "    # Define the start and end dates for the specified year\n",
    "    start_date = datetime(year, 1, 1)\n",
    "    end_date = datetime(year, 12, 31)\n",
    "    \n",
    "    # Append the date range filter to the query\n",
    "    date_filter = f'submittedDate:[{start_date.strftime(\"%Y%m%d%H%M%S\")} TO {end_date.strftime(\"%Y%m%d%H%M%S\")}]'\n",
    "    full_query = f'{query} AND {date_filter}'\n",
    "    \n",
    "    # Perform the search\n",
    "    search = arxiv.Search(query=full_query, max_results=MAX_NUM_PAPERS)\n",
    "    \n",
    "    # Fetch the results\n",
    "    results = list(arxiv_client.results(search))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "research_topic = \"LLMs for enhancing human's ability to research and learn\"\n",
    "keywords = generate_keywords_for_search(research_topic)\n",
    "structured_results = search_arxiv_papers(keywords.keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for paper in structured_results:\n",
    "#     print(f'Title: {paper.title}')\n",
    "#     print(f'URL: {paper.pdf_url}')\n",
    "#     print(f'Abstract: {paper.summary}\\n')\n",
    "#     print(\"******\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(structured_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_papers = filter_out_search_results(research_topic, structured_results)\n",
    "len(filtered_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLMs in educational technology and their impact on learning\n",
      "How large language models are revolutionizing research methodologies\n",
      "Enhancing human learning efficiency with AI and large language models\n",
      "Role of LLMs in improving cognitive abilities and research skills\n",
      "Applications of large language models in academic and educational enhancement\n"
     ]
    }
   ],
   "source": [
    "search_queries_obj = generate_search_queries(research_topic)\n",
    "search_queries = search_queries_obj.queries\n",
    "for query in search_queries:\n",
    "    print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_search_result = []\n",
    "for query in search_queries:\n",
    "    keywords = generate_keywords_for_search(query)\n",
    "    structured_results = search_arxiv_papers(keywords.keywords)\n",
    "    full_search_result.extend(filter_out_search_results(research_topic, structured_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(arxiv.Result(entry_id='http://arxiv.org/abs/2403.18679v2', updated=datetime.datetime(2024, 4, 16, 22, 10, 16, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 3, 27, 15, 21, 58, tzinfo=datetime.timezone.utc), title=\"An Exploratory Study on Upper-Level Computing Students' Use of Large Language Models as Tools in a Semester-Long Project\", authors=[arxiv.Result.Author('Ben Arie Tanay'), arxiv.Result.Author('Lexy Arinze'), arxiv.Result.Author('Siddhant S. Joshi'), arxiv.Result.Author('Kirsten A. Davis'), arxiv.Result.Author('James C. Davis')], summary=\"Background: Large Language Models (LLMs) such as ChatGPT and CoPilot are\\ninfluencing software engineering practice. Software engineering educators must\\nteach future software engineers how to use such tools well. As of yet, there\\nhave been few studies that report on the use of LLMs in the classroom. It is,\\ntherefore, important to evaluate students' perception of LLMs and possible ways\\nof adapting the computing curriculum to these shifting paradigms.\\n  Purpose: The purpose of this study is to explore computing students'\\nexperiences and approaches to using LLMs during a semester-long software\\nengineering project.\\n  Design/Method: We collected data from a senior-level software engineering\\ncourse at Purdue University. This course uses a project-based learning (PBL)\\ndesign. The students used LLMs such as ChatGPT and Copilot in their projects. A\\nsample of these student teams were interviewed to understand (1) how they used\\nLLMs in their projects; and (2) whether and how their perspectives on LLMs\\nchanged over the course of the semester. We analyzed the data to identify\\nthemes related to students' usage patterns and learning outcomes.\\n  Results/Discussion: When computing students utilize LLMs within a project,\\ntheir use cases cover both technical and professional applications. In\\naddition, these students perceive LLMs to be efficient tools in obtaining\\ninformation and completion of tasks. However, there were concerns about the\\nresponsible use of LLMs without being detrimental to their own learning\\noutcomes. Based on our findings, we recommend future research to investigate\\nthe usage of LLM's in lower-level computer engineering courses to understand\\nwhether and how LLMs can be integrated as a learning aid without hurting the\\nlearning outcomes.\", comment='Accepted to the 2024 General Conference of the American Society for\\n  Engineering Education (ASEE)', journal_ref=None, doi=None, primary_category='cs.SE', categories=['cs.SE', 'cs.HC'], links=[arxiv.Result.Link('http://arxiv.org/abs/2403.18679v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2403.18679v2', title='pdf', rel='related', content_type=None)]),\n",
       "  \"The paper explores how LLMs are utilized in an educational setting to enhance students' research and learning experiences, making it highly relevant to the topic.\"),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2405.00330v1', updated=datetime.datetime(2024, 5, 1, 5, 39, 7, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 5, 1, 5, 39, 7, tzinfo=datetime.timezone.utc), title=\"Integrating A.I. in Higher Education: Protocol for a Pilot Study with 'SAMCares: An Adaptive Learning Hub'\", authors=[arxiv.Result.Author('Syed Hasib Akhter Faruqui'), arxiv.Result.Author('Nazia Tasnim'), arxiv.Result.Author('Iftekhar Ibne Basith'), arxiv.Result.Author('Suleiman Obeidat'), arxiv.Result.Author('Faruk Yildiz')], summary=\"Learning never ends, and there is no age limit to grow yourself. However, the\\neducational landscape may face challenges in effectively catering to students'\\ninclusion and diverse learning needs. These students should have access to\\nstate-of-the-art methods for lecture delivery, online resources, and technology\\nneeds. However, with all the diverse learning sources, it becomes harder for\\nstudents to comprehend a large amount of knowledge in a short period of time.\\nTraditional assistive technologies and learning aids often lack the dynamic\\nadaptability required for individualized education plans. Large Language Models\\n(LLM) have been used in language translation, text summarization, and content\\ngeneration applications. With rapid growth in AI over the past years,\\nAI-powered chatbots and virtual assistants have been developed. This research\\naims to bridge this gap by introducing an innovative study buddy we will be\\ncalling the 'SAMCares'. The system leverages a Large Language Model (LLM) (in\\nour case, LLaMa-2 70B as the base model) and Retriever-Augmented Generation\\n(RAG) to offer real-time, context-aware, and adaptive educational support. The\\ncontext of the model will be limited to the knowledge base of Sam Houston State\\nUniversity (SHSU) course notes. The LLM component enables a chat-like\\nenvironment to interact with it to meet the unique learning requirements of\\neach student. For this, we will build a custom web-based GUI. At the same time,\\nRAG enhances real-time information retrieval and text generation, in turn\\nproviding more accurate and context-specific assistance. An option to upload\\nadditional study materials in the web GUI is added in case additional knowledge\\nsupport is required. The system's efficacy will be evaluated through controlled\\ntrials and iterative feedback mechanisms.\", comment='Accepted in ASEE Annual Conference 2024', journal_ref=None, doi=None, primary_category='cs.CY', categories=['cs.CY', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2405.00330v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2405.00330v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses the use of Large Language Models (LLMs) to provide adaptive educational support, directly aligning with enhancing human research and learning capabilities.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2408.01462v1', updated=datetime.datetime(2024, 7, 28, 14, 55, 22, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 7, 28, 14, 55, 22, tzinfo=datetime.timezone.utc), title='Faculty Perspectives on the Potential of RAG in Computer Science Higher Education', authors=[arxiv.Result.Author('Sagnik Dakshit')], summary='The emergence of Large Language Models (LLMs) has significantly impacted the\\nfield of Natural Language Processing and has transformed conversational tasks\\nacross various domains because of their widespread integration in applications\\nand public access. The discussion surrounding the application of LLMs in\\neducation has raised ethical concerns, particularly concerning plagiarism and\\npolicy compliance. Despite the prowess of LLMs in conversational tasks, the\\nlimitations of reliability and hallucinations exacerbate the need to guardrail\\nconversations, motivating our investigation of RAG in computer science higher\\neducation. We developed Retrieval Augmented Generation (RAG) applications for\\nthe two tasks of virtual teaching assistants and teaching aids. In our study,\\nwe collected the ratings and opinions of faculty members in undergraduate and\\ngraduate computer science university courses at various levels, using our\\npersonalized RAG systems for each course. This study is the first to gather\\nfaculty feedback on the application of LLM-based RAG in education. The\\ninvestigation revealed that while faculty members acknowledge the potential of\\nRAG systems as virtual teaching assistants and teaching aids, certain barriers\\nand features are suggested for their full-scale deployment. These findings\\ncontribute to the ongoing discussion on the integration of advanced language\\nmodels in educational settings, highlighting the need for careful consideration\\nof ethical implications and the development of appropriate safeguards to ensure\\nresponsible and effective implementation.', comment=None, journal_ref=None, doi=None, primary_category='cs.CY', categories=['cs.CY', 'cs.ET', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2408.01462v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2408.01462v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses the application of LLMs in education, specifically focusing on their role in enhancing teaching and learning, which directly relates to the research topic.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2403.12071v1', updated=datetime.datetime(2024, 2, 12, 17, 30, 5, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 2, 12, 17, 30, 5, tzinfo=datetime.timezone.utc), title='Tailoring Education with GenAI: A New Horizon in Lesson Planning', authors=[arxiv.Result.Author('Kostas Karpouzis'), arxiv.Result.Author('Dimitris Pantazatos'), arxiv.Result.Author('Joanna Taouki'), arxiv.Result.Author('Kalliopi Meli')], summary=\"The advent of Generative AI (GenAI) in education presents a transformative\\napproach to traditional teaching methodologies, which often overlook the\\ndiverse needs of individual students. This study introduces a GenAI tool, based\\non advanced natural language processing, designed as a digital assistant for\\neducators, enabling the creation of customized lesson plans. The tool utilizes\\nan innovative feature termed 'interactive mega-prompt,' a comprehensive query\\nsystem that allows educators to input detailed classroom specifics such as\\nstudent demographics, learning objectives, and preferred teaching styles. This\\ninput is then processed by the GenAI to generate tailored lesson plans. To\\nevaluate the tool's effectiveness, a comprehensive methodology incorporating\\nboth quantitative (i.e., % of time savings) and qualitative (i.e., user\\nsatisfaction) criteria was implemented, spanning various subjects and\\neducational levels, with continuous feedback collected from educators through a\\nstructured evaluation form. Preliminary results show that educators find the\\nGenAI-generated lesson plans effective, significantly reducing lesson planning\\ntime and enhancing the learning experience by accommodating diverse student\\nneeds. This AI-driven approach signifies a paradigm shift in education,\\nsuggesting its potential applicability in broader educational contexts,\\nincluding special education needs (SEN), where individualized attention and\\nspecific learning aids are paramount\", comment='Abstract accepted for EDUCON 2024 (IEEE Global Engineering Education\\n  Conference 2024)', journal_ref=None, doi=None, primary_category='cs.CY', categories=['cs.CY', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2403.12071v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2403.12071v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses the use of Generative AI tools in education, which aligns with the research topic of enhancing learning and research capabilities through LLMs.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2404.02798v1', updated=datetime.datetime(2024, 4, 3, 15, 7, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 4, 3, 15, 7, tzinfo=datetime.timezone.utc), title='AI and personalized learning: bridging the gap with modern educational goals', authors=[arxiv.Result.Author('Kristjan-Julius Laak'), arxiv.Result.Author('Jaan Aru')], summary='Personalized learning (PL) aspires to provide an alternative to the\\none-size-fits-all approach in education. Technology-based PL solutions have\\nshown notable effectiveness in enhancing learning performance. However, their\\nalignment with the broader goals of modern education is inconsistent across\\ntechnologies and research areas. In this paper, we examine the characteristics\\nof AI-driven PL solutions in light of the OECD Learning Compass 2030 goals. Our\\nanalysis indicates a gap between the objectives of modern education and the\\ncurrent direction of PL. We identify areas where most present-day PL\\ntechnologies could better embrace essential elements of contemporary education,\\nsuch as collaboration, cognitive engagement, and the development of general\\ncompetencies. While the present PL solutions are instrumental in aiding\\nlearning processes, the PL envisioned by educational experts extends beyond\\nsimple technological tools and requires a holistic change in the educational\\nsystem. Finally, we explore the potential of large language models, such as\\nChatGPT, and propose a hybrid model that blends artificial intelligence with a\\ncollaborative, teacher-facilitated approach to personalized learning.', comment=None, journal_ref=None, doi=None, primary_category='cs.CY', categories=['cs.CY', 'cs.HC'], links=[arxiv.Result.Link('http://arxiv.org/abs/2404.02798v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2404.02798v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses the potential of large language models (LLMs) in enhancing personalized learning, directly aligning with the topic of using LLMs to improve research and learning capabilities.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2408.01779v1', updated=datetime.datetime(2024, 8, 3, 13, 28, 19, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 8, 3, 13, 28, 19, tzinfo=datetime.timezone.utc), title='MathLearner: A Large Language Model Agent Framework for Learning to Solve Mathematical Problems', authors=[arxiv.Result.Author('Wenbei Xie'), arxiv.Result.Author('Donglin Liu'), arxiv.Result.Author('Haoran Yan'), arxiv.Result.Author('Wenjie Wu'), arxiv.Result.Author('Zongyang Liu')], summary='With the development of artificial intelligence (AI), large language models\\n(LLM) are widely used in many fields. However, the reasoning ability of LLM is\\nstill very limited when it comes to mathematical reasoning. Mathematics plays\\nan important role in all aspects of human society and is a technical guarantee\\nin the fields of healthcare, transport and aerospace, for this reason, the\\ndevelopment of AI big language models in the field of mathematics has great\\npotential significance. To improve the mathematical reasoning ability of large\\nlanguage models, we proposed an agent framework for learning to solve\\nmathematical problems based on inductive reasoning. By emulating the human\\nlearning process of generalization of learned information and effective\\napplication of previous knowledge in new reasoning tasks, this framework has\\ngreat performance in the mathematical reasoning process. It improves global\\naccuracy over the baseline method (chain-of-thought) by 20.96% and solves\\n17.54% of the mathematical problems that the baseline cannot solve. Benefiting\\nfrom the efficient RETRIEVAL method, our model improves the ability of large\\nlanguage models to efficiently use external knowledge, i.e., the mathematical\\ncomputation of the model can be based on written procedures. In education, our\\nmodel can be used as a personalised learning aid, thus reducing the inequality\\nof educational resources.', comment=None, journal_ref=None, doi=None, primary_category='cs.CL', categories=['cs.CL'], links=[arxiv.Result.Link('http://arxiv.org/abs/2408.01779v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2408.01779v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses the enhancement of large language models (LLMs) specifically for educational purposes, which aligns with the topic of improving research and learning capabilities.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2403.14702v1', updated=datetime.datetime(2024, 3, 16, 23, 50, 19, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 3, 16, 23, 50, 19, tzinfo=datetime.timezone.utc), title='Large language model-powered chatbots for internationalizing student support in higher education', authors=[arxiv.Result.Author('Achraf Hsain'), arxiv.Result.Author('Hamza El Housni')], summary=\"This research explores the integration of chatbot technology powered by\\nGPT-3.5 and GPT-4 Turbo into higher education to enhance internationalization\\nand leverage digital transformation. It delves into the design, implementation,\\nand application of Large Language Models (LLMs) for improving student\\nengagement, information access, and support. Utilizing technologies like Python\\n3, GPT API, LangChain, and Chroma Vector Store, the research emphasizes\\ncreating a high-quality, timely, and relevant transcript dataset for chatbot\\ntesting. Findings indicate the chatbot's efficacy in providing comprehensive\\nresponses, its preference over traditional methods by users, and a low error\\nrate. Highlighting the chatbot's real-time engagement, memory capabilities, and\\ncritical data access, the study demonstrates its potential to elevate\\naccessibility, efficiency, and satisfaction. Concluding, the research suggests\\nthe chatbot significantly aids higher education internationalization, proposing\\nfurther investigation into digital technology's role in educational enhancement\\nand strategy development.\", comment='Key Words: Chatbot, Higher Education, Large Language model, Student\\n  Support, Information retrieval. Presented in the conference: The\\n  Internationalization of Higher Education and Digital Transformation:\\n  Addressing Current and Future Possibilities in Oujda, Morocco', journal_ref=None, doi=None, primary_category='cs.CY', categories=['cs.CY', 'cs.IR', 'I.2, H.3'], links=[arxiv.Result.Link('http://arxiv.org/abs/2403.14702v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2403.14702v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses the use of LLMs, specifically in chatbot applications, to enhance student engagement and information access in higher education, directly relating to the topic of using LLMs to improve research and learning capabilities.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2403.14643v2', updated=datetime.datetime(2024, 3, 25, 5, 35, 12, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 2, 21, 16, 44, 35, tzinfo=datetime.timezone.utc), title='Exploring ChatGPT and its Impact on Society', authors=[arxiv.Result.Author('Md. Asraful Haque'), arxiv.Result.Author('Shuai Li')], summary=\"Artificial intelligence has been around for a while, but suddenly it has\\nreceived more attention than ever before. Thanks to innovations from companies\\nlike Google, Microsoft, Meta, and other major brands in technology. OpenAI,\\nthough, has triggered the button with its ground-breaking invention ChatGPT.\\nChatGPT is a Large Language Model (LLM) based on Transformer architecture that\\nhas the ability to generate human-like responses in a conversational context.\\nIt uses deep learning algorithms to generate natural language responses to\\ninput text. Its large number of parameters, contextual generation, and\\nopen-domain training make it a versatile and effective tool for a wide range of\\napplications, from chatbots to customer service to language translation. It has\\nthe potential to revolutionize various industries and transform the way we\\ninteract with technology. However, the use of ChatGPT has also raised several\\nconcerns, including ethical, social, and employment challenges, which must be\\ncarefully considered to ensure the responsible use of this technology. The\\narticle provides an overview of ChatGPT, delving into its architecture and\\ntraining process. It highlights the potential impacts of ChatGPT on the\\nsociety. In this paper, we suggest some approaches involving technology,\\nregulation, education, and ethics in an effort to maximize ChatGPT's benefits\\nwhile minimizing its negative impacts. This study is expected to contribute to\\na greater understanding of ChatGPT and aid in predicting the potential changes\\nit may bring about.\", comment='13 Pages', journal_ref='AI and Ethics (2024)', doi='10.1007/s43681-024-00435-4', primary_category='cs.CY', categories=['cs.CY', 'cs.AI', 'cs.CL', '68Txx'], links=[arxiv.Result.Link('http://dx.doi.org/10.1007/s43681-024-00435-4', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2403.14643v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2403.14643v2', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses ChatGPT, a type of LLM, and its potential applications and impacts, directly relating to enhancing human research and learning capabilities.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2409.02617v1', updated=datetime.datetime(2024, 9, 4, 11, 19, 17, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 9, 4, 11, 19, 17, tzinfo=datetime.timezone.utc), title='PUB: Plot Understanding Benchmark and Dataset for Evaluating Large Language Models on Synthetic Visual Data Interpretation', authors=[arxiv.Result.Author('Aneta Pawelec'), arxiv.Result.Author('Victoria Sara Wesołowska'), arxiv.Result.Author('Zuzanna Bączek'), arxiv.Result.Author('Piotr Sankowski')], summary=\"The ability of large language models (LLMs) to interpret visual\\nrepresentations of data is crucial for advancing their application in data\\nanalysis and decision-making processes. This paper presents a novel synthetic\\ndataset designed to evaluate the proficiency of LLMs in interpreting various\\nforms of data visualizations, including plots like time series, histograms,\\nviolins, boxplots, and clusters. Our dataset is generated using controlled\\nparameters to ensure comprehensive coverage of potential real-world scenarios.\\nWe employ multimodal text prompts with questions related to visual data in\\nimages to benchmark several state-of-the-art models like ChatGPT or Gemini,\\nassessing their understanding and interpretative accuracy.\\n  To ensure data integrity, our benchmark dataset is generated automatically,\\nmaking it entirely new and free from prior exposure to the models being tested.\\nThis strategy allows us to evaluate the models' ability to truly interpret and\\nunderstand the data, eliminating possibility of pre-learned responses, and\\nallowing for an unbiased evaluation of the models' capabilities. We also\\nintroduce quantitative metrics to assess the performance of the models,\\nproviding a robust and comprehensive evaluation tool.\\n  Benchmarking several state-of-the-art LLMs with this dataset reveals varying\\ndegrees of success, highlighting specific strengths and weaknesses in\\ninterpreting diverse types of visual data. The results provide valuable\\ninsights into the current capabilities of LLMs and identify key areas for\\nimprovement. This work establishes a foundational benchmark for future research\\nand development aimed at enhancing the visual interpretative abilities of\\nlanguage models. In the future, improved LLMs with robust visual interpretation\\nskills can significantly aid in automated data analysis, scientific research,\\neducational tools, and business intelligence applications.\", comment=None, journal_ref=None, doi=None, primary_category='cs.CL', categories=['cs.CL'], links=[arxiv.Result.Link('http://arxiv.org/abs/2409.02617v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2409.02617v1', title='pdf', rel='related', content_type=None)]),\n",
       "  \"The paper discusses LLMs' ability to interpret visual data, which is directly related to enhancing research and learning capabilities.\"),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2404.17739v2', updated=datetime.datetime(2024, 6, 10, 17, 29, 16, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 4, 27, 0, 38, 20, tzinfo=datetime.timezone.utc), title='How LLMs Aid in UML Modeling: An Exploratory Study with Novice Analysts', authors=[arxiv.Result.Author('Beian Wang'), arxiv.Result.Author('Chong Wang'), arxiv.Result.Author('Peng Liang'), arxiv.Result.Author('Bing Li'), arxiv.Result.Author('Cheng Zeng')], summary='Since the emergence of GPT-3, Large Language Models (LLMs) have caught the\\neyes of researchers, practitioners, and educators in the field of software\\nengineering. However, there has been relatively little investigation regarding\\nthe performance of LLMs in assisting with requirements analysis and UML\\nmodeling. This paper explores how LLMs can assist novice analysts in creating\\nthree types of typical UML models: use case models, class diagrams, and\\nsequence diagrams. For this purpose, we designed the modeling tasks of these\\nthree UML models for 45 undergraduate students who participated in a\\nrequirements modeling course, with the help of LLMs. By analyzing their project\\nreports, we found that LLMs can assist undergraduate students as novice\\nanalysts in UML modeling tasks, but LLMs also have shortcomings and limitations\\nthat should be considered when using them.', comment='The 21st IEEE International Conference on Software Services\\n  Engineering (SSE)', journal_ref=None, doi=None, primary_category='cs.SE', categories=['cs.SE'], links=[arxiv.Result.Link('http://arxiv.org/abs/2404.17739v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2404.17739v2', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper is relevant as it explores how LLMs can enhance the learning and research capabilities of novice analysts in software engineering through UML modeling.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2406.15352v1', updated=datetime.datetime(2024, 6, 21, 17, 59, 51, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 6, 21, 17, 59, 51, tzinfo=datetime.timezone.utc), title='A SMART Mnemonic Sounds like \"Glue Tonic\": Mixing LLMs with Student Feedback to Make Mnemonic Learning Stick', authors=[arxiv.Result.Author('Nishant Balepur'), arxiv.Result.Author('Matthew Shu'), arxiv.Result.Author('Alexander Hoyle'), arxiv.Result.Author('Alison Robey'), arxiv.Result.Author('Shi Feng'), arxiv.Result.Author('Seraphina Goldfarb-Tarrant'), arxiv.Result.Author('Jordan Boyd-Graber')], summary='Keyword mnemonics are memorable explanations that link new terms to simpler\\nkeywords. Prior works generate mnemonics for students, but they do not guide\\nmodels toward mnemonics students prefer and aid learning. We build SMART, a\\nmnemonic generator trained on feedback from real students learning new terms.\\nTo train SMART, we first fine-tune LLaMA-2 on a curated set of user-written\\nmnemonics. We then use LLM alignment to enhance SMART: we deploy mnemonics\\ngenerated by SMART in a flashcard app to find preferences on mnemonics students\\nfavor. We gather 2684 preferences from 45 students across two types: expressed\\n(inferred from ratings) and observed (inferred from student learning), yielding\\nthree key findings. First, expressed and observed preferences disagree; what\\nstudents think is helpful does not fully capture what is truly helpful. Second,\\nBayesian models can synthesize complementary data from multiple preference\\ntypes into a single effectiveness signal. SMART is tuned via Direct Preference\\nOptimization on this signal, which we show resolves ties and missing labels in\\nthe typical method of pairwise comparisons, augmenting data for LLM output\\nquality gains. Third, mnemonic experts assess SMART as matching GPT-4, at much\\nlower deployment costs, showing the utility of capturing diverse student\\nfeedback to align LLMs in education.', comment='In-Progress Preprint', journal_ref=None, doi=None, primary_category='cs.CL', categories=['cs.CL'], links=[arxiv.Result.Link('http://arxiv.org/abs/2406.15352v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2406.15352v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses the use of LLMs to enhance learning through personalized mnemonic generation, directly aligning with the research topic of LLMs enhancing human research and learning capabilities.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2403.14654v1', updated=datetime.datetime(2024, 2, 26, 2, 59, 7, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 2, 26, 2, 59, 7, tzinfo=datetime.timezone.utc), title='ChatGPT in Veterinary Medicine: A Practical Guidance of Generative Artificial Intelligence in Clinics, Education, and Research', authors=[arxiv.Result.Author('Candice P. Chu')], summary='ChatGPT, the most accessible generative artificial intelligence (AI) tool,\\noffers considerable potential for veterinary medicine, yet a dedicated review\\nof its specific applications is lacking. This review concisely synthesizes the\\nlatest research and practical applications of ChatGPT within the clinical,\\neducational, and research domains of veterinary medicine. It intends to provide\\nspecific guidance and actionable examples of how generative AI can be directly\\nutilized by veterinary professionals without a programming background. For\\npractitioners, ChatGPT can extract patient data, generate progress notes, and\\npotentially assist in diagnosing complex cases. Veterinary educators can create\\ncustom GPTs for student support, while students can utilize ChatGPT for exam\\npreparation. ChatGPT can aid in academic writing tasks in research, but\\nveterinary publishers have set specific requirements for authors to follow.\\nDespite its transformative potential, careful use is essential to avoid\\npitfalls like hallucination. This review addresses ethical considerations,\\nprovides learning resources, and offers tangible examples to guide responsible\\nimplementation. Carefully selected, up-to-date links to platforms that host\\nlarge language models are provided for advanced readers with programming\\ncapability. A table of key takeaways was provided to summarize this review. By\\nhighlighting potential benefits and limitations, this review equips\\nveterinarians, educators, and researchers to harness the power of ChatGPT\\neffectively.', comment=None, journal_ref=None, doi='10.3389/fvets.2024.1395934', primary_category='cs.CY', categories=['cs.CY'], links=[arxiv.Result.Link('http://dx.doi.org/10.3389/fvets.2024.1395934', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2403.14654v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2403.14654v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses the use of ChatGPT, a large language model, in educational and research contexts, aligning well with the topic of enhancing human research and learning.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2403.19303v2', updated=datetime.datetime(2024, 5, 13, 13, 33, 43, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 3, 28, 10, 40, 26, tzinfo=datetime.timezone.utc), title='Developing generative AI chatbots conceptual framework for higher education', authors=[arxiv.Result.Author('Joshua Ebere Chukwuere')], summary=\"This research explores the quickly changing field of generative artificial\\nintelligence (GAI) chatbots in higher education, an industry that is undergoing\\nmajor technological changes. AI chatbots, such as ChatGPT, HuggingChat, and\\nGoogle Bard, are becoming more and more common in a variety of sectors,\\nincluding education. Their acceptance is still in its early phases, with a\\nvariety of prospects and obstacles. However, their potential in higher\\neducation is particularly noteworthy, providing lecturers and students with\\naffordable, individualized support. Creating a comprehensive framework to aid\\nthe usage of generative AI chatbots in higher education institutions (HEIs) is\\nthe aim of this project. The Generative AI Chatbots Acceptance Model (GAICAM)\\nis the result of this study's synthesis of elements from well-known frameworks,\\nincluding the TAM, UTAUT2, TPB, and others along with variables like optimism,\\ninnovativeness, discomfort, insecurity, and others. Using a research method\\nthat encompasses a comprehensive analysis of extant literature from databases\\nsuch as IEEE, ACM, ScienceDirect, and Google Scholar, the study aims to\\ncomprehend the implications of AI Chatbots on higher education and pinpoint\\ncritical elements for their efficacious implementation. Peer-reviewed\\nEnglish-language publications published between 2020 and 2023 with a focus on\\nthe use of AI chatbots in higher education were the main focus of the search\\ncriteria. The results demonstrate how much AI chatbots can do to improve\\nstudent engagement, streamline the educational process, and support\\nadministrative and research duties. But there are also clear difficulties, such\\nas unfavorable student sentiments, doubts about the veracity of material\\nproduced by AI, and unease and nervousness with new technologies.\", comment='28 pages', journal_ref=None, doi=None, primary_category='cs.CY', categories=['cs.CY', 'cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2403.19303v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2403.19303v2', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper directly addresses the use of generative AI chatbots in higher education, focusing on their potential to enhance learning and research capabilities for both students and educators, aligning closely with the research topic.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2407.12036v1', updated=datetime.datetime(2024, 7, 1, 5, 37, 17, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 7, 1, 5, 37, 17, tzinfo=datetime.timezone.utc), title='Exploring Advanced Large Language Models with LLMsuite', authors=[arxiv.Result.Author('Giorgio Roffo')], summary='This tutorial explores the advancements and challenges in the development of\\nLarge Language Models (LLMs) such as ChatGPT and Gemini. It addresses inherent\\nlimitations like temporal knowledge cutoffs, mathematical inaccuracies, and the\\ngeneration of incorrect information, proposing solutions like Retrieval\\nAugmented Generation (RAG), Program-Aided Language Models (PAL), and frameworks\\nsuch as ReAct and LangChain. The integration of these techniques enhances LLM\\nperformance and reliability, especially in multi-step reasoning and complex\\ntask execution. The paper also covers fine-tuning strategies, including\\ninstruction fine-tuning, parameter-efficient methods like LoRA, and\\nReinforcement Learning from Human Feedback (RLHF) as well as Reinforced\\nSelf-Training (ReST). Additionally, it provides a comprehensive survey of\\ntransformer architectures and training techniques for LLMs. The toolbox for\\nimplementing these techniques is publicly available at\\nhttps://github.com/giorgioroffo/large_language_models_open_suite', comment='Keywords: Language Model Benchmarking, Pre-Trained LLM Comparison,\\n  LLM Performance Analysis, NLP Model Evaluation Tools, Public Dataset\\n  Inference for LLMs, BLEU and ROUGE Metrics for LLM, Open Source LLM Testing\\n  Tools, Large Language Model Evaluation Software, NLP Benchmarking Suite,\\n  Comprehensive LLM Evaluation Toolkit', journal_ref=None, doi='10.13140/RG.2.2.11774.80963', primary_category='cs.CL', categories=['cs.CL', 'cs.CV'], links=[arxiv.Result.Link('http://dx.doi.org/10.13140/RG.2.2.11774.80963', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2407.12036v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2407.12036v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses advancements and techniques in LLMs that directly relate to enhancing human research and learning capabilities.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2404.07387v3', updated=datetime.datetime(2024, 7, 12, 3, 23, 29, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 4, 10, 23, 28, 9, tzinfo=datetime.timezone.utc), title='BISCUIT: Scaffolding LLM-Generated Code with Ephemeral UIs in Computational Notebooks', authors=[arxiv.Result.Author('Ruijia Cheng'), arxiv.Result.Author('Titus Barik'), arxiv.Result.Author('Alan Leung'), arxiv.Result.Author('Fred Hohman'), arxiv.Result.Author('Jeffrey Nichols')], summary='Programmers frequently engage with machine learning tutorials in\\ncomputational notebooks and have been adopting code generation technologies\\nbased on large language models (LLMs). However, they encounter difficulties in\\nunderstanding and working with code produced by LLMs. To mitigate these\\nchallenges, we introduce a novel workflow into computational notebooks that\\naugments LLM-based code generation with an additional ephemeral UI step,\\noffering users UI scaffolds as an intermediate stage between user prompts and\\ncode generation. We present this workflow in BISCUIT, an extension for\\nJupyterLab that provides users with ephemeral UIs generated by LLMs based on\\nthe context of their code and intentions, scaffolding users to understand,\\nguide, and explore with LLM-generated code. Through a user study where 10\\nnovices used BISCUIT for machine learning tutorials, we found that BISCUIT\\noffers users representations of code to aid their understanding, reduces the\\ncomplexity of prompt engineering, and creates a playground for users to explore\\ndifferent variables and iterate on their ideas.', comment=None, journal_ref=None, doi=None, primary_category='cs.HC', categories=['cs.HC', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2404.07387v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2404.07387v3', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses a novel workflow that enhances understanding and learning through the use of LLMs in educational settings, directly aligning with the topic of improving human research and learning abilities.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2402.07862v2', updated=datetime.datetime(2024, 8, 22, 13, 57, 30, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 2, 12, 18, 14, 43, tzinfo=datetime.timezone.utc), title='AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy', authors=[arxiv.Result.Author('Philipp Schoenegger'), arxiv.Result.Author('Peter S. Park'), arxiv.Result.Author('Ezra Karger'), arxiv.Result.Author('Sean Trott'), arxiv.Result.Author('Philip E. Tetlock')], summary='Large language models (LLMs) match and sometimes exceeding human performance\\nin many domains. This study explores the potential of LLMs to augment human\\njudgement in a forecasting task. We evaluate the effect on human forecasters of\\ntwo LLM assistants: one designed to provide high-quality (\"superforecasting\")\\nadvice, and the other designed to be overconfident and base-rate neglecting,\\nthus providing noisy forecasting advice. We compare participants using these\\nassistants to a control group that received a less advanced model that did not\\nprovide numerical predictions or engaged in explicit discussion of predictions.\\nParticipants (N = 991) answered a set of six forecasting questions and had the\\noption to consult their assigned LLM assistant throughout. Our preregistered\\nanalyses show that interacting with each of our frontier LLM assistants\\nsignificantly enhances prediction accuracy by between 24 percent and 28 percent\\ncompared to the control group. Exploratory analyses showed a pronounced outlier\\neffect in one forecasting item, without which we find that the superforecasting\\nassistant increased accuracy by 41 percent, compared with 29 percent for the\\nnoisy assistant. We further examine whether LLM forecasting augmentation\\ndisproportionately benefits less skilled forecasters, degrades the\\nwisdom-of-the-crowd by reducing prediction diversity, or varies in\\neffectiveness with question difficulty. Our data do not consistently support\\nthese hypotheses. Our results suggest that access to a frontier LLM assistant,\\neven a noisy one, can be a helpful decision aid in cognitively demanding tasks\\ncompared to a less powerful model that does not provide specific forecasting\\nadvice. However, the effects of outliers suggest that further research into the\\nrobustness of this pattern is needed.', comment='22 pages pages (main text comprised of 19 pages, appendix comprised\\n  of three pages). 10 visualizations in the main text (four figures, six\\n  tables), three additional figures in the appendix', journal_ref=None, doi=None, primary_category='cs.CY', categories=['cs.CY', 'cs.AI', 'cs.CL', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2402.07862v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2402.07862v2', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses the enhancement of human forecasting abilities through the use of LLMs, which aligns directly with the topic of using LLMs to enhance research and learning capabilities.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2403.09409v1', updated=datetime.datetime(2024, 3, 14, 14, 1, 26, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 3, 14, 14, 1, 26, tzinfo=datetime.timezone.utc), title='\"Like a Nesting Doll\": Analyzing Recursion Analogies Generated by CS Students using Large Language Models', authors=[arxiv.Result.Author('Seth Bernstein'), arxiv.Result.Author('Paul Denny'), arxiv.Result.Author('Juho Leinonen'), arxiv.Result.Author('Lauren Kan'), arxiv.Result.Author('Arto Hellas'), arxiv.Result.Author('Matt Littlefield'), arxiv.Result.Author('Sami Sarsa'), arxiv.Result.Author('Stephen MacNeil')], summary='Grasping complex computing concepts often poses a challenge for students who\\nstruggle to anchor these new ideas to familiar experiences and understandings.\\nTo help with this, a good analogy can bridge the gap between unfamiliar\\nconcepts and familiar ones, providing an engaging way to aid understanding.\\nHowever, creating effective educational analogies is difficult even for\\nexperienced instructors. We investigate to what extent large language models\\n(LLMs), specifically ChatGPT, can provide access to personally relevant\\nanalogies on demand. Focusing on recursion, a challenging threshold concept, we\\nconducted an investigation analyzing the analogies generated by more than 350\\nfirst-year computing students. They were provided with a code snippet and\\ntasked to generate their own recursion-based analogies using ChatGPT,\\noptionally including personally relevant topics in their prompts. We observed a\\ngreat deal of diversity in the analogies produced with student-prescribed\\ntopics, in contrast to the otherwise generic analogies, highlighting the value\\nof student creativity when working with LLMs. Not only did students enjoy the\\nactivity and report an improved understanding of recursion, but they described\\nmore easily remembering analogies that were personally and culturally relevant.', comment='7 pages, 2 figures, ITiCSE 2024 preprint', journal_ref=None, doi=None, primary_category='cs.HC', categories=['cs.HC', 'cs.AI', 'cs.CL'], links=[arxiv.Result.Link('http://arxiv.org/abs/2403.09409v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2403.09409v1', title='pdf', rel='related', content_type=None)]),\n",
       "  \"The paper directly explores the use of LLMs, specifically ChatGPT, in enhancing students' understanding and learning through personalized analogies, which is relevant to the topic of using LLMs to improve research and learning capabilities.\"),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2403.09606v1', updated=datetime.datetime(2024, 3, 14, 17, 47, 20, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 3, 14, 17, 47, 20, tzinfo=datetime.timezone.utc), title='Large Language Models and Causal Inference in Collaboration: A Comprehensive Survey', authors=[arxiv.Result.Author('Xiaoyu Liu'), arxiv.Result.Author('Paiheng Xu'), arxiv.Result.Author('Junda Wu'), arxiv.Result.Author('Jiaxin Yuan'), arxiv.Result.Author('Yifan Yang'), arxiv.Result.Author('Yuhang Zhou'), arxiv.Result.Author('Fuxiao Liu'), arxiv.Result.Author('Tianrui Guan'), arxiv.Result.Author('Haoliang Wang'), arxiv.Result.Author('Tong Yu'), arxiv.Result.Author('Julian McAuley'), arxiv.Result.Author('Wei Ai'), arxiv.Result.Author('Furong Huang')], summary=\"Causal inference has shown potential in enhancing the predictive accuracy,\\nfairness, robustness, and explainability of Natural Language Processing (NLP)\\nmodels by capturing causal relationships among variables. The emergence of\\ngenerative Large Language Models (LLMs) has significantly impacted various NLP\\ndomains, particularly through their advanced reasoning capabilities. This\\nsurvey focuses on evaluating and improving LLMs from a causal view in the\\nfollowing areas: understanding and improving the LLMs' reasoning capacity,\\naddressing fairness and safety issues in LLMs, complementing LLMs with\\nexplanations, and handling multimodality. Meanwhile, LLMs' strong reasoning\\ncapacities can in turn contribute to the field of causal inference by aiding\\ncausal relationship discovery and causal effect estimations. This review\\nexplores the interplay between causal inference frameworks and LLMs from both\\nperspectives, emphasizing their collective potential to further the development\\nof more advanced and equitable artificial intelligence systems.\", comment=None, journal_ref=None, doi=None, primary_category='cs.CL', categories=['cs.CL', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2403.09606v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2403.09606v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper addresses the role of Large Language Models (LLMs) in enhancing reasoning and understanding, which directly relates to improving research and learning capabilities.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2407.11054v2', updated=datetime.datetime(2024, 8, 1, 14, 10, 22, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 7, 9, 9, 25, 27, tzinfo=datetime.timezone.utc), title='Generative AI for Health Technology Assessment: Opportunities, Challenges, and Policy Considerations', authors=[arxiv.Result.Author('Rachael Fleurence'), arxiv.Result.Author('Jiang Bian'), arxiv.Result.Author('Xiaoyan Wang'), arxiv.Result.Author('Hua Xu'), arxiv.Result.Author('Dalia Dawoud'), arxiv.Result.Author('Mitch Higashi'), arxiv.Result.Author('Jagpreet Chhatwal')], summary='This review introduces the transformative potential of generative Artificial\\nIntelligence (AI) and foundation models, including large language models\\n(LLMs), for health technology assessment (HTA). We explore their applications\\nin four critical areas, evidence synthesis, evidence generation, clinical\\ntrials and economic modeling: (1) Evidence synthesis: Generative AI has the\\npotential to assist in automating literature reviews and meta-analyses by\\nproposing search terms, screening abstracts, and extracting data with notable\\naccuracy; (2) Evidence generation: These models can potentially facilitate\\nautomating the process and analyze the increasingly available large collections\\nof real-world data (RWD), including unstructured clinical notes and imaging,\\nenhancing the speed and quality of real-world evidence (RWE) generation; (3)\\nClinical trials: Generative AI can be used to optimize trial design, improve\\npatient matching, and manage trial data more efficiently; and (4) Economic\\nmodeling: Generative AI can also aid in the development of health economic\\nmodels, from conceptualization to validation, thus streamlining the overall HTA\\nprocess. Despite their promise, these technologies, while rapidly improving,\\nare still nascent and continued careful evaluation in their applications to HTA\\nis required. To ensure their responsible use and implementation, both\\ndevelopers and users of research incorporating these tools, should familiarize\\nthemselves with their current limitations, including the issues related to\\nscientific validity, risk of bias, and consider equity and ethical\\nimplications. We also surveyed the current policy landscape and provide\\nsuggestions for HTA agencies on responsibly integrating generative AI into\\ntheir workflows, emphasizing the importance of human oversight and the\\nfast-evolving nature of these tools.', comment='24 pages, 1 figure, 1 table, 2 boxes, 103 references', journal_ref=None, doi=None, primary_category='cs.LG', categories=['cs.LG', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2407.11054v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2407.11054v2', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses the applications of large language models (LLMs) in enhancing research processes, which directly aligns with the research topic of improving human abilities to research and learn.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2405.01467v1', updated=datetime.datetime(2024, 5, 2, 16, 58, 17, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 5, 2, 16, 58, 17, tzinfo=datetime.timezone.utc), title='Student Reflections on Self-Initiated GenAI Use in HCI Education', authors=[arxiv.Result.Author('Hauke Sandhaus'), arxiv.Result.Author('Maria Teresa Parreira'), arxiv.Result.Author('Wendy Ju')], summary=\"This study explores students' self-initiated use of Generative Artificial\\nIntelligence (GenAI) tools in an interactive systems design class. Through 12\\ngroup interviews, students revealed the dual nature of GenAI in (1) stimulating\\ncreativity and (2) speeding up design iterations, alongside concerns over its\\npotential to cause shallow learning and reliance. GenAI's benefits were\\npronounced in the execution phase of design, aiding rapid prototyping and\\nideation, while its use in initial insight generation posed risks to depth and\\nreflective practice. This reflection highlights the complex role of GenAI in\\nHuman-Computer Interaction education, emphasizing the need for balanced\\nintegration to leverage its advantages without compromising fundamental\\nlearning outcomes.\", comment=\"Published to the CHI '24 Workshop: LLMs as Research Tools:\\n  Applications and Evaluations in HCI Data Work\\n  (https://sites.google.com/view/llmsindatawork/)\", journal_ref=None, doi=None, primary_category='cs.HC', categories=['cs.HC', 'K.3.1; K.3.2'], links=[arxiv.Result.Link('http://arxiv.org/abs/2405.01467v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2405.01467v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses the dual role of Generative AI in enhancing creativity and design while addressing concerns about reliance and shallow learning, aligning with the use of LLMs to enhance research and learning capabilities.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2405.02357v1', updated=datetime.datetime(2024, 5, 3, 2, 54, 43, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 5, 3, 2, 54, 43, tzinfo=datetime.timezone.utc), title='Large Language Models for Mobility in Transportation Systems: A Survey on Forecasting Tasks', authors=[arxiv.Result.Author('Zijian Zhang'), arxiv.Result.Author('Yujie Sun'), arxiv.Result.Author('Zepu Wang'), arxiv.Result.Author('Yuqi Nie'), arxiv.Result.Author('Xiaobo Ma'), arxiv.Result.Author('Peng Sun'), arxiv.Result.Author('Ruolin Li')], summary='Mobility analysis is a crucial element in the research area of transportation\\nsystems. Forecasting traffic information offers a viable solution to address\\nthe conflict between increasing transportation demands and the limitations of\\ntransportation infrastructure. Predicting human travel is significant in aiding\\nvarious transportation and urban management tasks, such as taxi dispatch and\\nurban planning. Machine learning and deep learning methods are favored for\\ntheir flexibility and accuracy. Nowadays, with the advent of large language\\nmodels (LLMs), many researchers have combined these models with previous\\ntechniques or applied LLMs to directly predict future traffic information and\\nhuman travel behaviors. However, there is a lack of comprehensive studies on\\nhow LLMs can contribute to this field. This survey explores existing approaches\\nusing LLMs for mobility forecasting problems. We provide a literature review\\nconcerning the forecasting applications within transportation systems,\\nelucidating how researchers utilize LLMs, showcasing recent state-of-the-art\\nadvancements, and identifying the challenges that must be overcome to fully\\nleverage LLMs in this domain.', comment='9 pages', journal_ref=None, doi=None, primary_category='cs.LG', categories=['cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2405.02357v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2405.02357v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper analyzes the application of LLMs in mobility forecasting, which can enhance research and learning by providing insights into advanced methodologies and applications in transportation systems.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2403.06050v1', updated=datetime.datetime(2024, 3, 10, 0, 23, 8, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 3, 10, 0, 23, 8, tzinfo=datetime.timezone.utc), title='Explaining Code with a Purpose: An Integrated Approach for Developing Code Comprehension and Prompting Skills', authors=[arxiv.Result.Author('Paul Denny'), arxiv.Result.Author('David H. Smith IV'), arxiv.Result.Author('Max Fowler'), arxiv.Result.Author('James Prather'), arxiv.Result.Author('Brett A. Becker'), arxiv.Result.Author('Juho Leinonen')], summary=\"Reading, understanding and explaining code have traditionally been important\\nskills for novices learning programming. As large language models (LLMs) become\\nprevalent, these foundational skills are more important than ever given the\\nincreasing need to understand and evaluate model-generated code. Brand new\\nskills are also needed, such as the ability to formulate clear prompts that can\\nelicit intended code from an LLM. Thus, there is great interest in integrating\\npedagogical approaches for the development of both traditional coding\\ncompetencies and the novel skills required to interact with LLMs. One effective\\nway to develop and assess code comprehension ability is with ``Explain in plain\\nEnglish'' (EiPE) questions, where students succinctly explain the purpose of a\\nfragment of code. However, grading EiPE questions has always been difficult\\ngiven the subjective nature of evaluating written explanations and this has\\nstifled their uptake. In this paper, we explore a natural synergy between EiPE\\nquestions and code-generating LLMs to overcome this limitation. We propose\\nusing an LLM to generate code based on students' responses to EiPE questions --\\nnot only enabling EiPE responses to be assessed automatically, but helping\\nstudents develop essential code comprehension and prompt crafting skills in\\nparallel. We investigate this idea in an introductory programming course and\\nreport student success in creating effective prompts for solving EiPE\\nquestions. We also examine student perceptions of this activity and how it\\ninfluences their views on the use of LLMs for aiding and assessing learning.\", comment='Accepted to ITiCSE 2024', journal_ref=None, doi=None, primary_category='cs.HC', categories=['cs.HC', 'cs.CY', 'cs.SE'], links=[arxiv.Result.Link('http://arxiv.org/abs/2403.06050v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2403.06050v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper directly discusses how LLMs can enhance learning and research skills in programming education, which aligns closely with the given research topic.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2407.01245v2', updated=datetime.datetime(2024, 7, 23, 12, 23, 38, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 7, 1, 12, 44, 52, tzinfo=datetime.timezone.utc), title='SINKT: A Structure-Aware Inductive Knowledge Tracing Model with Large Language Model', authors=[arxiv.Result.Author('Lingyue Fu'), arxiv.Result.Author('Hao Guan'), arxiv.Result.Author('Kounianhua Du'), arxiv.Result.Author('Jianghao Lin'), arxiv.Result.Author('Wei Xia'), arxiv.Result.Author('Weinan Zhang'), arxiv.Result.Author('Ruiming Tang'), arxiv.Result.Author('Yasheng Wang'), arxiv.Result.Author('Yong Yu')], summary=\"Knowledge Tracing (KT) aims to determine whether students will respond\\ncorrectly to the next question, which is a crucial task in intelligent tutoring\\nsystems (ITS). In educational KT scenarios, transductive ID-based methods often\\nface severe data sparsity and cold start problems, where interactions between\\nindividual students and questions are sparse, and new questions and concepts\\nconsistently arrive in the database. In addition, existing KT models only\\nimplicitly consider the correlation between concepts and questions, lacking\\ndirect modeling of the more complex relationships in the heterogeneous graph of\\nconcepts and questions. In this paper, we propose a Structure-aware Inductive\\nKnowledge Tracing model with large language model (dubbed SINKT), which, for\\nthe first time, introduces large language models (LLMs) and realizes inductive\\nknowledge tracing. Firstly, SINKT utilizes LLMs to introduce structural\\nrelationships between concepts and constructs a heterogeneous graph for\\nconcepts and questions. Secondly, by encoding concepts and questions with LLMs,\\nSINKT incorporates semantic information to aid prediction. Finally, SINKT\\npredicts the student's response to the target question by interacting with the\\nstudent's knowledge state and the question representation. Experiments on four\\nreal-world datasets demonstrate that SINKT achieves state-of-the-art\\nperformance among 12 existing transductive KT models. Additionally, we explore\\nthe performance of SINKT on the inductive KT task and provide insights into\\nvarious modules.\", comment=None, journal_ref=None, doi=None, primary_category='cs.AI', categories=['cs.AI', 'cs.CY'], links=[arxiv.Result.Link('http://arxiv.org/abs/2407.01245v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2407.01245v2', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses the use of large language models to enhance knowledge tracing in educational settings, which directly relates to improving human research and learning capabilities.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2408.06872v1', updated=datetime.datetime(2024, 8, 13, 13, 10, 3, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 8, 13, 13, 10, 3, tzinfo=datetime.timezone.utc), title='Generative AI Tools in Academic Research: Applications and Implications for Qualitative and Quantitative Research Methodologies', authors=[arxiv.Result.Author('Mike Perkins'), arxiv.Result.Author('Jasper Roe')], summary='This study examines the impact of Generative Artificial Intelligence (GenAI)\\non academic research, focusing on its application to qualitative and\\nquantitative data analysis. As GenAI tools evolve rapidly, they offer new\\npossibilities for enhancing research productivity and democratising complex\\nanalytical processes. However, their integration into academic practice raises\\nsignificant questions regarding research integrity and security, authorship,\\nand the changing nature of scholarly work. Through an examination of current\\ncapabilities and potential future applications, this study provides insights\\ninto how researchers may utilise GenAI tools responsibly and ethically.\\n  We present case studies that demonstrate the application of GenAI in various\\nresearch methodologies, discuss the challenges of replicability and consistency\\nin AI-assisted research, and consider the ethical implications of increased AI\\nintegration in academia. This study explores both qualitative and quantitative\\napplications of GenAI, highlighting tools for transcription, coding, thematic\\nanalysis, visual analytics, and statistical analysis. By addressing these\\nissues, we aim to contribute to the ongoing discourse on the role of AI in\\nshaping the future of academic research and provide guidance for researchers\\nexploring the rapidly evolving landscape of AI-assisted research tools and\\nresearch.', comment=None, journal_ref=None, doi=None, primary_category='cs.HC', categories=['cs.HC', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2408.06872v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2408.06872v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses the impact of Generative AI on academic research, which aligns with enhancing human capability to research and learn.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2404.03044v1', updated=datetime.datetime(2024, 4, 3, 20, 8, 15, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 4, 3, 20, 8, 15, tzinfo=datetime.timezone.utc), title='The Artificial Intelligence Ontology: LLM-assisted construction of AI concept hierarchies', authors=[arxiv.Result.Author('Marcin P. Joachimiak'), arxiv.Result.Author('Mark A. Miller'), arxiv.Result.Author('J. Harry Caufield'), arxiv.Result.Author('Ryan Ly'), arxiv.Result.Author('Nomi L. Harris'), arxiv.Result.Author('Andrew Tritt'), arxiv.Result.Author('Christopher J. Mungall'), arxiv.Result.Author('Kristofer E. Bouchard')], summary=\"The Artificial Intelligence Ontology (AIO) is a systematization of artificial\\nintelligence (AI) concepts, methodologies, and their interrelations. Developed\\nvia manual curation, with the additional assistance of large language models\\n(LLMs), AIO aims to address the rapidly evolving landscape of AI by providing a\\ncomprehensive framework that encompasses both technical and ethical aspects of\\nAI technologies. The primary audience for AIO includes AI researchers,\\ndevelopers, and educators seeking standardized terminology and concepts within\\nthe AI domain. The ontology is structured around six top-level branches:\\nNetworks, Layers, Functions, LLMs, Preprocessing, and Bias, each designed to\\nsupport the modular composition of AI methods and facilitate a deeper\\nunderstanding of deep learning architectures and ethical considerations in AI.\\n  AIO's development utilized the Ontology Development Kit (ODK) for its\\ncreation and maintenance, with its content being dynamically updated through\\nAI-driven curation support. This approach not only ensures the ontology's\\nrelevance amidst the fast-paced advancements in AI but also significantly\\nenhances its utility for researchers, developers, and educators by simplifying\\nthe integration of new AI concepts and methodologies.\\n  The ontology's utility is demonstrated through the annotation of AI methods\\ndata in a catalog of AI research publications and the integration into the\\nBioPortal ontology resource, highlighting its potential for cross-disciplinary\\nresearch. The AIO ontology is open source and is available on GitHub\\n(https://github.com/berkeleybop/artificial-intelligence-ontology) and BioPortal\\n(https://bioportal.bioontology.org/ontologies/AIO).\", comment=None, journal_ref=None, doi=None, primary_category='cs.LG', categories=['cs.LG', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2404.03044v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2404.03044v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses the use of large language models (LLMs) in creating a structured ontology that facilitates research and learning in AI, aligning with the topic of enhancing human ability to research and learn.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2404.08680v1', updated=datetime.datetime(2024, 4, 8, 0, 8, 29, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 4, 8, 0, 8, 29, tzinfo=datetime.timezone.utc), title='Automating Research Synthesis with Domain-Specific Large Language Model Fine-Tuning', authors=[arxiv.Result.Author('Teo Susnjak'), arxiv.Result.Author('Peter Hwang'), arxiv.Result.Author('Napoleon H. Reyes'), arxiv.Result.Author('Andre L. C. Barczak'), arxiv.Result.Author('Timothy R. McIntosh'), arxiv.Result.Author('Surangika Ranathunga')], summary='This research pioneers the use of fine-tuned Large Language Models (LLMs) to\\nautomate Systematic Literature Reviews (SLRs), presenting a significant and\\nnovel contribution in integrating AI to enhance academic research\\nmethodologies. Our study employed the latest fine-tuning methodologies together\\nwith open-sourced LLMs, and demonstrated a practical and efficient approach to\\nautomating the final execution stages of an SLR process that involves knowledge\\nsynthesis. The results maintained high fidelity in factual accuracy in LLM\\nresponses, and were validated through the replication of an existing\\nPRISMA-conforming SLR. Our research proposed solutions for mitigating LLM\\nhallucination and proposed mechanisms for tracking LLM responses to their\\nsources of information, thus demonstrating how this approach can meet the\\nrigorous demands of scholarly research. The findings ultimately confirmed the\\npotential of fine-tuned LLMs in streamlining various labor-intensive processes\\nof conducting literature reviews. Given the potential of this approach and its\\napplicability across all research domains, this foundational study also\\nadvocated for updating PRISMA reporting guidelines to incorporate AI-driven\\nprocesses, ensuring methodological transparency and reliability in future SLRs.\\nThis study broadens the appeal of AI-enhanced tools across various academic and\\nresearch fields, setting a new standard for conducting comprehensive and\\naccurate literature reviews with more efficiency in the face of ever-increasing\\nvolumes of academic studies.', comment=None, journal_ref=None, doi=None, primary_category='cs.CL', categories=['cs.CL', 'cs.DL', 'cs.IR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2404.08680v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2404.08680v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper is highly relevant as it explores the application of LLMs to enhance the systematic literature review process, directly aligning with the topic of using LLMs to improve research capabilities.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2404.04436v1', updated=datetime.datetime(2024, 4, 5, 22, 30, 47, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 4, 5, 22, 30, 47, tzinfo=datetime.timezone.utc), title='AI Knowledge and Reasoning: Emulating Expert Creativity in Scientific Research', authors=[arxiv.Result.Author('Anirban Mukherjee'), arxiv.Result.Author('Hannah Hanwen Chang')], summary=\"We investigate whether modern AI can emulate expert creativity in complex\\nscientific endeavors. We introduce novel methodology that utilizes original\\nresearch articles published after the AI's training cutoff, ensuring no prior\\nexposure, mitigating concerns of rote memorization and prior training. The AI\\nare tasked with redacting findings, predicting outcomes from redacted research,\\nand assessing prediction accuracy against reported results. Analysis on 589\\npublished studies in four leading psychology journals over a 28-month period,\\nshowcase the AI's proficiency in understanding specialized research, deductive\\nreasoning, and evaluating evidentiary alignment--cognitive hallmarks of human\\nsubject matter expertise and creativity. These findings suggest the potential\\nof general-purpose AI to transform academia, with roles requiring\\nknowledge-based creativity become increasingly susceptible to technological\\nsubstitution.\", comment=None, journal_ref=None, doi=None, primary_category='cs.AI', categories=['cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2404.04436v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2404.04436v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper explores the use of AI in enhancing research capabilities, which aligns with the topic of LLMs improving research and learning abilities.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2405.10689v1', updated=datetime.datetime(2024, 5, 17, 10, 48, 14, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 5, 17, 10, 48, 14, tzinfo=datetime.timezone.utc), title='Revolutionizing Process Mining: A Novel Architecture for ChatGPT Integration and Enhanced User Experience through Optimized Prompt Engineering', authors=[arxiv.Result.Author('Mehrdad Agha Mohammad Ali Kermani'), arxiv.Result.Author('Hamid Reza Seddighi'), arxiv.Result.Author('Mehrdad Maghsoudi')], summary='In the rapidly evolving field of business process management, there is a\\ngrowing need for analytical tools that can transform complex data into\\nactionable insights. This research introduces a novel approach by integrating\\nLarge Language Models (LLMs), such as ChatGPT, into process mining tools,\\nmaking process analytics more accessible to a wider audience. The study aims to\\ninvestigate how ChatGPT enhances analytical capabilities, improves user\\nexperience, increases accessibility, and optimizes the architectural frameworks\\nof process mining tools. The key innovation of this research lies in developing\\na tailored prompt engineering strategy for each process mining submodule,\\nensuring that the AI-generated outputs are accurate and relevant to the\\ncontext. The integration architecture follows an Extract, Transform, Load (ETL)\\nprocess, which includes various process mining engine modules and utilizes\\nzero-shot and optimized prompt engineering techniques. ChatGPT is connected via\\nAPIs and receives structured outputs from the process mining modules, enabling\\nconversational interactions. To validate the effectiveness of this approach,\\nthe researchers used data from 17 companies that employ BehfaLab\\'s Process\\nMining Tool. The results showed significant improvements in user experience,\\nwith an expert panel rating 72% of the results as \"Good\". This research\\ncontributes to the advancement of business process analysis methodologies by\\ncombining process mining with artificial intelligence. Future research\\ndirections include further optimization of prompt engineering, exploration of\\nintegration with other AI technologies, and assessment of scalability across\\nvarious business environments. This study paves the way for continuous\\ninnovation at the intersection of process mining and artificial intelligence,\\npromising to revolutionize the way businesses analyze and optimize their\\nprocesses.', comment=None, journal_ref=None, doi=None, primary_category='cs.CL', categories=['cs.CL'], links=[arxiv.Result.Link('http://arxiv.org/abs/2405.10689v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2405.10689v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses the integration of LLMs like ChatGPT to enhance data analysis and user experience, which is directly related to improving human research and learning capabilities.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2403.04190v1', updated=datetime.datetime(2024, 3, 7, 3, 38, 44, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 3, 7, 3, 38, 44, tzinfo=datetime.timezone.utc), title='Generative AI for Synthetic Data Generation: Methods, Challenges and the Future', authors=[arxiv.Result.Author('Xu Guo'), arxiv.Result.Author('Yiqiang Chen')], summary='The recent surge in research focused on generating synthetic data from large\\nlanguage models (LLMs), especially for scenarios with limited data\\navailability, marks a notable shift in Generative Artificial Intelligence (AI).\\nTheir ability to perform comparably to real-world data positions this approach\\nas a compelling solution to low-resource challenges. This paper delves into\\nadvanced technologies that leverage these gigantic LLMs for the generation of\\ntask-specific training data. We outline methodologies, evaluation techniques,\\nand practical applications, discuss the current limitations, and suggest\\npotential pathways for future research.', comment=None, journal_ref=None, doi=None, primary_category='cs.LG', categories=['cs.LG', 'cs.AI', 'cs.CL', 'I.2.0'], links=[arxiv.Result.Link('http://arxiv.org/abs/2403.04190v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2403.04190v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper explores the use of LLMs to generate synthetic data, which directly relates to enhancing research and learning capabilities in scenarios with limited data.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2403.18838v1', updated=datetime.datetime(2024, 2, 22, 15, 10, 2, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 2, 22, 15, 10, 2, tzinfo=datetime.timezone.utc), title='Unleashing the Power of AI. A Systematic Review of Cutting-Edge Techniques in AI-Enhanced Scientometrics, Webometrics, and Bibliometrics', authors=[arxiv.Result.Author('Hamid Reza Saeidnia'), arxiv.Result.Author('Elaheh Hosseini'), arxiv.Result.Author('Shadi Abdoli'), arxiv.Result.Author('Marcel Ausloos')], summary='Purpose: The study aims to analyze the synergy of Artificial Intelligence\\n(AI), with scientometrics, webometrics, and bibliometrics to unlock and to\\nemphasize the potential of the applications and benefits of AI algorithms in\\nthese fields.\\n  Design/methodology/approach: By conducting a systematic literature review,\\nour aim is to explore the potential of AI in revolutionizing the methods used\\nto measure and analyze scholarly communication, identify emerging research\\ntrends, and evaluate the impact of scientific publications. To achieve this, we\\nimplemented a comprehensive search strategy across reputable databases such as\\nProQuest, IEEE Explore, EBSCO, Web of Science, and Scopus. Our search\\nencompassed articles published from January 1, 2000, to September 2022,\\nresulting in a thorough review of 61 relevant articles.\\n  Findings: (i) Regarding scientometrics, the application of AI yields various\\ndistinct advantages, such as conducting analyses of publications, citations,\\nresearch impact prediction, collaboration, research trend analysis, and\\nknowledge mapping, in a more objective and reliable framework. (ii) In terms of\\nwebometrics, AI algorithms are able to enhance web crawling and data\\ncollection, web link analysis, web content analysis, social media analysis, web\\nimpact analysis, and recommender systems. (iii) Moreover, automation of data\\ncollection, analysis of citations, disambiguation of authors, analysis of\\nco-authorship networks, assessment of research impact, text mining, and\\nrecommender systems are considered as the potential of AI integration in the\\nfield of bibliometrics.\\n  Originality/value: This study covers the particularly new benefits and\\npotential of AI-enhanced scientometrics, webometrics, and bibliometrics to\\nhighlight the significant prospects of the synergy of this integration through\\nAI.', comment='to be published in Library High Tech; 30 pages; 80 references; 4\\n  figures; 3 tables', journal_ref=None, doi='10.1108/LHT-10-2023-0514', primary_category='cs.DL', categories=['cs.DL', 'cs.AI', 'physics.soc-ph'], links=[arxiv.Result.Link('http://dx.doi.org/10.1108/LHT-10-2023-0514', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2403.18838v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2403.18838v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses the application of AI, which includes LLMs, in enhancing methods of research analysis and scholarly communication, aligning with the goal of improving human research and learning capabilities.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2402.08565v2', updated=datetime.datetime(2024, 8, 6, 15, 40, 4, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 2, 13, 16, 5, 51, tzinfo=datetime.timezone.utc), title='Artificial Intelligence for Literature Reviews: Opportunities and Challenges', authors=[arxiv.Result.Author('Francisco Bolanos'), arxiv.Result.Author('Angelo Salatino'), arxiv.Result.Author('Francesco Osborne'), arxiv.Result.Author('Enrico Motta')], summary='This manuscript presents a comprehensive review of the use of Artificial\\nIntelligence (AI) in Systematic Literature Reviews (SLRs). A SLR is a rigorous\\nand organised methodology that assesses and integrates previous research on a\\ngiven topic. Numerous tools have been developed to assist and partially\\nautomate the SLR process. The increasing role of AI in this field shows great\\npotential in providing more effective support for researchers, moving towards\\nthe semi-automatic creation of literature reviews. Our study focuses on how AI\\ntechniques are applied in the semi-automation of SLRs, specifically in the\\nscreening and extraction phases. We examine 21 leading SLR tools using a\\nframework that combines 23 traditional features with 11 AI features. We also\\nanalyse 11 recent tools that leverage large language models for searching the\\nliterature and assisting academic writing. Finally, the paper discusses current\\ntrends in the field, outlines key research challenges, and suggests directions\\nfor future research.', comment='Updated with the reviewers comments. This version is now accepted at\\n  the Artificial Intelligence Review journal', journal_ref=None, doi=None, primary_category='cs.AI', categories=['cs.AI', 'cs.HC', 'cs.IR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2402.08565v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2402.08565v2', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses how AI and large language models enhance the process of systematic literature reviews, directly connecting to the topic of improving research and learning capabilities.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2408.11043v1', updated=datetime.datetime(2024, 8, 20, 17, 49, 51, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 8, 20, 17, 49, 51, tzinfo=datetime.timezone.utc), title='Reconciling Methodological Paradigms: Employing Large Language Models as Novice Qualitative Research Assistants in Talent Management Research', authors=[arxiv.Result.Author('Sreyoshi Bhaduri'), arxiv.Result.Author('Satya Kapoor'), arxiv.Result.Author('Alex Gil'), arxiv.Result.Author('Anshul Mittal'), arxiv.Result.Author('Rutu Mulkar')], summary='Qualitative data collection and analysis approaches, such as those employing\\ninterviews and focus groups, provide rich insights into customer attitudes,\\nsentiment, and behavior. However, manually analyzing qualitative data requires\\nextensive time and effort to identify relevant topics and thematic insights.\\nThis study proposes a novel approach to address this challenge by leveraging\\nRetrieval Augmented Generation (RAG) based Large Language Models (LLMs) for\\nanalyzing interview transcripts. The novelty of this work lies in strategizing\\nthe research inquiry as one that is augmented by an LLM that serves as a novice\\nresearch assistant. This research explores the mental model of LLMs to serve as\\nnovice qualitative research assistants for researchers in the talent management\\nspace. A RAG-based LLM approach is extended to enable topic modeling of\\nsemi-structured interview data, showcasing the versatility of these models\\nbeyond their traditional use in information retrieval and search. Our findings\\ndemonstrate that the LLM-augmented RAG approach can successfully extract topics\\nof interest, with significant coverage compared to manually generated topics\\nfrom the same dataset. This establishes the viability of employing LLMs as\\nnovice qualitative research assistants. Additionally, the study recommends that\\nresearchers leveraging such models lean heavily on quality criteria used in\\ntraditional qualitative research to ensure rigor and trustworthiness of their\\napproach. Finally, the paper presents key recommendations for industry\\npractitioners seeking to reconcile the use of LLMs with established qualitative\\nresearch paradigms, providing a roadmap for the effective integration of these\\npowerful, albeit novice, AI tools in the analysis of qualitative datasets\\nwithin talent', comment=\"Accepted to KDD '24 workshop on Talent Management and Computing (TMC\\n  2024). 9 pages\", journal_ref=None, doi=None, primary_category='cs.CY', categories=['cs.CY', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2408.11043v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2408.11043v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses the use of LLMs to enhance qualitative research analysis, aligning directly with the topic of using LLMs to improve human research and learning capabilities.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2401.01519v3', updated=datetime.datetime(2024, 3, 16, 13, 37, 48, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 1, 3, 3, 1, 29, tzinfo=datetime.timezone.utc), title='Exploring the Frontiers of LLMs in Psychological Applications: A Comprehensive Review', authors=[arxiv.Result.Author('Luoma Ke'), arxiv.Result.Author('Song Tong'), arxiv.Result.Author('Peng Cheng'), arxiv.Result.Author('Kaiping Peng')], summary=\"This paper explores the frontiers of large language models (LLMs) in\\npsychology applications. Psychology has undergone several theoretical changes,\\nand the current use of Artificial Intelligence (AI) and Machine Learning,\\nparticularly LLMs, promises to open up new research directions. We provide a\\ndetailed exploration of how LLMs like ChatGPT are transforming psychological\\nresearch. It discusses the impact of LLMs across various branches of\\npsychology, including cognitive and behavioral, clinical and counseling,\\neducational and developmental, and social and cultural psychology, highlighting\\ntheir potential to simulate aspects of human cognition and behavior. The paper\\ndelves into the capabilities of these models to emulate human-like text\\ngeneration, offering innovative tools for literature review, hypothesis\\ngeneration, experimental design, experimental subjects, data analysis, academic\\nwriting, and peer review in psychology. While LLMs are essential in advancing\\nresearch methodologies in psychology, the paper also cautions about their\\ntechnical and ethical challenges. There are issues like data privacy, the\\nethical implications of using LLMs in psychological research, and the need for\\na deeper understanding of these models' limitations. Researchers should\\nresponsibly use LLMs in psychological studies, adhering to ethical standards\\nand considering the potential consequences of deploying these technologies in\\nsensitive areas. Overall, the article provides a comprehensive overview of the\\ncurrent state of LLMs in psychology, exploring potential benefits and\\nchallenges. It serves as a call to action for researchers to leverage LLMs'\\nadvantages responsibly while addressing associated risks.\", comment=None, journal_ref=None, doi=None, primary_category='cs.LG', categories=['cs.LG', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2401.01519v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2401.01519v3', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses the transformative role of LLMs in enhancing research capabilities, particularly in psychology, which directly relates to using LLMs for improving human research and learning.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2404.05752v1', updated=datetime.datetime(2024, 4, 5, 3, 52, 27, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 4, 5, 3, 52, 27, tzinfo=datetime.timezone.utc), title='Physics Event Classification Using Large Language Models', authors=[arxiv.Result.Author('Cristiano Fanelli'), arxiv.Result.Author('James Giroux'), arxiv.Result.Author('Patrick Moran'), arxiv.Result.Author('Hemalata Nayak'), arxiv.Result.Author('Karthik Suresh'), arxiv.Result.Author('Eric Walter')], summary='The 2023 AI4EIC hackathon was the culmination of the third annual AI4EIC\\nworkshop at The Catholic University of America. This workshop brought together\\nresearchers from physics, data science and computer science to discuss the\\nlatest developments in Artificial Intelligence (AI) and Machine Learning (ML)\\nfor the Electron Ion Collider (EIC), including applications for detectors,\\naccelerators, and experimental control. The hackathon, held on the final day of\\nthe workshop, involved using a chatbot powered by a Large Language Model,\\nChatGPT-3.5, to train a binary classifier neutrons and photons in simulated\\ndata from the \\\\textsc{GlueX} Barrel Calorimeter. In total, six teams of up to\\nfour participants from all over the world took part in this intense educational\\nand research event. This article highlights the hackathon challenge, the\\nresources and methodology used, and the results and insights gained from\\nanalyzing physics data using the most cutting-edge tools in AI/ML.', comment='10 pages, 4 figures', journal_ref=None, doi=None, primary_category='physics.data-an', categories=['physics.data-an', 'cs.LG', 'hep-ex'], links=[arxiv.Result.Link('http://arxiv.org/abs/2404.05752v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2404.05752v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses the application of a Large Language Model in an educational and research context, which aligns with the topic of enhancing human research and learning capabilities.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2405.04687v1', updated=datetime.datetime(2024, 5, 7, 21, 59, 57, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 5, 7, 21, 59, 57, tzinfo=datetime.timezone.utc), title='Towards Human-AI Mutual Learning: A New Research Paradigm', authors=[arxiv.Result.Author('Xiaomei Wang'), arxiv.Result.Author('Xiaoyu Chen')], summary='This paper describes a new research paradigm for studying human-AI\\ncollaboration, named \"human-AI mutual learning\", defined as the process where\\nhumans and AI agents preserve, exchange, and improve knowledge during human-AI\\ncollaboration. We describe relevant methodologies, motivations, domain\\nexamples, benefits, challenges, and future research agenda under this paradigm.', comment=None, journal_ref=None, doi=None, primary_category='cs.HC', categories=['cs.HC', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2405.04687v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2405.04687v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses human-AI collaboration, which is directly relevant to how LLMs can enhance human research and learning processes.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2406.11861v1', updated=datetime.datetime(2024, 4, 10, 7, 45, 29, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 4, 10, 7, 45, 29, tzinfo=datetime.timezone.utc), title='Digital Transformation of Education, Systems Approach and Applied Research', authors=[arxiv.Result.Author('Elie Allouche')], summary=\"This article proposes the construction of a systemic model of digital\\neducation as part of research applied to public policy (French Ministry of\\nEducation). Considering the digital domain in its pervasiveness, it highlights\\nthe importance of a complex approach to understanding the transformation of\\npractices. As an applied research modality, we present digital theme groups\\n(GTnum). The methodological approach combines a reflexive posture informed by\\nresearch contributions, conceptual choices centered on digital humanities and\\nthe systems approach, participatory research and open science via the\\nHypotheses ''Education, digital and research'' notebook. As a result, our\\nmodeling is centered on a ''digital environment'' and six units of action put\\nto the test via the GTnum themes. We interpret these results through a\\ncomparison with other systemic frameworks, an application to the axes of\\ndigital transformation in academies, a prospective reflection with the\\ndevelopment of generative AI and perspectives for participatory research.\\nFinally, the article discusses the limits and contributions of this approach:\\nvariability in the understanding of the issues at stake and in the integration\\nof research contributions, as well as avenues for anticipating a new digital\\nconfiguration with the place of AI.\", comment='in French language', journal_ref=None, doi=None, primary_category='cs.CY', categories=['cs.CY'], links=[arxiv.Result.Link('http://arxiv.org/abs/2406.11861v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2406.11861v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses a systemic model of digital education and the integration of AI, which aligns with enhancing research and learning capabilities.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2408.01556v1', updated=datetime.datetime(2024, 8, 2, 20, 5, 24, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 8, 2, 20, 5, 24, tzinfo=datetime.timezone.utc), title='pathfinder: A Semantic Framework for Literature Review and Knowledge Discovery in Astronomy', authors=[arxiv.Result.Author('Kartheik G. Iyer'), arxiv.Result.Author('Mikaeel Yunus'), arxiv.Result.Author(\"Charles O'Neill\"), arxiv.Result.Author('Christine Ye'), arxiv.Result.Author('Alina Hyk'), arxiv.Result.Author('Kiera McCormick'), arxiv.Result.Author('Ioana Ciuca'), arxiv.Result.Author('John F. Wu'), arxiv.Result.Author('Alberto Accomazzi'), arxiv.Result.Author('Simone Astarita'), arxiv.Result.Author('Rishabh Chakrabarty'), arxiv.Result.Author('Jesse Cranney'), arxiv.Result.Author('Anjalie Field'), arxiv.Result.Author('Tirthankar Ghosal'), arxiv.Result.Author('Michele Ginolfi'), arxiv.Result.Author('Marc Huertas-Company'), arxiv.Result.Author('Maja Jablonska'), arxiv.Result.Author('Sandor Kruk'), arxiv.Result.Author('Huiling Liu'), arxiv.Result.Author('Gabriel Marchidan'), arxiv.Result.Author('Rohit Mistry'), arxiv.Result.Author('J. P. Naiman'), arxiv.Result.Author('J. E. G. Peek'), arxiv.Result.Author('Mugdha Polimera'), arxiv.Result.Author('Sergio J. Rodriguez'), arxiv.Result.Author('Kevin Schawinski'), arxiv.Result.Author('Sanjib Sharma'), arxiv.Result.Author('Michael J. Smith'), arxiv.Result.Author('Yuan-Sen Ting'), arxiv.Result.Author('Mike Walmsley')], summary=\"The exponential growth of astronomical literature poses significant\\nchallenges for researchers navigating and synthesizing general insights or even\\ndomain-specific knowledge. We present Pathfinder, a machine learning framework\\ndesigned to enable literature review and knowledge discovery in astronomy,\\nfocusing on semantic searching with natural language instead of syntactic\\nsearches with keywords. Utilizing state-of-the-art large language models (LLMs)\\nand a corpus of 350,000 peer-reviewed papers from the Astrophysics Data System\\n(ADS), Pathfinder offers an innovative approach to scientific inquiry and\\nliterature exploration. Our framework couples advanced retrieval techniques\\nwith LLM-based synthesis to search astronomical literature by semantic context\\nas a complement to currently existing methods that use keywords or citation\\ngraphs. It addresses complexities of jargon, named entities, and temporal\\naspects through time-based and citation-based weighting schemes. We demonstrate\\nthe tool's versatility through case studies, showcasing its application in\\nvarious research scenarios. The system's performance is evaluated using custom\\nbenchmarks, including single-paper and multi-paper tasks. Beyond literature\\nreview, Pathfinder offers unique capabilities for reformatting answers in ways\\nthat are accessible to various audiences (e.g. in a different language or as\\nsimplified text), visualizing research landscapes, and tracking the impact of\\nobservatories and methodologies. This tool represents a significant advancement\\nin applying AI to astronomical research, aiding researchers at all career\\nstages in navigating modern astronomy literature.\", comment='25 pages, 9 figures, submitted to AAS jorunals. Comments are welcome,\\n  and the tools mentioned are available online at https://pfdr.app', journal_ref=None, doi=None, primary_category='astro-ph.IM', categories=['astro-ph.IM', 'cs.DL', 'cs.IR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2408.01556v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2408.01556v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses the use of large language models (LLMs) to enhance literature review and knowledge discovery, directly aligning with the research topic of improving human research and learning capabilities.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2405.13024v1', updated=datetime.datetime(2024, 5, 15, 15, 9, 41, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 5, 15, 15, 9, 41, tzinfo=datetime.timezone.utc), title='Intelligent Tutor: Leveraging ChatGPT and Microsoft Copilot Studio to Deliver a Generative AI Student Support and Feedback System within Teams', authors=[arxiv.Result.Author('Wei-Yu Chen')], summary=\"This study explores the integration of the ChatGPT API with GPT-4 model and\\nMicrosoft Copilot Studio on the Microsoft Teams platform to develop an\\nintelligent tutoring system. Designed to provide instant support to students,\\nthe system dynamically adjusts educational content in response to the learners'\\nprogress and feedback. Utilizing advancements in natural language processing\\nand machine learning, it interprets student inquiries, offers tailored\\nfeedback, and facilitates the educational journey. Initial implementation\\nhighlights the system's potential in boosting students' motivation and\\nengagement, while equipping educators with critical insights into the learning\\nprocess, thus promoting tailored educational experiences and enhancing\\ninstructional effectiveness.\", comment=None, journal_ref=None, doi=None, primary_category='cs.CL', categories=['cs.CL', 'cs.AI', 'cs.CY'], links=[arxiv.Result.Link('http://arxiv.org/abs/2405.13024v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2405.13024v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses the integration of LLMs in education, which aligns directly with enhancing human research and learning abilities.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2405.08008v2', updated=datetime.datetime(2024, 7, 10, 7, 59, 15, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 5, 9, 9, 24, 13, tzinfo=datetime.timezone.utc), title='Iris: An AI-Driven Virtual Tutor For Computer Science Education', authors=[arxiv.Result.Author('Patrick Bassner'), arxiv.Result.Author('Eduard Frankford'), arxiv.Result.Author('Stephan Krusche')], summary=\"Integrating AI-driven tools in higher education is an emerging area with\\ntransformative potential. This paper introduces Iris, a chat-based virtual\\ntutor integrated into the interactive learning platform Artemis that offers\\npersonalized, context-aware assistance in large-scale educational settings.\\nIris supports computer science students by guiding them through programming\\nexercises and is designed to act as a tutor in a didactically meaningful way.\\nIts calibrated assistance avoids revealing complete solutions, offering subtle\\nhints or counter-questions to foster independent problem-solving skills. For\\neach question, it issues multiple prompts in a Chain-of-Thought to\\nGPT-3.5-Turbo. The prompts include a tutor role description and examples of\\nmeaningful answers through few-shot learning. Iris employs contextual awareness\\nby accessing the problem statement, student code, and automated feedback to\\nprovide tailored advice.\\n  An empirical evaluation shows that students perceive Iris as effective\\nbecause it understands their questions, provides relevant support, and\\ncontributes to the learning process. While students consider Iris a valuable\\ntool for programming exercises and homework, they also feel confident solving\\nprogramming tasks in computer-based exams without Iris. The findings underscore\\nstudents' appreciation for Iris' immediate and personalized support, though\\nstudents predominantly view it as a complement to, rather than a replacement\\nfor, human tutors. Nevertheless, Iris creates a space for students to ask\\nquestions without being judged by others.\", comment='Published in Proceedings of the 2024 Innovation and Technology in\\n  Computer Science Education V. 1 (ITiCSE 2024), Pages 534 - 540, July 8--10,\\n  2024, Milan, Italy', journal_ref=None, doi='10.1145/3649217.3653543', primary_category='cs.HC', categories=['cs.HC', 'cs.AI'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/3649217.3653543', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2405.08008v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2405.08008v2', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper focuses on an AI-driven tutor designed to enhance learning in educational settings, directly aligning with the topic of using LLMs to improve research and learning capabilities.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2407.15012v1', updated=datetime.datetime(2024, 6, 15, 1, 5, 56, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 6, 15, 1, 5, 56, tzinfo=datetime.timezone.utc), title='Transformative Influence of LLM and AI Tools in Student Social Media Engagement: Analyzing Personalization, Communication Efficiency, and Collaborative Learning', authors=[arxiv.Result.Author('Masoud Bashiri'), arxiv.Result.Author('Kamran Kowsari')], summary=\"The advent of Large Language Models (LLMs) and Artificial Intelligence (AI)\\ntools has revolutionized various facets of our lives, particularly in the realm\\nof social media. For students, these advancements have unlocked unprecedented\\nopportunities for learning, collaboration, and personal growth. AI-driven\\napplications are transforming how students interact with social media, offering\\npersonalized content and recommendations, and enabling smarter, more efficient\\ncommunication. Recent studies utilizing data from UniversityCube underscore the\\nprofound impact of AI tools on students' academic and social experiences. These\\nstudies reveal that students engaging with AI-enhanced social media platforms\\nreport higher academic performance, enhanced critical thinking skills, and\\nincreased engagement in collaborative projects.\\n  Moreover, AI tools assist in filtering out distracting content, allowing\\nstudents to concentrate more on educational materials and pertinent\\ndiscussions. The integration of LLMs in social media has further facilitated\\nimproved peer-to-peer communication and mentorship opportunities. AI algorithms\\neffectively match students based on shared academic interests and career goals,\\nfostering a supportive and intellectually stimulating online community, thereby\\ncontributing to increased student satisfaction and retention rates.\\n  In this article, we delve into the data provided by UniversityCube to explore\\nhow LLMs and AI tools are specifically transforming social media for students.\\nThrough case studies and statistical analyses, we offer a comprehensive\\nunderstanding of the educational and social benefits these technologies offer.\\nOur exploration highlights the potential of AI-driven tools to create a more\\nenriched, efficient, and supportive educational environment for students in the\\ndigital age.\", comment=None, journal_ref=None, doi=None, primary_category='cs.CY', categories=['cs.CY', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2407.15012v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2407.15012v1', title='pdf', rel='related', content_type=None)]),\n",
       "  \"This paper explores how Large Language Models and AI enhance students' learning and research capabilities through improved social media interactions.\"),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2403.14668v1', updated=datetime.datetime(2024, 3, 4, 8, 14, 7, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 3, 4, 8, 14, 7, tzinfo=datetime.timezone.utc), title='Predicting Learning Performance with Large Language Models: A Study in Adult Literacy', authors=[arxiv.Result.Author('Liang Zhang'), arxiv.Result.Author('Jionghao Lin'), arxiv.Result.Author('Conrad Borchers'), arxiv.Result.Author('John Sabatini'), arxiv.Result.Author('John Hollander'), arxiv.Result.Author('Meng Cao'), arxiv.Result.Author('Xiangen Hu')], summary='Intelligent Tutoring Systems (ITSs) have significantly enhanced adult\\nliteracy training, a key factor for societal participation, employment\\nopportunities, and lifelong learning. Our study investigates the application of\\nadvanced AI models, including Large Language Models (LLMs) like GPT-4, for\\npredicting learning performance in adult literacy programs in ITSs. This\\nresearch is motivated by the potential of LLMs to predict learning performance\\nbased on its inherent reasoning and computational capabilities. By using\\nreading comprehension datasets from the ITS, AutoTutor, we evaluate the\\npredictive capabilities of GPT-4 versus traditional machine learning methods in\\npredicting learning performance through five-fold cross-validation techniques.\\nOur findings show that the GPT-4 presents the competitive predictive abilities\\nwith traditional machine learning methods such as Bayesian Knowledge Tracing,\\nPerformance Factor Analysis, Sparse Factor Analysis Lite (SPARFA-Lite), tensor\\nfactorization and eXtreme Gradient Boosting (XGBoost). While XGBoost (trained\\non local machine) outperforms GPT-4 in predictive accuracy, GPT-4-selected\\nXGBoost and its subsequent tuning on the GPT-4 platform demonstrates superior\\nperformance compared to local machine execution. Moreover, our investigation\\ninto hyper-parameter tuning by GPT-4 versus grid-search suggests comparable\\nperformance, albeit with less stability in the automated approach, using\\nXGBoost as the case study. Our study contributes to the field by highlighting\\nthe potential of integrating LLMs with traditional machine learning models to\\nenhance predictive accuracy and personalize adult literacy education, setting a\\nfoundation for future research in applying LLMs within ITSs.', comment='26TH International Conference on Human-Computer Interaction', journal_ref=None, doi=None, primary_category='cs.CY', categories=['cs.CY', 'cs.AI', 'cs.CL', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2403.14668v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2403.14668v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper investigates the application of LLMs, like GPT-4, in enhancing adult literacy training, which aligns with the research topic on utilizing LLMs to enhance learning.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2402.17236v1', updated=datetime.datetime(2024, 2, 27, 6, 9, 48, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 2, 27, 6, 9, 48, tzinfo=datetime.timezone.utc), title='A Review of Data Mining in Personalized Education: Current Trends and Future Prospects', authors=[arxiv.Result.Author('Zhang Xiong'), arxiv.Result.Author('Haoxuan Li'), arxiv.Result.Author('Zhuang Liu'), arxiv.Result.Author('Zhuofan Chen'), arxiv.Result.Author('Hao Zhou'), arxiv.Result.Author('Wenge Rong'), arxiv.Result.Author('Yuanxin Ouyang')], summary='Personalized education, tailored to individual student needs, leverages\\neducational technology and artificial intelligence (AI) in the digital age to\\nenhance learning effectiveness. The integration of AI in educational platforms\\nprovides insights into academic performance, learning preferences, and\\nbehaviors, optimizing the personal learning process. Driven by data mining\\ntechniques, it not only benefits students but also provides educators and\\ninstitutions with tools to craft customized learning experiences. To offer a\\ncomprehensive review of recent advancements in personalized educational data\\nmining, this paper focuses on four primary scenarios: educational\\nrecommendation, cognitive diagnosis, knowledge tracing, and learning analysis.\\nThis paper presents a structured taxonomy for each area, compiles commonly used\\ndatasets, and identifies future research directions, emphasizing the role of\\ndata mining in enhancing personalized education and paving the way for future\\nexploration and innovation.', comment='25 pages, 5 figures', journal_ref='Frontiers of Digital Education, 2024 ,1(1): 26-50', doi='10.3868/s110-009-024-0004-9', primary_category='cs.CY', categories=['cs.CY', 'cs.LG'], links=[arxiv.Result.Link('http://dx.doi.org/10.3868/s110-009-024-0004-9', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2402.17236v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2402.17236v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses the integration of AI and data mining in personalized education, which aligns with enhancing human abilities to research and learn.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2403.14654v1', updated=datetime.datetime(2024, 2, 26, 2, 59, 7, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 2, 26, 2, 59, 7, tzinfo=datetime.timezone.utc), title='ChatGPT in Veterinary Medicine: A Practical Guidance of Generative Artificial Intelligence in Clinics, Education, and Research', authors=[arxiv.Result.Author('Candice P. Chu')], summary='ChatGPT, the most accessible generative artificial intelligence (AI) tool,\\noffers considerable potential for veterinary medicine, yet a dedicated review\\nof its specific applications is lacking. This review concisely synthesizes the\\nlatest research and practical applications of ChatGPT within the clinical,\\neducational, and research domains of veterinary medicine. It intends to provide\\nspecific guidance and actionable examples of how generative AI can be directly\\nutilized by veterinary professionals without a programming background. For\\npractitioners, ChatGPT can extract patient data, generate progress notes, and\\npotentially assist in diagnosing complex cases. Veterinary educators can create\\ncustom GPTs for student support, while students can utilize ChatGPT for exam\\npreparation. ChatGPT can aid in academic writing tasks in research, but\\nveterinary publishers have set specific requirements for authors to follow.\\nDespite its transformative potential, careful use is essential to avoid\\npitfalls like hallucination. This review addresses ethical considerations,\\nprovides learning resources, and offers tangible examples to guide responsible\\nimplementation. Carefully selected, up-to-date links to platforms that host\\nlarge language models are provided for advanced readers with programming\\ncapability. A table of key takeaways was provided to summarize this review. By\\nhighlighting potential benefits and limitations, this review equips\\nveterinarians, educators, and researchers to harness the power of ChatGPT\\neffectively.', comment=None, journal_ref=None, doi='10.3389/fvets.2024.1395934', primary_category='cs.CY', categories=['cs.CY'], links=[arxiv.Result.Link('http://dx.doi.org/10.3389/fvets.2024.1395934', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2403.14654v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2403.14654v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses the applications of ChatGPT in enhancing educational and research capabilities for veterinary professionals, which aligns with the topic of using LLMs to improve research and learning.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2402.01647v1', updated=datetime.datetime(2024, 1, 6, 8, 3, 8, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 1, 6, 8, 3, 8, tzinfo=datetime.timezone.utc), title='Build Your Own Robot Friend: An Open-Source Learning Module for Accessible and Engaging AI Education', authors=[arxiv.Result.Author('Zhonghao Shi'), arxiv.Result.Author(\"Allison O'Connell\"), arxiv.Result.Author('Zongjian Li'), arxiv.Result.Author('Siqi Liu'), arxiv.Result.Author('Jennifer Ayissi'), arxiv.Result.Author('Guy Hoffman'), arxiv.Result.Author('Mohammad Soleymani'), arxiv.Result.Author('Maja J. Matarić')], summary='As artificial intelligence (AI) is playing an increasingly important role in\\nour society and global economy, AI education and literacy have become necessary\\ncomponents in college and K-12 education to prepare students for an AI-powered\\nsociety. However, current AI curricula have not yet been made accessible and\\nengaging enough for students and schools from all socio-economic backgrounds\\nwith different educational goals. In this work, we developed an open-source\\nlearning module for college and high school students, which allows students to\\nbuild their own robot companion from the ground up. This open platform can be\\nused to provide hands-on experience and introductory knowledge about various\\naspects of AI, including robotics, machine learning (ML), software engineering,\\nand mechanical engineering. Because of the social and personal nature of a\\nsocially assistive robot companion, this module also puts a special emphasis on\\nhuman-centered AI, enabling students to develop a better understanding of\\nhuman-AI interaction and AI ethics through hands-on learning activities. With\\nopen-source documentation, assembling manuals and affordable materials,\\nstudents from different socio-economic backgrounds can personalize their\\nlearning experience based on their individual educational goals. To evaluate\\nthe student-perceived quality of our module, we conducted a usability testing\\nworkshop with 15 college students recruited from a minority-serving\\ninstitution. Our results indicate that our AI module is effective,\\neasy-to-follow, and engaging, and it increases student interest in studying\\nAI/ML and robotics in the future. We hope that this work will contribute toward\\naccessible and engaging AI education in human-AI interaction for college and\\nhigh school students.', comment='Accepted to the Proceedings of the AAAI Conference on Artificial\\n  Intelligence (2024)', journal_ref=None, doi=None, primary_category='cs.CY', categories=['cs.CY', 'cs.AI', 'cs.HC', 'cs.LG', 'cs.RO'], links=[arxiv.Result.Link('http://arxiv.org/abs/2402.01647v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2402.01647v1', title='pdf', rel='related', content_type=None)]),\n",
       "  \"The paper discusses an educational module that enhances students' understanding of AI and human-AI interaction, which aligns with using LLMs to improve research and learning capabilities.\"),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2407.17007v1', updated=datetime.datetime(2024, 7, 24, 5, 7, 53, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 7, 24, 5, 7, 53, tzinfo=datetime.timezone.utc), title='Pensieve Discuss: Scalable Small-Group CS Tutoring System with AI', authors=[arxiv.Result.Author('Yoonseok Yang'), arxiv.Result.Author('Jack Liu'), arxiv.Result.Author('J. D. Zamfirescu-Pereira'), arxiv.Result.Author('John DeNero')], summary=\"Small-group tutoring in Computer Science (CS) is effective, but presents the\\nchallenge of providing a dedicated tutor for each group and encouraging\\ncollaboration among group members at scale. We present Pensieve Discuss, a\\nsoftware platform that integrates synchronous editing for scaffolded\\nprogramming problems with online human and AI tutors, designed to improve\\nstudent collaboration and experience during group tutoring sessions. Our\\nsemester-long deployment to 800 students in a CS1 course demonstrated\\nconsistently high collaboration rates, positive feedback about the AI tutor's\\nhelpfulness and correctness, increased satisfaction with the group tutoring\\nexperience, and a substantial increase in question volume. The use of our\\nsystem was preferred over an interface lacking AI tutors and synchronous\\nediting capabilities. Our experiences suggest that small-group tutoring\\nsessions are an important avenue for future research in educational AI.\", comment='6 pages, 7 figures, 4 tables, 1 page of references', journal_ref=None, doi=None, primary_category='cs.CY', categories=['cs.CY', 'cs.AI', 'cs.HC'], links=[arxiv.Result.Link('http://arxiv.org/abs/2407.17007v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2407.17007v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses the use of AI tutors in enhancing collaboration and learning in a tutoring environment, directly aligning with the topic of LLMs improving research and learning abilities.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2404.07883v1', updated=datetime.datetime(2024, 4, 11, 16, 14, 23, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 4, 11, 16, 14, 23, tzinfo=datetime.timezone.utc), title='Apprentice Tutor Builder: A Platform For Users to Create and Personalize Intelligent Tutors', authors=[arxiv.Result.Author('Glen Smith'), arxiv.Result.Author('Adit Gupta'), arxiv.Result.Author('Christopher MacLellan')], summary=\"Intelligent tutoring systems (ITS) are effective for improving students'\\nlearning outcomes. However, their development is often complex, time-consuming,\\nand requires specialized programming and tutor design knowledge, thus hindering\\ntheir widespread application and personalization. We present the Apprentice\\nTutor Builder (ATB) , a platform that simplifies tutor creation and\\npersonalization. Instructors can utilize ATB's drag-and-drop tool to build\\ntutor interfaces. Instructors can then interactively train the tutors'\\nunderlying AI agent to produce expert models that can solve problems. Training\\nis achieved via using multiple interaction modalities including demonstrations,\\nfeedback, and user labels. We conducted a user study with 14 instructors to\\nevaluate the effectiveness of ATB's design with end users. We found that users\\nenjoyed the flexibility of the interface builder and ease and speed of agent\\nteaching, but often desired additional time-saving features. With these\\ninsights, we identified a set of design recommendations for our platform and\\nothers that utilize interactive AI agents for tutor creation and customization.\", comment=None, journal_ref=None, doi=None, primary_category='cs.HC', categories=['cs.HC', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2404.07883v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2404.07883v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses a platform for creating intelligent tutoring systems, which aligns with enhancing human learning and research abilities through AI.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2402.04975v1', updated=datetime.datetime(2024, 2, 7, 15, 55, 51, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 2, 7, 15, 55, 51, tzinfo=datetime.timezone.utc), title='ChatScratch: An AI-Augmented System Toward Autonomous Visual Programming Learning for Children Aged 6-12', authors=[arxiv.Result.Author('Liuqing Chen'), arxiv.Result.Author('Shuhong Xiao'), arxiv.Result.Author('Yunnong Chen'), arxiv.Result.Author('Ruoyu Wu'), arxiv.Result.Author('Yaxuan Song'), arxiv.Result.Author('Lingyun Sun')], summary=\"As Computational Thinking (CT) continues to permeate younger age groups in\\nK-12 education, established CT platforms such as Scratch face challenges in\\ncatering to these younger learners, particularly those in the elementary school\\n(ages 6-12). Through formative investigation with Scratch experts, we uncover\\nthree key obstacles to children's autonomous Scratch learning: artist's block\\nin project planning, bounded creativity in asset creation, and inadequate\\ncoding guidance during implementation. To address these barriers, we introduce\\nChatScratch, an AI-augmented system to facilitate autonomous programming\\nlearning for young children. ChatScratch employs structured interactive\\nstoryboards and visual cues to overcome artist's block, integrates digital\\ndrawing and advanced image generation technologies to elevate creativity, and\\nleverages Scratch-specialized Large Language Models (LLMs) for professional\\ncoding guidance. Our study shows that, compared to Scratch, ChatScratch\\nefficiently fosters autonomous programming learning, and contributes to the\\ncreation of high-quality, personally meaningful Scratch projects for children.\", comment='29 pages, 7 figures, accepted by CHI 2024', journal_ref=None, doi='10.1145/3613904.3642229', primary_category='cs.HC', categories=['cs.HC', 'cs.AI', 'cs.PL'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/3613904.3642229', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2402.04975v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2402.04975v1', title='pdf', rel='related', content_type=None)]),\n",
       "  \"The paper discusses the use of LLMs in an educational context to enhance children's programming skills, which is relevant to the topic of using LLMs to improve human learning and research capabilities.\"),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2406.14871v1', updated=datetime.datetime(2024, 6, 21, 5, 35, 57, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 6, 21, 5, 35, 57, tzinfo=datetime.timezone.utc), title=\"I don't trust you (anymore)! -- The effect of students' LLM use on Lecturer-Student-Trust in Higher Education\", authors=[arxiv.Result.Author('Simon Kloker'), arxiv.Result.Author('Matthew Bazanya'), arxiv.Result.Author('Twaha Kateete')], summary=\"Trust plays a pivotal role in Lecturer-Student-Collaboration, encompassing\\nteaching and research aspects. The advent of Large Language Models (LLMs) in\\nplatforms like Open AI's ChatGPT, coupled with their cost-effectiveness and\\nhigh-quality results, has led to their rapid adoption among university\\nstudents. However, discerning genuine student input from LLM-generated output\\nposes a challenge for lecturers. This dilemma jeopardizes the trust\\nrelationship between lecturers and students, potentially impacting university\\ndownstream activities, particularly collaborative research initiatives. Despite\\nattempts to establish guidelines for student LLM use, a clear framework\\nmutually beneficial for lecturers and students in higher education remains\\nelusive. This study addresses the research question: How does the use of LLMs\\nby students impact Informational and Procedural Justice, influencing Team Trust\\nand Expected Team Performance? Methodically, we applied a quantitative\\nconstruct-based survey, evaluated using techniques of Structural Equation\\nModelling (PLS- SEM) to examine potential relationships among these constructs.\\nOur findings based on 23 valid respondents from Ndejje University indicate that\\nlecturers are less concerned about the fairness of LLM use per se but are more\\nfocused on the transparency of student utilization, which significantly\\ninfluences Team Trust positively. This research contributes to the global\\ndiscourse on integrating and regulating LLMs and subsequent models in\\neducation. We propose that guidelines should support LLM use while enforcing\\ntransparency in Lecturer-Student- Collaboration to foster Team Trust and\\nPerformance. The study contributes valuable insights for shaping policies\\nenabling ethical and transparent LLMs usage in education to ensure\\neffectiveness of collaborative learning environments.\", comment='Working Paper', journal_ref=None, doi=None, primary_category='cs.CY', categories=['cs.CY', 'cs.AI', 'cs.ET', 'cs.HC', 'cs.LG', 'K.3.1; K.4.2; K.4.3; J.4; H.0; I.2.0'], links=[arxiv.Result.Link('http://arxiv.org/abs/2406.14871v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2406.14871v1', title='pdf', rel='related', content_type=None)]),\n",
       "  \"The paper discusses the impact of LLMs on teaching and research collaboration, aligning with the topic of enhancing human's ability to research and learn.\"),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2404.12944v1', updated=datetime.datetime(2024, 4, 19, 15, 21, 26, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 4, 19, 15, 21, 26, tzinfo=datetime.timezone.utc), title='Visualizing Intelligent Tutor Interactions for Responsive Pedagogy', authors=[arxiv.Result.Author('Grace Guo'), arxiv.Result.Author('Aishwarya Mudgal Sunil Kumar'), arxiv.Result.Author('Adit Gupta'), arxiv.Result.Author('Adam Coscia'), arxiv.Result.Author('Chris MacLellan'), arxiv.Result.Author('Alex Endert')], summary='Intelligent tutoring systems leverage AI models of expert learning and\\nstudent knowledge to deliver personalized tutoring to students. While these\\nintelligent tutors have demonstrated improved student learning outcomes, it is\\nstill unclear how teachers might integrate them into curriculum and course\\nplanning to support responsive pedagogy. In this paper, we conducted a design\\nstudy with five teachers who have deployed Apprentice Tutors, an intelligent\\ntutoring platform, in their classes. We characterized their challenges around\\nanalyzing student interaction data from intelligent tutoring systems and built\\nVisTA (Visualizations for Tutor Analytics), a visual analytics system that\\nshows detailed provenance data across multiple coordinated views. We evaluated\\nVisTA with the same five teachers, and found that the visualizations helped\\nthem better interpret intelligent tutor data, gain insights into student\\nproblem-solving provenance, and decide on necessary follow-up actions - such as\\nproviding students with further support or reviewing skills in the classroom.\\nFinally, we discuss potential extensions of VisTA into sequence query and\\ndetection, as well as the potential for the visualizations to be useful for\\nencouraging self-directed learning in students.', comment='9 pages, 5 figures, ACM AVI 2024', journal_ref=None, doi='10.1145/3656650.3656667', primary_category='cs.HC', categories=['cs.HC'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/3656650.3656667', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2404.12944v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2404.12944v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses the use of intelligent tutoring systems and visual analytics to support student learning, which aligns with the theme of enhancing human research and learning abilities through AI models.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2401.17434v2', updated=datetime.datetime(2024, 2, 1, 16, 59, 14, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 1, 30, 20, 45, 49, tzinfo=datetime.timezone.utc), title='Integrating Generative AI in Hackathons: Opportunities, Challenges, and Educational Implications', authors=[arxiv.Result.Author('Ramteja Sajja'), arxiv.Result.Author('Carlos Erazo Ramirez'), arxiv.Result.Author('Zhouyayan Li'), arxiv.Result.Author('Bekir Z. Demiray'), arxiv.Result.Author('Yusuf Sermet'), arxiv.Result.Author('Ibrahim Demir')], summary=\"Hackathons and software competitions, increasingly pivotal in the software\\nindustry, serve as vital catalysts for innovation and skill development for\\nboth organizations and students. These platforms enable companies to prototype\\nideas swiftly, while students gain enriched learning experiences, enhancing\\ntheir practical skills. Over the years, hackathons have transitioned from mere\\ncompetitive events to significant educational tools, fusing theoretical\\nknowledge with real-world problem-solving. The integration of hackathons into\\ncomputer science and software engineering curricula aims to align educational\\nproficiencies within a collaborative context, promoting peer connectivity and\\nenriched learning via industry-academia collaborations. However, the infusion\\nof advanced technologies, notably artificial intelligence (AI), and machine\\nlearning, into hackathons is revolutionizing their structure and outcomes. This\\nevolution brings forth both opportunities, like enhanced learning experiences,\\nand challenges, such as ethical concerns. This study delves into the impact of\\ngenerative AI, examining its influence on student's technological choices based\\non a case study on the University of Iowa 2023 event. The exploration provides\\ninsights into AI's role in hackathons, and its educational implications, and\\noffers a roadmap for the integration of such technologies in future events,\\nensuring innovation is balanced with ethical and educational considerations.\", comment='8491 words, 23 pages, 12 figures', journal_ref=None, doi=None, primary_category='cs.CY', categories=['cs.CY', 'cs.AI', 'cs.HC'], links=[arxiv.Result.Link('http://arxiv.org/abs/2401.17434v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2401.17434v2', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses the integration of generative AI in hackathons, focusing on its impact on learning and skill development, which aligns with enhancing human research and learning capabilities.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2405.15145v1', updated=datetime.datetime(2024, 5, 24, 1, 49, 2, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 5, 24, 1, 49, 2, tzinfo=datetime.timezone.utc), title='CulturePark: Boosting Cross-cultural Understanding in Large Language Models', authors=[arxiv.Result.Author('Cheng Li'), arxiv.Result.Author('Damien Teney'), arxiv.Result.Author('Linyi Yang'), arxiv.Result.Author('Qingsong Wen'), arxiv.Result.Author('Xing Xie'), arxiv.Result.Author('Jindong Wang')], summary=\"Cultural bias is pervasive in many large language models (LLMs), largely due\\nto the deficiency of data representative of different cultures. Typically,\\ncultural datasets and benchmarks are constructed either by extracting subsets\\nof existing datasets or by aggregating from platforms such as Wikipedia and\\nsocial media. However, these approaches are highly dependent on real-world data\\nand human annotations, making them costly and difficult to scale. Inspired by\\ncognitive theories on social communication, this paper introduces CulturePark,\\nan LLM-powered multi-agent communication framework for cultural data\\ncollection. CulturePark simulates cross-cultural human communication with\\nLLM-based agents playing roles in different cultures. It generates high-quality\\ncross-cultural dialogues encapsulating human beliefs, norms, and customs. Using\\nCulturePark, we generated 41,000 cultural samples to fine-tune eight\\nculture-specific LLMs. We evaluated these models across three downstream tasks:\\ncontent moderation, cultural alignment, and cultural education. Results show\\nthat for content moderation, our GPT-3.5-based models either match or\\noutperform GPT-4 on datasets. Regarding cultural alignment, our models surpass\\nGPT-4 on Hofstede's VSM 13 framework. Furthermore, for cultural education of\\nhuman participants, our models demonstrate superior outcomes in both learning\\nefficacy and user experience compared to GPT-4. CulturePark proves an important\\nstep in addressing cultural bias and advancing the democratization of AI,\\nhighlighting the critical role of culturally inclusive data in model training.\", comment='Technical report; 28 pages', journal_ref=None, doi=None, primary_category='cs.AI', categories=['cs.AI', 'cs.CL', 'cs.MA'], links=[arxiv.Result.Link('http://arxiv.org/abs/2405.15145v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2405.15145v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses an LLM framework aimed at enhancing cultural education and reducing bias, which directly relates to improving human research and learning abilities.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2405.00273v2', updated=datetime.datetime(2024, 7, 19, 20, 40, 23, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 5, 1, 1, 45, 50, tzinfo=datetime.timezone.utc), title='Social Life Simulation for Non-Cognitive Skills Learning', authors=[arxiv.Result.Author('Zihan Yan'), arxiv.Result.Author('Yaohong Xiang'), arxiv.Result.Author('Yun Huang')], summary='Non-cognitive skills are crucial for personal and social life well-being, and\\nsuch skill development can be supported by narrative-based (e.g., storytelling)\\ntechnologies. While generative AI enables interactive and role-playing\\nstorytelling, little is known about how users engage with and perceive the use\\nof AI in social life simulation for non-cognitive skills learning.\\nAdditionally, the benefits of AI mentorship on self-reflection awareness and\\nability in this context remain largely underexplored. To this end, we\\nintroduced Simulife++, an interactive platform enabled by a large language\\nmodel (LLM). The system allows users to act as protagonists, creating stories\\nwith one or multiple AI-based characters in diverse social scenarios. In\\nparticular, we expanded the Human-AI interaction to a Human-AI-AI collaboration\\nby including a Sage Agent, who acts as a bystander, providing users with some\\nperspectives and guidance on their choices and conversations in terms of\\nnon-cognitive skills to promote reflection. In a within-subject user study, our\\nquantitative results reveal that, when accompanied by Sage Agent, users exhibit\\nsignificantly higher levels of reflection on motivation, self-perceptions, and\\nresilience & coping, along with an enhanced experience of narrative\\ntransportation. Additionally, our qualitative findings suggest that Sage Agent\\nplays a crucial role in promoting reflection on non-cognitive skills, enhancing\\nsocial communication and decision-making performance, and improving overall\\nuser experience within Simulife++. Multiple supportive relationships between\\nSage Agent and users were also reported. We offer design implications for the\\napplication of generative AI in narrative solutions and the future potential of\\nSage Agent for non-cognitive skill development in broader social contexts.', comment=None, journal_ref=None, doi=None, primary_category='cs.CL', categories=['cs.CL', 'cs.HC'], links=[arxiv.Result.Link('http://arxiv.org/abs/2405.00273v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2405.00273v2', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses the use of LLMs in an interactive platform aimed at enhancing non-cognitive skills, which aligns with the theme of LLMs for improving human learning and research.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2404.02831v2', updated=datetime.datetime(2024, 7, 24, 20, 31, 52, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 4, 3, 16, 8, 1, tzinfo=datetime.timezone.utc), title='Empowering Biomedical Discovery with AI Agents', authors=[arxiv.Result.Author('Shanghua Gao'), arxiv.Result.Author('Ada Fang'), arxiv.Result.Author('Yepeng Huang'), arxiv.Result.Author('Valentina Giunchiglia'), arxiv.Result.Author('Ayush Noori'), arxiv.Result.Author('Jonathan Richard Schwarz'), arxiv.Result.Author('Yasha Ektefaie'), arxiv.Result.Author('Jovana Kondic'), arxiv.Result.Author('Marinka Zitnik')], summary='We envision \"AI scientists\" as systems capable of skeptical learning and\\nreasoning that empower biomedical research through collaborative agents that\\nintegrate AI models and biomedical tools with experimental platforms. Rather\\nthan taking humans out of the discovery process, biomedical AI agents combine\\nhuman creativity and expertise with AI\\'s ability to analyze large datasets,\\nnavigate hypothesis spaces, and execute repetitive tasks. AI agents are poised\\nto be proficient in various tasks, planning discovery workflows and performing\\nself-assessment to identify and mitigate gaps in their knowledge. These agents\\nuse large language models and generative models to feature structured memory\\nfor continual learning and use machine learning tools to incorporate scientific\\nknowledge, biological principles, and theories. AI agents can impact areas\\nranging from virtual cell simulation, programmable control of phenotypes, and\\nthe design of cellular circuits to developing new therapies.', comment=None, journal_ref=None, doi=None, primary_category='cs.AI', categories=['cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2404.02831v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2404.02831v2', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses the integration of AI models, including LLMs, to enhance biomedical research, aligning directly with the topic of using LLMs to improve human research and learning capabilities.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2402.07913v2', updated=datetime.datetime(2024, 2, 23, 2, 35, 41, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 1, 30, 13, 11, 23, tzinfo=datetime.timezone.utc), title='QACP: An Annotated Question Answering Dataset for Assisting Chinese Python Programming Learners', authors=[arxiv.Result.Author('Rui Xiao'), arxiv.Result.Author('Lu Han'), arxiv.Result.Author('Xiaoying Zhou'), arxiv.Result.Author('Jiong Wang'), arxiv.Result.Author('Na Zong'), arxiv.Result.Author('Pengyu Zhang')], summary=\"In online learning platforms, particularly in rapidly growing computer\\nprogramming courses, addressing the thousands of students' learning queries\\nrequires considerable human cost. The creation of intelligent assistant large\\nlanguage models (LLMs) tailored for programming education necessitates distinct\\ndata support. However, in real application scenarios, the data resources for\\ntraining such LLMs are relatively scarce. Therefore, to address the data\\nscarcity in intelligent educational systems for programming, this paper\\nproposes a new Chinese question-and-answer dataset for Python learners. To\\nensure the authenticity and reliability of the sources of the questions, we\\ncollected questions from actual student questions and categorized them\\naccording to various dimensions such as the type of questions and the type of\\nlearners. This annotation principle is designed to enhance the effectiveness\\nand quality of online programming education, providing a solid data foundation\\nfor developing the programming teaching assists (TA). Furthermore, we conducted\\ncomprehensive evaluations of various LLMs proficient in processing and\\ngenerating Chinese content, highlighting the potential limitations of general\\nLLMs as intelligent teaching assistants in computer programming courses.\", comment=None, journal_ref=None, doi=None, primary_category='cs.CL', categories=['cs.CL', 'cs.AI', 'cs.HC'], links=[arxiv.Result.Link('http://arxiv.org/abs/2402.07913v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2402.07913v2', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses the use of LLMs in enhancing programming education, which directly relates to the research topic on LLMs improving human research and learning abilities.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2406.01276v2', updated=datetime.datetime(2024, 6, 4, 11, 44, 59, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 6, 3, 12, 45, 40, tzinfo=datetime.timezone.utc), title='EduNLP: Towards a Unified and Modularized Library for Educational Resources', authors=[arxiv.Result.Author('Zhenya Huang'), arxiv.Result.Author('Yuting Ning'), arxiv.Result.Author('Longhu Qin'), arxiv.Result.Author('Shiwei Tong'), arxiv.Result.Author('Shangzi Xue'), arxiv.Result.Author('Tong Xiao'), arxiv.Result.Author('Xin Lin'), arxiv.Result.Author('Jiayu Liu'), arxiv.Result.Author('Qi Liu'), arxiv.Result.Author('Enhong Chen'), arxiv.Result.Author('Shijing Wang')], summary=\"Educational resource understanding is vital to online learning platforms,\\nwhich have demonstrated growing applications recently. However, researchers and\\ndevelopers always struggle with using existing general natural language\\ntoolkits or domain-specific models. The issue raises a need to develop an\\neffective and easy-to-use one that benefits AI education-related research and\\napplications. To bridge this gap, we present a unified, modularized, and\\nextensive library, EduNLP, focusing on educational resource understanding. In\\nthe library, we decouple the whole workflow to four key modules with consistent\\ninterfaces including data configuration, processing, model implementation, and\\nmodel evaluation. We also provide a configurable pipeline to unify the data\\nusage and model usage in standard ways, where users can customize their own\\nneeds. For the current version, we primarily provide 10 typical models from\\nfour categories, and 5 common downstream-evaluation tasks in the education\\ndomain on 8 subjects for users' usage. The project is released at:\\nhttps://github.com/bigdata-ustc/EduNLP.\", comment=None, journal_ref=None, doi=None, primary_category='cs.CL', categories=['cs.CL'], links=[arxiv.Result.Link('http://arxiv.org/abs/2406.01276v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2406.01276v2', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses a modularized library aimed at enhancing educational resource understanding, which aligns with the use of LLMs to improve research and learning capabilities.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2403.13179v1', updated=datetime.datetime(2024, 3, 19, 22, 19, 29, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 3, 19, 22, 19, 29, tzinfo=datetime.timezone.utc), title='Predictive, scalable and interpretable knowledge tracing on structured domains', authors=[arxiv.Result.Author('Hanqi Zhou'), arxiv.Result.Author('Robert Bamler'), arxiv.Result.Author('Charley M. Wu'), arxiv.Result.Author('Álvaro Tejero-Cantero')], summary=\"Intelligent tutoring systems optimize the selection and timing of learning\\nmaterials to enhance understanding and long-term retention. This requires\\nestimates of both the learner's progress (''knowledge tracing''; KT), and the\\nprerequisite structure of the learning domain (''knowledge mapping''). While\\nrecent deep learning models achieve high KT accuracy, they do so at the expense\\nof the interpretability of psychologically-inspired models. In this work, we\\npresent a solution to this trade-off. PSI-KT is a hierarchical generative\\napproach that explicitly models how both individual cognitive traits and the\\nprerequisite structure of knowledge influence learning dynamics, thus achieving\\ninterpretability by design. Moreover, by using scalable Bayesian inference,\\nPSI-KT targets the real-world need for efficient personalization even with a\\ngrowing body of learners and learning histories. Evaluated on three datasets\\nfrom online learning platforms, PSI-KT achieves superior multi-step predictive\\naccuracy and scalable inference in continual-learning settings, all while\\nproviding interpretable representations of learner-specific traits and the\\nprerequisite structure of knowledge that causally supports learning. In sum,\\npredictive, scalable and interpretable knowledge tracing with solid knowledge\\nmapping lays a key foundation for effective personalized learning to make\\neducation accessible to a broad, global audience.\", comment=None, journal_ref=None, doi=None, primary_category='cs.LG', categories=['cs.LG', 'cs.CY', 'stat.ML'], links=[arxiv.Result.Link('http://arxiv.org/abs/2403.13179v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2403.13179v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses intelligent tutoring systems and personalized learning, which align with the use of LLMs to enhance human research and learning capabilities.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2405.19941v1', updated=datetime.datetime(2024, 5, 30, 11, 2, 8, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 5, 30, 11, 2, 8, tzinfo=datetime.timezone.utc), title='Synthetic Patients: Simulating Difficult Conversations with Multimodal Generative AI for Medical Education', authors=[arxiv.Result.Author('Simon N. Chu'), arxiv.Result.Author('Alex J. Goodell')], summary='Problem: Effective patient-centered communication is a core competency for\\nphysicians. However, both seasoned providers and medical trainees report\\ndecreased confidence in leading conversations on sensitive topics such as goals\\nof care or end-of-life discussions. The significant administrative burden and\\nthe resources required to provide dedicated training in leading difficult\\nconversations has been a long-standing problem in medical education.\\n  Approach: In this work, we present a novel educational tool designed to\\nfacilitate interactive, real-time simulations of difficult conversations in a\\nvideo-based format through the use of multimodal generative artificial\\nintelligence (AI). Leveraging recent advances in language modeling, computer\\nvision, and generative audio, this tool creates realistic, interactive\\nscenarios with avatars, or \"synthetic patients.\" These synthetic patients\\ninteract with users throughout various stages of medical care using a\\ncustom-built video chat application, offering learners the chance to practice\\nconversations with patients from diverse belief systems, personalities, and\\nethnic backgrounds.\\n  Outcomes: While the development of this platform demanded substantial upfront\\ninvestment in labor, it offers a highly-realistic simulation experience with\\nminimal financial investment. For medical trainees, this educational tool can\\nbe implemented within programs to simulate patient-provider conversations and\\ncan be incorporated into existing palliative care curriculum to provide a\\nscalable, high-fidelity simulation environment for mastering difficult\\nconversations.\\n  Next Steps: Future developments will explore enhancing the authenticity of\\nthese encounters by working with patients to incorporate their histories and\\npersonalities, as well as employing the use of AI-generated evaluations to\\noffer immediate, constructive feedback to learners post-simulation.', comment=None, journal_ref=None, doi=None, primary_category='cs.HC', categories=['cs.HC', 'cs.CY'], links=[arxiv.Result.Link('http://arxiv.org/abs/2405.19941v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2405.19941v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper explores the use of generative AI and multimodal language models to enhance training and learning in a medical education context, directly aligning with the research topic of LLMs for improving research and learning capabilities.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2405.01858v1', updated=datetime.datetime(2024, 5, 3, 5, 19, 9, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 5, 3, 5, 19, 9, tzinfo=datetime.timezone.utc), title='SUKHSANDESH: An Avatar Therapeutic Question Answering Platform for Sexual Education in Rural India', authors=[arxiv.Result.Author('Salam Michael Singh'), arxiv.Result.Author('Shubhmoy Kumar Garg'), arxiv.Result.Author('Amitesh Misra'), arxiv.Result.Author('Aaditeshwar Seth'), arxiv.Result.Author('Tanmoy Chakraborty')], summary=\"Sexual education aims to foster a healthy lifestyle in terms of emotional,\\nmental and social well-being. In countries like India, where adolescents form\\nthe largest demographic group, they face significant vulnerabilities concerning\\nsexual health. Unfortunately, sexual education is often stigmatized, creating\\nbarriers to providing essential counseling and information to this at-risk\\npopulation. Consequently, issues such as early pregnancy, unsafe abortions,\\nsexually transmitted infections, and sexual violence become prevalent. Our\\ncurrent proposal aims to provide a safe and trustworthy platform for sexual\\neducation to the vulnerable rural Indian population, thereby fostering the\\nhealthy and overall growth of the nation. In this regard, we strive towards\\ndesigning SUKHSANDESH, a multi-staged AI-based Question Answering platform for\\nsexual education tailored to rural India, adhering to safety guardrails and\\nregional language support. By utilizing information retrieval techniques and\\nlarge language models, SUKHSANDESH will deliver effective responses to user\\nqueries. We also propose to anonymise the dataset to mitigate safety measures\\nand set AI guardrails against any harmful or unwanted response generation.\\nMoreover, an innovative feature of our proposal involves integrating ``avatar\\ntherapy'' with SUKHSANDESH. This feature will convert AI-generated responses\\ninto real-time audio delivered by an animated avatar speaking regional Indian\\nlanguages. This approach aims to foster empathy and connection, which is\\nparticularly beneficial for individuals with limited literacy skills.\\nPartnering with Gram Vaani, an industry leader, we will deploy SUKHSANDESH to\\naddress sexual education needs in rural India.\", comment=None, journal_ref=None, doi=None, primary_category='cs.CL', categories=['cs.CL', 'cs.CY'], links=[arxiv.Result.Link('http://arxiv.org/abs/2405.01858v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2405.01858v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses utilizing large language models (LLMs) to provide an AI-based platform for sexual education, which relates directly to enhancing human learning and research capabilities.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2401.11839v1', updated=datetime.datetime(2024, 1, 22, 10, 57, 9, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 1, 22, 10, 57, 9, tzinfo=datetime.timezone.utc), title='AI for social science and social science of AI: A Survey', authors=[arxiv.Result.Author('Ruoxi Xu'), arxiv.Result.Author('Yingfei Sun'), arxiv.Result.Author('Mengjie Ren'), arxiv.Result.Author('Shiguang Guo'), arxiv.Result.Author('Ruotong Pan'), arxiv.Result.Author('Hongyu Lin'), arxiv.Result.Author('Le Sun'), arxiv.Result.Author('Xianpei Han')], summary='Recent advancements in artificial intelligence, particularly with the\\nemergence of large language models (LLMs), have sparked a rethinking of\\nartificial general intelligence possibilities. The increasing human-like\\ncapabilities of AI are also attracting attention in social science research,\\nleading to various studies exploring the combination of these two fields. In\\nthis survey, we systematically categorize previous explorations in the\\ncombination of AI and social science into two directions that share common\\ntechnical approaches but differ in their research objectives. The first\\ndirection is focused on AI for social science, where AI is utilized as a\\npowerful tool to enhance various stages of social science research. While the\\nsecond direction is the social science of AI, which examines AI agents as\\nsocial entities with their human-like cognitive and linguistic capabilities. By\\nconducting a thorough review, particularly on the substantial progress\\nfacilitated by recent advancements in large language models, this paper\\nintroduces a fresh perspective to reassess the relationship between AI and\\nsocial science, provides a cohesive framework that allows researchers to\\nunderstand the distinctions and connections between AI for social science and\\nsocial science of AI, and also summarized state-of-art experiment simulation\\nplatforms to facilitate research in these two directions. We believe that as AI\\ntechnology continues to advance and intelligent agents find increasing\\napplications in our daily lives, the significance of the combination of AI and\\nsocial science will become even more prominent.', comment='Accepted by Information Processing and Management (IP&M)', journal_ref=None, doi=None, primary_category='cs.CL', categories=['cs.CL', 'cs.CY'], links=[arxiv.Result.Link('http://arxiv.org/abs/2401.11839v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2401.11839v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses the application of large language models in enhancing social science research, which aligns with the topic of using LLMs to enhance human research and learning capabilities.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2407.15012v1', updated=datetime.datetime(2024, 6, 15, 1, 5, 56, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 6, 15, 1, 5, 56, tzinfo=datetime.timezone.utc), title='Transformative Influence of LLM and AI Tools in Student Social Media Engagement: Analyzing Personalization, Communication Efficiency, and Collaborative Learning', authors=[arxiv.Result.Author('Masoud Bashiri'), arxiv.Result.Author('Kamran Kowsari')], summary=\"The advent of Large Language Models (LLMs) and Artificial Intelligence (AI)\\ntools has revolutionized various facets of our lives, particularly in the realm\\nof social media. For students, these advancements have unlocked unprecedented\\nopportunities for learning, collaboration, and personal growth. AI-driven\\napplications are transforming how students interact with social media, offering\\npersonalized content and recommendations, and enabling smarter, more efficient\\ncommunication. Recent studies utilizing data from UniversityCube underscore the\\nprofound impact of AI tools on students' academic and social experiences. These\\nstudies reveal that students engaging with AI-enhanced social media platforms\\nreport higher academic performance, enhanced critical thinking skills, and\\nincreased engagement in collaborative projects.\\n  Moreover, AI tools assist in filtering out distracting content, allowing\\nstudents to concentrate more on educational materials and pertinent\\ndiscussions. The integration of LLMs in social media has further facilitated\\nimproved peer-to-peer communication and mentorship opportunities. AI algorithms\\neffectively match students based on shared academic interests and career goals,\\nfostering a supportive and intellectually stimulating online community, thereby\\ncontributing to increased student satisfaction and retention rates.\\n  In this article, we delve into the data provided by UniversityCube to explore\\nhow LLMs and AI tools are specifically transforming social media for students.\\nThrough case studies and statistical analyses, we offer a comprehensive\\nunderstanding of the educational and social benefits these technologies offer.\\nOur exploration highlights the potential of AI-driven tools to create a more\\nenriched, efficient, and supportive educational environment for students in the\\ndigital age.\", comment=None, journal_ref=None, doi=None, primary_category='cs.CY', categories=['cs.CY', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2407.15012v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2407.15012v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses how LLMs and AI tools enhance learning and collaboration among students, aligning directly with the focus on improving research and learning capabilities.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2405.00273v2', updated=datetime.datetime(2024, 7, 19, 20, 40, 23, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 5, 1, 1, 45, 50, tzinfo=datetime.timezone.utc), title='Social Life Simulation for Non-Cognitive Skills Learning', authors=[arxiv.Result.Author('Zihan Yan'), arxiv.Result.Author('Yaohong Xiang'), arxiv.Result.Author('Yun Huang')], summary='Non-cognitive skills are crucial for personal and social life well-being, and\\nsuch skill development can be supported by narrative-based (e.g., storytelling)\\ntechnologies. While generative AI enables interactive and role-playing\\nstorytelling, little is known about how users engage with and perceive the use\\nof AI in social life simulation for non-cognitive skills learning.\\nAdditionally, the benefits of AI mentorship on self-reflection awareness and\\nability in this context remain largely underexplored. To this end, we\\nintroduced Simulife++, an interactive platform enabled by a large language\\nmodel (LLM). The system allows users to act as protagonists, creating stories\\nwith one or multiple AI-based characters in diverse social scenarios. In\\nparticular, we expanded the Human-AI interaction to a Human-AI-AI collaboration\\nby including a Sage Agent, who acts as a bystander, providing users with some\\nperspectives and guidance on their choices and conversations in terms of\\nnon-cognitive skills to promote reflection. In a within-subject user study, our\\nquantitative results reveal that, when accompanied by Sage Agent, users exhibit\\nsignificantly higher levels of reflection on motivation, self-perceptions, and\\nresilience & coping, along with an enhanced experience of narrative\\ntransportation. Additionally, our qualitative findings suggest that Sage Agent\\nplays a crucial role in promoting reflection on non-cognitive skills, enhancing\\nsocial communication and decision-making performance, and improving overall\\nuser experience within Simulife++. Multiple supportive relationships between\\nSage Agent and users were also reported. We offer design implications for the\\napplication of generative AI in narrative solutions and the future potential of\\nSage Agent for non-cognitive skill development in broader social contexts.', comment=None, journal_ref=None, doi=None, primary_category='cs.CL', categories=['cs.CL', 'cs.HC'], links=[arxiv.Result.Link('http://arxiv.org/abs/2405.00273v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2405.00273v2', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses the use of LLMs to enhance human learning and self-reflection in the context of non-cognitive skill development.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2408.04394v1', updated=datetime.datetime(2024, 8, 8, 11, 56, 57, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 8, 8, 11, 56, 57, tzinfo=datetime.timezone.utc), title=\"Automated Educational Question Generation at Different Bloom's Skill Levels using Large Language Models: Strategies and Evaluation\", authors=[arxiv.Result.Author('Nicy Scaria'), arxiv.Result.Author('Suma Dharani Chenna'), arxiv.Result.Author('Deepak Subramani')], summary=\"Developing questions that are pedagogically sound, relevant, and promote\\nlearning is a challenging and time-consuming task for educators. Modern-day\\nlarge language models (LLMs) generate high-quality content across multiple\\ndomains, potentially helping educators to develop high-quality questions.\\nAutomated educational question generation (AEQG) is important in scaling online\\neducation catering to a diverse student population. Past attempts at AEQG have\\nshown limited abilities to generate questions at higher cognitive levels. In\\nthis study, we examine the ability of five state-of-the-art LLMs of different\\nsizes to generate diverse and high-quality questions of different cognitive\\nlevels, as defined by Bloom's taxonomy. We use advanced prompting techniques\\nwith varying complexity for AEQG. We conducted expert and LLM-based evaluations\\nto assess the linguistic and pedagogical relevance and quality of the\\nquestions. Our findings suggest that LLms can generate relevant and\\nhigh-quality educational questions of different cognitive levels when prompted\\nwith adequate information, although there is a significant variance in the\\nperformance of the five LLms considered. We also show that automated evaluation\\nis not on par with human evaluation.\", comment=None, journal_ref='Artificial Intelligence in Education. AIED 2024', doi='10.1007/978-3-031-64299-9_12', primary_category='cs.CL', categories=['cs.CL', 'cs.AI'], links=[arxiv.Result.Link('http://dx.doi.org/10.1007/978-3-031-64299-9_12', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2408.04394v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2408.04394v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper focuses on the use of LLMs to generate educational questions, which directly relates to enhancing research and learning capabilities.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2408.12496v1', updated=datetime.datetime(2024, 8, 22, 15, 41, 58, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 8, 22, 15, 41, 58, tzinfo=datetime.timezone.utc), title='MEDCO: Medical Education Copilots Based on A Multi-Agent Framework', authors=[arxiv.Result.Author('Hao Wei'), arxiv.Result.Author('Jianing Qiu'), arxiv.Result.Author('Haibao Yu'), arxiv.Result.Author('Wu Yuan')], summary='Large language models (LLMs) have had a significant impact on diverse\\nresearch domains, including medicine and healthcare. However, the potential of\\nLLMs as copilots in medical education remains underexplored. Current\\nAI-assisted educational tools are limited by their solitary learning approach\\nand inability to simulate the multi-disciplinary and interactive nature of\\nactual medical training. To address these limitations, we propose MEDCO\\n(Medical EDucation COpilots), a novel multi-agent-based copilot system\\nspecially developed to emulate real-world medical training environments. MEDCO\\nincorporates three primary agents: an agentic patient, an expert doctor, and a\\nradiologist, facilitating a multi-modal and interactive learning environment.\\nOur framework emphasizes the learning of proficient question-asking skills,\\nmulti-disciplinary collaboration, and peer discussions between students. Our\\nexperiments show that simulated virtual students who underwent training with\\nMEDCO not only achieved substantial performance enhancements comparable to\\nthose of advanced models, but also demonstrated human-like learning behaviors\\nand improvements, coupled with an increase in the number of learning samples.\\nThis work contributes to medical education by introducing a copilot that\\nimplements an interactive and collaborative learning approach. It also provides\\nvaluable insights into the effectiveness of AI-integrated training paradigms.', comment=None, journal_ref='ECCV 2024 Workshop', doi=None, primary_category='cs.AI', categories=['cs.AI', 'cs.MA'], links=[arxiv.Result.Link('http://arxiv.org/abs/2408.12496v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2408.12496v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses the use of LLMs in education, specifically in medical training, which aligns with the topic of enhancing human research and learning capabilities.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2407.09573v1', updated=datetime.datetime(2024, 7, 11, 16, 38, 40, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 7, 11, 16, 38, 40, tzinfo=datetime.timezone.utc), title='Have We Reached AGI? Comparing ChatGPT, Claude, and Gemini to Human Literacy and Education Benchmarks', authors=[arxiv.Result.Author('Mfon Akpan')], summary=\"Recent advancements in AI, particularly in large language models (LLMs) like\\nChatGPT, Claude, and Gemini, have prompted questions about their proximity to\\nArtificial General Intelligence (AGI). This study compares LLM performance on\\neducational benchmarks with Americans' average educational attainment and\\nliteracy levels, using data from the U.S. Census Bureau and technical reports.\\nResults show that LLMs significantly outperform human benchmarks in tasks such\\nas undergraduate knowledge and advanced reading comprehension, indicating\\nsubstantial progress toward AGI. However, true AGI requires broader cognitive\\nassessments. The study highlights the implications for AI development,\\neducation, and societal impact, emphasizing the need for ongoing research and\\nethical considerations.\", comment=None, journal_ref=None, doi=None, primary_category='cs.AI', categories=['cs.AI', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2407.09573v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2407.09573v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses the performance of LLMs in educational contexts, which directly relates to enhancing human research and learning capabilities.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2407.16444v1', updated=datetime.datetime(2024, 7, 23, 12, 53, 41, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 7, 23, 12, 53, 41, tzinfo=datetime.timezone.utc), title='Psychomatics -- A Multidisciplinary Framework for Understanding Artificial Minds', authors=[arxiv.Result.Author('Giuseppe Riva'), arxiv.Result.Author('Fabrizia Mantovani'), arxiv.Result.Author('Brenda K. Wiederhold'), arxiv.Result.Author('Antonella Marchetti'), arxiv.Result.Author('Andrea Gaggioli')], summary=\"Although LLMs and other artificial intelligence systems demonstrate cognitive\\nskills similar to humans, like concept learning and language acquisition, the\\nway they process information fundamentally differs from biological cognition.\\nTo better understand these differences this paper introduces Psychomatics, a\\nmultidisciplinary framework bridging cognitive science, linguistics, and\\ncomputer science. It aims to better understand the high-level functioning of\\nLLMs, focusing specifically on how LLMs acquire, learn, remember, and use\\ninformation to produce their outputs. To achieve this goal, Psychomatics will\\nrely on a comparative methodology, starting from a theory-driven research\\nquestion - is the process of language development and use different in humans\\nand LLMs? - drawing parallels between LLMs and biological systems. Our analysis\\nshows how LLMs can map and manipulate complex linguistic patterns in their\\ntraining data. Moreover, LLMs can follow Grice's Cooperative Principle to\\nprovide relevant and informative responses. However, human cognition draws from\\nmultiple sources of meaning, including experiential, emotional, and imaginative\\nfacets, which transcend mere language processing and are rooted in our social\\nand developmental trajectories. Moreover, current LLMs lack physical\\nembodiment, reducing their ability to make sense of the intricate interplay\\nbetween perception, action, and cognition that shapes human understanding and\\nexpression. Ultimately, Psychomatics holds the potential to yield\\ntransformative insights into the nature of language, cognition, and\\nintelligence, both artificial and biological. Moreover, by drawing parallels\\nbetween LLMs and human cognitive processes, Psychomatics can inform the\\ndevelopment of more robust and human-like AI systems.\", comment='15 pages, 4 tables, 2 figures', journal_ref=None, doi=None, primary_category='cs.AI', categories=['cs.AI', 'cs.CL'], links=[arxiv.Result.Link('http://arxiv.org/abs/2407.16444v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2407.16444v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses how LLMs process information and compares this to human cognition, which is directly relevant to enhancing human research and learning abilities.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2404.13414v3', updated=datetime.datetime(2024, 5, 3, 0, 28, 46, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 4, 20, 15, 58, 22, tzinfo=datetime.timezone.utc), title='Evaluating the Effectiveness of LLMs in Introductory Computer Science Education: A Semester-Long Field Study', authors=[arxiv.Result.Author('Wenhan Lyu'), arxiv.Result.Author('Yimeng Wang'), arxiv.Result.Author('Tingting'), arxiv.Result.Author('Chung'), arxiv.Result.Author('Yifan Sun'), arxiv.Result.Author('Yixuan Zhang')], summary=\"The integration of AI assistants, especially through the development of Large\\nLanguage Models (LLMs), into computer science education has sparked significant\\ndebate. An emerging body of work has looked into using LLMs in education, but\\nfew have examined the impacts of LLMs on students in entry-level programming\\ncourses, particularly in real-world contexts and over extended periods. To\\naddress this research gap, we conducted a semester-long, between-subjects study\\nwith 50 students using CodeTutor, an LLM-powered assistant developed by our\\nresearch team. Our study results show that students who used CodeTutor (the\\nexperimental group) achieved statistically significant improvements in their\\nfinal scores compared to peers who did not use the tool (the control group).\\nWithin the experimental group, those without prior experience with LLM-powered\\ntools demonstrated significantly greater performance gain than their\\ncounterparts. We also found that students expressed positive feedback regarding\\nCodeTutor's capability, though they also had concerns about CodeTutor's limited\\nrole in developing critical thinking skills. Over the semester, students'\\nagreement with CodeTutor's suggestions decreased, with a growing preference for\\nsupport from traditional human teaching assistants. Our analysis further\\nreveals that the quality of user prompts was significantly correlated with\\nCodeTutor's response effectiveness. Building upon our results, we discuss the\\nimplications of our findings for integrating Generative AI literacy into\\ncurricula to foster critical thinking skills and turn to examining the temporal\\ndynamics of user engagement with LLM-powered tools. We further discuss the\\ndiscrepancy between the anticipated functions of tools and students' actual\\ncapabilities, which sheds light on the need for tailored strategies to improve\\neducational outcomes.\", comment='Accepted to Learning @ Scale 2024', journal_ref=None, doi='10.1145/3657604.3662036', primary_category='cs.HC', categories=['cs.HC'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/3657604.3662036', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2404.13414v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2404.13414v3', title='pdf', rel='related', content_type=None)]),\n",
       "  'This paper directly examines the impact of LLMs on learning outcomes in computer science education, making it highly relevant to the topic of enhancing research and learning abilities using LLMs.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2407.13787v2', updated=datetime.datetime(2024, 7, 24, 4, 57, 55, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 7, 12, 11, 31, tzinfo=datetime.timezone.utc), title='The Honorific Effect: Exploring the Impact of Japanese Linguistic Formalities on AI-Generated Physics Explanations', authors=[arxiv.Result.Author('Keisuke Sato')], summary=\"This study investigates the influence of Japanese honorifics on the responses\\nof large language models (LLMs) when explaining the law of conservation of\\nmomentum. We analyzed the outputs of six state-of-the-art AI models, including\\nvariations of ChatGPT, Coral, and Gemini, using 14 different honorific forms.\\nOur findings reveal that honorifics significantly affect the quality,\\nconsistency, and formality of AI-generated responses, demonstrating LLMs'\\nability to interpret and adapt to social context cues embedded in language.\\nNotable variations were observed across different models, with some emphasizing\\nhistorical context and derivations, while others focused on intuitive\\nexplanations. The study highlights the potential for using honorifics to adjust\\nthe depth and complexity of AI-generated explanations in educational contexts.\\nFurthermore, the responsiveness of AI models to cultural linguistic elements\\nunderscores the importance of considering cultural factors in AI development\\nfor educational applications. These results open new avenues for research in\\nAI-assisted education and cultural adaptation in AI systems, with significant\\nimplications for personalizing learning experiences and developing culturally\\nsensitive AI tools for global education.\", comment=None, journal_ref=None, doi=None, primary_category='physics.ed-ph', categories=['physics.ed-ph', 'cs.CL'], links=[arxiv.Result.Link('http://arxiv.org/abs/2407.13787v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2407.13787v2', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses how LLMs adapt to social context and cultural factors, which is relevant to enhancing human research and learning capabilities.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2402.01812v1', updated=datetime.datetime(2024, 2, 2, 13, 23, 15, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 2, 2, 13, 23, 15, tzinfo=datetime.timezone.utc), title=\"Distilling LLMs' Decomposition Abilities into Compact Language Models\", authors=[arxiv.Result.Author('Denis Tarasov'), arxiv.Result.Author('Kumar Shridhar')], summary=\"Large Language Models (LLMs) have demonstrated proficiency in their reasoning\\nabilities, yet their large size presents scalability challenges and limits any\\nfurther customization. In contrast, compact models offer customized training\\nbut often fall short in solving complex reasoning tasks. This study focuses on\\ndistilling the LLMs' decomposition skills into compact models using offline\\nreinforcement learning. We leverage the advancements in the LLM`s capabilities\\nto provide feedback and generate a specialized task-specific dataset for\\ntraining compact models. The development of an AI-generated dataset and the\\nestablishment of baselines constitute the primary contributions of our work,\\nunderscoring the potential of compact models in replicating complex\\nproblem-solving skills.\", comment='https://github.com/DT6A/GSM8K-AI-SubQ', journal_ref=None, doi=None, primary_category='cs.CL', categories=['cs.CL', 'cs.AI', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2402.01812v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2402.01812v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses enhancing reasoning abilities in compact models derived from LLMs, which aligns with the topic of using LLMs to improve human research and learning capabilities.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2408.15686v1', updated=datetime.datetime(2024, 8, 28, 10, 22, 5, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 8, 28, 10, 22, 5, tzinfo=datetime.timezone.utc), title=\"Navigating the Future of Education: Educators' Insights on AI Integration and Challenges in Greece, Hungary, Latvia, Ireland and Armenia\", authors=[arxiv.Result.Author('Evangelia Daskalaki'), arxiv.Result.Author('Katerina Psaroudaki'), arxiv.Result.Author('Paraskevi Fragopoulou')], summary=\"Understanding teachers' perspectives on AI in Education (AIEd) is crucial for\\nits effective integration into the educational framework. This paper aims to\\nexplore how teachers currently use AI and how it can enhance the educational\\nprocess. We conducted a cross-national study spanning Greece, Hungary, Latvia,\\nIreland, and Armenia, surveying 1754 educators through an online questionnaire,\\naddressing three research questions. Our first research question examines\\neducators' understanding of AIEd, their skepticism, and its integration within\\nschools. Most educators report a solid understanding of AI and acknowledge its\\npotential risks. AIEd is primarily used for educator support and engaging\\nstudents. However, concerns exist about AI's impact on fostering critical\\nthinking and exposing students to biased data. The second research question\\ninvestigates student engagement with AI tools from educators' perspectives.\\nTeachers indicate that students use AI mainly to manage their academic\\nworkload, while outside school, AI tools are primarily used for entertainment.\\nThe third research question addresses future implications of AI in education.\\nEducators are optimistic about AI's potential to enhance educational processes,\\nparticularly through personalized learning experiences. Nonetheless, they\\nexpress significant concerns about AI's impact on cultivating critical thinking\\nand ethical issues related to potential misuse. There is a strong emphasis on\\nthe need for professional development through training seminars, workshops, and\\nonline courses to integrate AI effectively into teaching practices. Overall,\\nthe findings highlight a cautious optimism among educators regarding AI in\\neducation, alongside a clear demand for targeted professional development to\\naddress concerns and enhance skills in using AI tools.\", comment='19 pages, 9 figures, 3 tables', journal_ref=None, doi=None, primary_category='cs.CY', categories=['cs.CY'], links=[arxiv.Result.Link('http://arxiv.org/abs/2408.15686v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2408.15686v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses the perspectives of educators on the integration of AI in education, which is relevant to enhancing human abilities in research and learning through AI-driven tools.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2401.15595v3', updated=datetime.datetime(2024, 5, 14, 4, 34, 20, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 1, 28, 7, 37, 33, tzinfo=datetime.timezone.utc), title='Comuniqa : Exploring Large Language Models for improving speaking skills', authors=[arxiv.Result.Author('Manas Mhasakar'), arxiv.Result.Author('Shikhar Sharma'), arxiv.Result.Author('Apurv Mehra'), arxiv.Result.Author('Utkarsh Venaik'), arxiv.Result.Author('Ujjwal Singhal'), arxiv.Result.Author('Dhruv Kumar'), arxiv.Result.Author('Kashish Mittal')], summary='In this paper, we investigate the potential of Large Language Models (LLMs)\\nto improve English speaking skills. This is particularly relevant in countries\\nlike India, where English is crucial for academic, professional, and personal\\ncommunication but remains a non-native language for many. Traditional methods\\nfor enhancing speaking skills often rely on human experts, which can be limited\\nin terms of scalability, accessibility, and affordability. Recent advancements\\nin Artificial Intelligence (AI) offer promising solutions to overcome these\\nlimitations.\\n  We propose Comuniqa, a novel LLM-based system designed to enhance English\\nspeaking skills. We adopt a human-centric evaluation approach, comparing\\nComuniqa with the feedback and instructions provided by human experts. In our\\nevaluation, we divide the participants in three groups: those who use LLM-based\\nsystem for improving speaking skills, those guided by human experts for the\\nsame task and those who utilize both the LLM-based system as well as the human\\nexperts. Using surveys, interviews, and actual study sessions, we provide a\\ndetailed perspective on the effectiveness of different learning modalities. Our\\npreliminary findings suggest that while LLM-based systems have commendable\\naccuracy, they lack human-level cognitive capabilities, both in terms of\\naccuracy and empathy. Nevertheless, Comuniqa represents a significant step\\ntowards achieving Sustainable Development Goal 4: Quality Education by\\nproviding a valuable learning tool for individuals who may not have access to\\nhuman experts for improving their speaking skills.', comment='Accepted at 7th ACM SIGCAS/SIGCHI Conference of Computing and\\n  Sustainable Societies : ACM COMPASS 2024', journal_ref=None, doi=None, primary_category='cs.HC', categories=['cs.HC', 'cs.CY'], links=[arxiv.Result.Link('http://arxiv.org/abs/2401.15595v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2401.15595v3', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper explores the use of LLMs to enhance learning, specifically focusing on a novel system to improve English speaking skills, which aligns with the research topic of LLMs for enhancing human abilities to research and learn.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2404.03044v1', updated=datetime.datetime(2024, 4, 3, 20, 8, 15, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 4, 3, 20, 8, 15, tzinfo=datetime.timezone.utc), title='The Artificial Intelligence Ontology: LLM-assisted construction of AI concept hierarchies', authors=[arxiv.Result.Author('Marcin P. Joachimiak'), arxiv.Result.Author('Mark A. Miller'), arxiv.Result.Author('J. Harry Caufield'), arxiv.Result.Author('Ryan Ly'), arxiv.Result.Author('Nomi L. Harris'), arxiv.Result.Author('Andrew Tritt'), arxiv.Result.Author('Christopher J. Mungall'), arxiv.Result.Author('Kristofer E. Bouchard')], summary=\"The Artificial Intelligence Ontology (AIO) is a systematization of artificial\\nintelligence (AI) concepts, methodologies, and their interrelations. Developed\\nvia manual curation, with the additional assistance of large language models\\n(LLMs), AIO aims to address the rapidly evolving landscape of AI by providing a\\ncomprehensive framework that encompasses both technical and ethical aspects of\\nAI technologies. The primary audience for AIO includes AI researchers,\\ndevelopers, and educators seeking standardized terminology and concepts within\\nthe AI domain. The ontology is structured around six top-level branches:\\nNetworks, Layers, Functions, LLMs, Preprocessing, and Bias, each designed to\\nsupport the modular composition of AI methods and facilitate a deeper\\nunderstanding of deep learning architectures and ethical considerations in AI.\\n  AIO's development utilized the Ontology Development Kit (ODK) for its\\ncreation and maintenance, with its content being dynamically updated through\\nAI-driven curation support. This approach not only ensures the ontology's\\nrelevance amidst the fast-paced advancements in AI but also significantly\\nenhances its utility for researchers, developers, and educators by simplifying\\nthe integration of new AI concepts and methodologies.\\n  The ontology's utility is demonstrated through the annotation of AI methods\\ndata in a catalog of AI research publications and the integration into the\\nBioPortal ontology resource, highlighting its potential for cross-disciplinary\\nresearch. The AIO ontology is open source and is available on GitHub\\n(https://github.com/berkeleybop/artificial-intelligence-ontology) and BioPortal\\n(https://bioportal.bioontology.org/ontologies/AIO).\", comment=None, journal_ref=None, doi=None, primary_category='cs.LG', categories=['cs.LG', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2404.03044v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2404.03044v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses the use of large language models (LLMs) in creating an ontology that enhances understanding of AI concepts, which aligns closely with the topic of improving research and learning abilities through LLMs.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2406.10934v1', updated=datetime.datetime(2024, 6, 16, 13, 31, 35, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 6, 16, 13, 31, 35, tzinfo=datetime.timezone.utc), title='Beyond Answers: Large Language Model-Powered Tutoring System in Physics Education for Deep Learning and Precise Understanding', authors=[arxiv.Result.Author('Zhoumingju Jiang'), arxiv.Result.Author('Mengjun Jiang')], summary=\"The integration of artificial intelligence (AI) in education has shown\\nsignificant promise, yet the effective personalization of learning,\\nparticularly in physics education, remains a challenge. This paper proposes\\nPhysics-STAR, a framework for large language model (LLM)- powered tutoring\\nsystem designed to address this gap by providing personalized and adaptive\\nlearning experiences for high school students. Our study evaluates Physics-STAR\\nagainst traditional teacher-led lectures and generic LLM tutoring through a\\ncontrolled experiment with 12 high school sophomores. Results showed that\\nPhysics-STAR increased students' average scores and efficiency on conceptual,\\ncomputational, and on informational questions. In particular, students' average\\nscores on complex information problems increased by 100% and their efficiency\\nincreased by 5.95%. By facilitating step-by-step guidance and reflective\\nlearning, Physics-STAR helps students develop critical thinking skills and a\\nrobust comprehension of abstract concepts. The findings underscore the\\npotential of AI-driven personalized tutoring systems to transform physics\\neducation. As LLM continues to advance, the future of student-centered AI in\\neducation looks promising, with the potential to significantly improve learning\\noutcomes and efficiency.\", comment='13 pages, 3 figures, CSCW 2O24', journal_ref=None, doi=None, primary_category='physics.ed-ph', categories=['physics.ed-ph', 'cs.HC'], links=[arxiv.Result.Link('http://arxiv.org/abs/2406.10934v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2406.10934v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper directly discusses the use of LLMs in enhancing personalized learning and improving educational outcomes, aligning well with the research topic.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2406.14769v1', updated=datetime.datetime(2024, 6, 20, 22, 46, 56, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 6, 20, 22, 46, 56, tzinfo=datetime.timezone.utc), title='How critically can an AI think? A framework for evaluating the quality of thinking of generative artificial intelligence', authors=[arxiv.Result.Author('Luke Zaphir'), arxiv.Result.Author('Jason M. Lodge'), arxiv.Result.Author('Jacinta Lisec'), arxiv.Result.Author('Dom McGrath'), arxiv.Result.Author('Hassan Khosravi')], summary='Generative AI such as those with large language models have created\\nopportunities for innovative assessment design practices. Due to recent\\ntechnological developments, there is a need to know the limits and capabilities\\nof generative AI in terms of simulating cognitive skills. Assessing student\\ncritical thinking skills has been a feature of assessment for time immemorial,\\nbut the demands of digital assessment create unique challenges for equity,\\nacademic integrity and assessment authorship. Educators need a framework for\\ndetermining their assessments vulnerability to generative AI to inform\\nassessment design practices. This paper presents a framework that explores the\\ncapabilities of the LLM ChatGPT4 application, which is the current industry\\nbenchmark. This paper presents the Mapping of questions, AI vulnerability\\ntesting, Grading, Evaluation (MAGE) framework to methodically critique their\\nassessments within their own disciplinary contexts. This critique will provide\\nspecific and targeted indications of their questions vulnerabilities in terms\\nof the critical thinking skills. This can go on to form the basis of assessment\\ndesign for their tasks.', comment=None, journal_ref=None, doi=None, primary_category='cs.AI', categories=['cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2406.14769v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2406.14769v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses the capabilities of large language models in the context of assessment design, which directly relates to enhancing human learning and research abilities.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2403.15586v1', updated=datetime.datetime(2024, 3, 22, 19, 21, 29, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 3, 22, 19, 21, 29, tzinfo=datetime.timezone.utc), title=\"Generative AI in Education: A Study of Educators' Awareness, Sentiments, and Influencing Factors\", authors=[arxiv.Result.Author('Aashish Ghimire'), arxiv.Result.Author('James Prather'), arxiv.Result.Author('John Edwards')], summary=\"The rapid advancement of artificial intelligence (AI) and the expanding\\nintegration of large language models (LLMs) have ignited a debate about their\\napplication in education. This study delves into university instructors'\\nexperiences and attitudes toward AI language models, filling a gap in the\\nliterature by analyzing educators' perspectives on AI's role in the classroom\\nand its potential impacts on teaching and learning. The objective of this\\nresearch is to investigate the level of awareness, overall sentiment\\ntowardsadoption, and the factors influencing these attitudes for LLMs and\\ngenerative AI-based tools in higher education. Data was collected through a\\nsurvey using a Likert scale, which was complemented by follow-up interviews to\\ngain a more nuanced understanding of the instructors' viewpoints. The collected\\ndata was processed using statistical and thematic analysis techniques. Our\\nfindings reveal that educators are increasingly aware of and generally positive\\ntowards these tools. We find no correlation between teaching style and attitude\\ntoward generative AI. Finally, while CS educators show far more confidence in\\ntheir technical understanding of generative AI tools and more positivity\\ntowards them than educators in other fields, they show no more confidence in\\ntheir ability to detect AI-generated work.\", comment=None, journal_ref=None, doi=None, primary_category='cs.AI', categories=['cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2403.15586v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2403.15586v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper explores the attitudes of educators towards LLMs and their application in educational settings, which directly relates to the enhancement of research and learning capabilities through AI.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2409.02387v1', updated=datetime.datetime(2024, 9, 4, 2, 30, 12, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 9, 4, 2, 30, 12, tzinfo=datetime.timezone.utc), title='Large Language Models and Cognitive Science: A Comprehensive Review of Similarities, Differences, and Challenges', authors=[arxiv.Result.Author('Qian Niu'), arxiv.Result.Author('Junyu Liu'), arxiv.Result.Author('Ziqian Bi'), arxiv.Result.Author('Pohsun Feng'), arxiv.Result.Author('Benji Peng'), arxiv.Result.Author('Keyu Chen')], summary='This comprehensive review explores the intersection of Large Language Models\\n(LLMs) and cognitive science, examining similarities and differences between\\nLLMs and human cognitive processes. We analyze methods for evaluating LLMs\\ncognitive abilities and discuss their potential as cognitive models. The review\\ncovers applications of LLMs in various cognitive fields, highlighting insights\\ngained for cognitive science research. We assess cognitive biases and\\nlimitations of LLMs, along with proposed methods for improving their\\nperformance. The integration of LLMs with cognitive architectures is examined,\\nrevealing promising avenues for enhancing artificial intelligence (AI)\\ncapabilities. Key challenges and future research directions are identified,\\nemphasizing the need for continued refinement of LLMs to better align with\\nhuman cognition. This review provides a balanced perspective on the current\\nstate and future potential of LLMs in advancing our understanding of both\\nartificial and human intelligence.', comment='10 pages, 1 figure', journal_ref=None, doi=None, primary_category='cs.AI', categories=['cs.AI', 'cs.CL'], links=[arxiv.Result.Link('http://arxiv.org/abs/2409.02387v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2409.02387v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses the relationship between LLMs and cognitive processes, which directly relates to enhancing human abilities to research and learn.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2401.02985v1', updated=datetime.datetime(2024, 1, 2, 3, 54, 50, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 1, 2, 3, 54, 50, tzinfo=datetime.timezone.utc), title='Evaluating Large Language Models on the GMAT: Implications for the Future of Business Education', authors=[arxiv.Result.Author('Vahid Ashrafimoghari'), arxiv.Result.Author('Necdet Gürkan'), arxiv.Result.Author('Jordan W. Suchow')], summary=\"The rapid evolution of artificial intelligence (AI), especially in the domain\\nof Large Language Models (LLMs) and generative AI, has opened new avenues for\\napplication across various fields, yet its role in business education remains\\nunderexplored. This study introduces the first benchmark to assess the\\nperformance of seven major LLMs, OpenAI's models (GPT-3.5 Turbo, GPT-4, and\\nGPT-4 Turbo), Google's models (PaLM 2, Gemini 1.0 Pro), and Anthropic's models\\n(Claude 2 and Claude 2.1), on the GMAT, which is a key exam in the admission\\nprocess for graduate business programs. Our analysis shows that most LLMs\\noutperform human candidates, with GPT-4 Turbo not only outperforming the other\\nmodels but also surpassing the average scores of graduate students at top\\nbusiness schools. Through a case study, this research examines GPT-4 Turbo's\\nability to explain answers, evaluate responses, identify errors, tailor\\ninstructions, and generate alternative scenarios. The latest LLM versions,\\nGPT-4 Turbo, Claude 2.1, and Gemini 1.0 Pro, show marked improvements in\\nreasoning tasks compared to their predecessors, underscoring their potential\\nfor complex problem-solving. While AI's promise in education, assessment, and\\ntutoring is clear, challenges remain. Our study not only sheds light on LLMs'\\nacademic potential but also emphasizes the need for careful development and\\napplication of AI in education. As AI technology advances, it is imperative to\\nestablish frameworks and protocols for AI interaction, verify the accuracy of\\nAI-generated content, ensure worldwide access for diverse learners, and create\\nan educational environment where AI supports human expertise. This research\\nsets the stage for further exploration into the responsible use of AI to enrich\\neducational experiences and improve exam preparation and assessment methods.\", comment=None, journal_ref=None, doi=None, primary_category='cs.CL', categories=['cs.CL', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2401.02985v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2401.02985v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses the application of LLMs in education, specifically in assessment and tutoring, which directly relates to enhancing human abilities in research and learning.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2401.01519v3', updated=datetime.datetime(2024, 3, 16, 13, 37, 48, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 1, 3, 3, 1, 29, tzinfo=datetime.timezone.utc), title='Exploring the Frontiers of LLMs in Psychological Applications: A Comprehensive Review', authors=[arxiv.Result.Author('Luoma Ke'), arxiv.Result.Author('Song Tong'), arxiv.Result.Author('Peng Cheng'), arxiv.Result.Author('Kaiping Peng')], summary=\"This paper explores the frontiers of large language models (LLMs) in\\npsychology applications. Psychology has undergone several theoretical changes,\\nand the current use of Artificial Intelligence (AI) and Machine Learning,\\nparticularly LLMs, promises to open up new research directions. We provide a\\ndetailed exploration of how LLMs like ChatGPT are transforming psychological\\nresearch. It discusses the impact of LLMs across various branches of\\npsychology, including cognitive and behavioral, clinical and counseling,\\neducational and developmental, and social and cultural psychology, highlighting\\ntheir potential to simulate aspects of human cognition and behavior. The paper\\ndelves into the capabilities of these models to emulate human-like text\\ngeneration, offering innovative tools for literature review, hypothesis\\ngeneration, experimental design, experimental subjects, data analysis, academic\\nwriting, and peer review in psychology. While LLMs are essential in advancing\\nresearch methodologies in psychology, the paper also cautions about their\\ntechnical and ethical challenges. There are issues like data privacy, the\\nethical implications of using LLMs in psychological research, and the need for\\na deeper understanding of these models' limitations. Researchers should\\nresponsibly use LLMs in psychological studies, adhering to ethical standards\\nand considering the potential consequences of deploying these technologies in\\nsensitive areas. Overall, the article provides a comprehensive overview of the\\ncurrent state of LLMs in psychology, exploring potential benefits and\\nchallenges. It serves as a call to action for researchers to leverage LLMs'\\nadvantages responsibly while addressing associated risks.\", comment=None, journal_ref=None, doi=None, primary_category='cs.LG', categories=['cs.LG', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2401.01519v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2401.01519v3', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses the application of LLMs in enhancing research methodologies and learning in psychology, which directly relates to the topic of using LLMs to enhance human capabilities in research and learning.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2406.04276v1', updated=datetime.datetime(2024, 6, 6, 17, 25, 7, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 6, 6, 17, 25, 7, tzinfo=datetime.timezone.utc), title='Generative AI-in-the-loop: Integrating LLMs and GPTs into the Next Generation Networks', authors=[arxiv.Result.Author('Han Zhang'), arxiv.Result.Author('Akram Bin Sediq'), arxiv.Result.Author('Ali Afana'), arxiv.Result.Author('Melike Erol-Kantarci')], summary='In recent years, machine learning (ML) techniques have created numerous\\nopportunities for intelligent mobile networks and have accelerated the\\nautomation of network operations. However, complex network tasks may involve\\nvariables and considerations even beyond the capacity of traditional ML\\nalgorithms. On the other hand, large language models (LLMs) have recently\\nemerged, demonstrating near-human-level performance in cognitive tasks across\\nvarious fields. However, they remain prone to hallucinations and often lack\\ncommon sense in basic tasks. Therefore, they are regarded as assistive tools\\nfor humans. In this work, we propose the concept of \"generative AI-in-the-loop\"\\nand utilize the semantic understanding, context awareness, and reasoning\\nabilities of LLMs to assist humans in handling complex or unforeseen situations\\nin mobile communication networks. We believe that combining LLMs and ML models\\nallows both to leverage their respective capabilities and achieve better\\nresults than either model alone. To support this idea, we begin by analyzing\\nthe capabilities of LLMs and compare them with traditional ML algorithms. We\\nthen explore potential LLM-based applications in line with the requirements of\\nnext-generation networks. We further examine the integration of ML and LLMs,\\ndiscussing how they can be used together in mobile networks. Unlike existing\\nstudies, our research emphasizes the fusion of LLMs with traditional ML-driven\\nnext-generation networks and serves as a comprehensive refinement of existing\\nsurveys. Finally, we provide a case study to enhance ML-based network intrusion\\ndetection with synthesized data generated by LLMs. Our case study further\\ndemonstrates the advantages of our proposed idea.', comment=None, journal_ref=None, doi=None, primary_category='cs.LG', categories=['cs.LG', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2406.04276v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2406.04276v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses the capabilities of LLMs as assistive tools for humans, which is directly relevant to the enhancement of human research and learning.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2408.07302v1', updated=datetime.datetime(2024, 7, 30, 15, 5, 24, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 7, 30, 15, 5, 24, tzinfo=datetime.timezone.utc), title=\"Effects of a Prompt Engineering Intervention on Undergraduate Students' AI Self-Efficacy, AI Knowledge and Prompt Engineering Ability: A Mixed Methods Study\", authors=[arxiv.Result.Author('David James Woo'), arxiv.Result.Author('Deliang Wang'), arxiv.Result.Author('Tim Yung'), arxiv.Result.Author('Kai Guo')], summary=\"Prompt engineering is critical for effective interaction with large language\\nmodels (LLMs) such as ChatGPT. However, efforts to teach this skill to students\\nhave been limited. This study designed and implemented a prompt engineering\\nintervention, examining its influence on undergraduate students' AI\\nself-efficacy, AI knowledge, and proficiency in creating effective prompts. The\\nintervention involved 27 students who participated in a 100-minute workshop\\nconducted during their history course at a university in Hong Kong. During the\\nworkshop, students were introduced to prompt engineering strategies, which they\\napplied to plan the course's final essay task. Multiple data sources were\\ncollected, including students' responses to pre- and post-workshop\\nquestionnaires, pre- and post-workshop prompt libraries, and written\\nreflections. The study's findings revealed that students demonstrated a higher\\nlevel of AI self-efficacy, an enhanced understanding of AI concepts, and\\nimproved prompt engineering skills because of the intervention. These findings\\nhave implications for AI literacy education, as they highlight the importance\\nof prompt engineering training for specific higher education use cases. This is\\na significant shift from students haphazardly and intuitively learning to\\nengineer prompts. Through prompt engineering education, educators can faciitate\\nstudents' effective navigation and leverage of LLMs to support their\\ncoursework.\", comment='34 pages, 6 figures', journal_ref=None, doi=None, primary_category='cs.CY', categories=['cs.CY', 'cs.CL', 'cs.HC'], links=[arxiv.Result.Link('http://arxiv.org/abs/2408.07302v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2408.07302v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper directly addresses how prompt engineering can enhance the ability of students to interact with LLMs, which aligns with the topic of enhancing human research and learning capabilities.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2405.13001v1', updated=datetime.datetime(2024, 5, 12, 1, 50, 1, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 5, 12, 1, 50, 1, tzinfo=datetime.timezone.utc), title='Large Language Models for Education: A Survey', authors=[arxiv.Result.Author('Hanyi Xu'), arxiv.Result.Author('Wensheng Gan'), arxiv.Result.Author('Zhenlian Qi'), arxiv.Result.Author('Jiayang Wu'), arxiv.Result.Author('Philip S. Yu')], summary='Artificial intelligence (AI) has a profound impact on traditional education.\\nIn recent years, large language models (LLMs) have been increasingly used in\\nvarious applications such as natural language processing, computer vision,\\nspeech recognition, and autonomous driving. LLMs have also been applied in many\\nfields, including recommendation, finance, government, education, legal\\naffairs, and finance. As powerful auxiliary tools, LLMs incorporate various\\ntechnologies such as deep learning, pre-training, fine-tuning, and\\nreinforcement learning. The use of LLMs for smart education (LLMEdu) has been a\\nsignificant strategic direction for countries worldwide. While LLMs have shown\\ngreat promise in improving teaching quality, changing education models, and\\nmodifying teacher roles, the technologies are still facing several challenges.\\nIn this paper, we conduct a systematic review of LLMEdu, focusing on current\\ntechnologies, challenges, and future developments. We first summarize the\\ncurrent state of LLMEdu and then introduce the characteristics of LLMs and\\neducation, as well as the benefits of integrating LLMs into education. We also\\nreview the process of integrating LLMs into the education industry, as well as\\nthe introduction of related technologies. Finally, we discuss the challenges\\nand problems faced by LLMEdu, as well as prospects for future optimization of\\nLLMEdu.', comment='Journal of Machine Learning and Cybernetics. 4 tables, 6 figures', journal_ref=None, doi=None, primary_category='cs.CL', categories=['cs.CL', 'cs.AI', 'cs.CY'], links=[arxiv.Result.Link('http://arxiv.org/abs/2405.13001v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2405.13001v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses the application of LLMs in education, which directly relates to enhancing human abilities to research and learn.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2407.02885v3', updated=datetime.datetime(2024, 8, 18, 21, 23, 42, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 7, 3, 7, 59, 52, tzinfo=datetime.timezone.utc), title='CogErgLLM: Exploring Large Language Model Systems Design Perspective Using Cognitive Ergonomics', authors=[arxiv.Result.Author('Azmine Toushik Wasi')], summary='Integrating cognitive ergonomics with LLMs is essential for enhancing safety,\\nreliability, and user satisfaction in human-AI interactions. Current LLM design\\noften lacks this integration, leading to systems that may not fully align with\\nhuman cognitive capabilities and limitations. Insufficient focus on\\nincorporating cognitive science methods exacerbates biases in LLM outputs,\\nwhile inconsistent application of user-centered design principles results in\\nsub-optimal user experiences. To address these challenges, our position paper\\nexplores the critical integration of cognitive ergonomics principles into LLM\\ndesign, aiming to provide a comprehensive framework and practical guidelines\\nfor ethical LLM development. Through our contributions, we seek to advance\\nunderstanding and practice in integrating cognitive ergonomics into LLM\\nsystems, fostering safer, more reliable, and ethically sound human-AI\\ninteractions.', comment='8 Page, 3 Figures. Accepted to Large Language Models and Cognition @\\n  ICML 2024 (https://llm-cognition.github.io/#:~:text=CogErgLLM); Read in\\n  OpenReview: https://openreview.net/forum?id=63C9YSc77p', journal_ref='ICML 2024 Workshop on LLMs and Cognition', doi=None, primary_category='cs.HC', categories=['cs.HC', 'cs.CL', 'cs.CY', 'cs.SI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2407.02885v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2407.02885v3', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses the integration of cognitive ergonomics with LLMs, which is directly related to enhancing human interactions and learning experiences with these systems.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2403.17089v2', updated=datetime.datetime(2024, 4, 17, 15, 0, 58, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 3, 25, 18, 25, 10, tzinfo=datetime.timezone.utc), title='GOLF: Goal-Oriented Long-term liFe tasks supported by human-AI collaboration', authors=[arxiv.Result.Author('Ben Wang')], summary=\"The advent of ChatGPT and similar large language models (LLMs) has\\nrevolutionized the human-AI interaction and information-seeking process.\\nLeveraging LLMs as an alternative to search engines, users can now access\\nsummarized information tailored to their queries, significantly reducing the\\ncognitive load associated with navigating vast information resources. This\\nshift underscores the potential of LLMs in redefining information access\\nparadigms. Drawing on the foundation of task-focused information retrieval and\\nLLMs' task planning ability, this research extends the scope of LLM\\ncapabilities beyond routine task automation to support users in navigating\\nlong-term and significant life tasks. It introduces the GOLF framework\\n(Goal-Oriented Long-term liFe tasks), which focuses on enhancing LLMs' ability\\nto assist in significant life decisions through goal orientation and long-term\\nplanning. The methodology encompasses a comprehensive simulation study to test\\nthe framework's efficacy, followed by model and human evaluations to develop a\\ndataset benchmark for long-term life tasks, and experiments across different\\nmodels and settings. By shifting the focus from short-term tasks to the broader\\nspectrum of long-term life goals, this research underscores the transformative\\npotential of LLMs in enhancing human decision-making processes and task\\nmanagement, marking a significant step forward in the evolution of human-AI\\ncollaboration.\", comment=None, journal_ref='Proceedings of the 47th International ACM SIGIR Conference on\\n  Research and Development in Information Retrieval (SIGIR 2024)', doi='10.1145/3626772.3657655', primary_category='cs.HC', categories=['cs.HC', 'cs.AI', 'cs.IR'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/3626772.3657655', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2403.17089v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2403.17089v2', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper explores how LLMs can augment research and learning by assisting users in navigating complex life tasks and enhancing decision-making processes.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2403.16159v2', updated=datetime.datetime(2024, 4, 5, 15, 8, 35, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 3, 24, 13, 54, 5, tzinfo=datetime.timezone.utc), title='Designing Child-Centric AI Learning Environments: Insights from LLM-Enhanced Creative Project-Based Learning', authors=[arxiv.Result.Author('Siyu Zha'), arxiv.Result.Author('Yuehan Qiao'), arxiv.Result.Author('Qingyu Hu'), arxiv.Result.Author('Zhongsheng Li'), arxiv.Result.Author('Jiangtao Gong'), arxiv.Result.Author('Yingqing Xu')], summary=\"Project-based learning (PBL) is an instructional method that is very helpful\\nin nurturing students' creativity, but it requires significant time and energy\\nfrom both students and teachers. Large language models (LLMs) have been proven\\nto assist in creative tasks, yet much controversy exists regarding their role\\nin fostering creativity. This paper explores the potential of LLMs in PBL\\nsettings, with a special focus on fostering creativity. We began with an\\nexploratory study involving 12 middle school students and identified five\\ndesign considerations for LLM applications in PBL. Building on this, we\\ndeveloped an LLM-empowered, 48-hour PBL program and conducted an instructional\\nexperiment with 31 middle school students. Our results indicated that LLMs can\\nenhance every stage of PBL. Additionally, we also discovered ambivalent\\nperspectives among students and mentors toward LLM usage. Furthermore, we\\nexplored the challenge and design implications of integrating LLMs into PBL and\\nreflected on the program. By bridging AI advancements into educational\\npractice, our work aims to inspire further discourse and investigation into\\nharnessing AI's potential in child-centric educational settings.\", comment=None, journal_ref=None, doi=None, primary_category='cs.HC', categories=['cs.HC'], links=[arxiv.Result.Link('http://arxiv.org/abs/2403.16159v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2403.16159v2', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper explores the role of LLMs in enhancing project-based learning, directly aligning with the topic of using LLMs to improve human research and learning capabilities.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2406.13903v1', updated=datetime.datetime(2024, 6, 20, 0, 25, 43, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 6, 20, 0, 25, 43, tzinfo=datetime.timezone.utc), title='Generative AI for Enhancing Active Learning in Education: A Comparative Study of GPT-3.5 and GPT-4 in Crafting Customized Test Questions', authors=[arxiv.Result.Author('Hamdireza Rouzegar'), arxiv.Result.Author('Masoud Makrehchi')], summary=\"This study investigates how LLMs, specifically GPT-3.5 and GPT-4, can develop\\ntailored questions for Grade 9 math, aligning with active learning principles.\\nBy utilizing an iterative method, these models adjust questions based on\\ndifficulty and content, responding to feedback from a simulated 'student'\\nmodel. A novel aspect of the research involved using GPT-4 as a 'teacher' to\\ncreate complex questions, with GPT-3.5 as the 'student' responding to these\\nchallenges. This setup mirrors active learning, promoting deeper engagement.\\nThe findings demonstrate GPT-4's superior ability to generate precise,\\nchallenging questions and notable improvements in GPT-3.5's ability to handle\\nmore complex problems after receiving instruction from GPT-4. These results\\nunderscore the potential of LLMs to mimic and enhance active learning\\nscenarios, offering a promising path for AI in customized education. This\\nresearch contributes to understanding how AI can support personalized learning\\nexperiences, highlighting the need for further exploration in various\\neducational contexts\", comment='Publisher: Canadian Artificial Intelligence Association. URL:\\n  https://caiac.pubpub.org/pub/kmn55wd2#nssvokovikx', journal_ref='Proceedings of The 37th Canadian Conference on Artificial\\n  Intelligence, 2024', doi=None, primary_category='cs.CL', categories=['cs.CL', 'cs.AI', 'cs.LG', '68T50', 'I.2.7'], links=[arxiv.Result.Link('http://arxiv.org/abs/2406.13903v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2406.13903v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper directly investigates how LLMs enhance personalized learning and research capabilities through tailored question generation, aligning well with the research topic.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2405.14713v1', updated=datetime.datetime(2024, 5, 23, 15, 46, 10, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 5, 23, 15, 46, 10, tzinfo=datetime.timezone.utc), title='Towards Educator-Driven Tutor Authoring: Generative AI Approaches for Creating Intelligent Tutor Interfaces', authors=[arxiv.Result.Author('Tommaso Calo'), arxiv.Result.Author('Christopher J. MacLellan')], summary='Intelligent Tutoring Systems (ITSs) have shown great potential in delivering\\npersonalized and adaptive education, but their widespread adoption has been\\nhindered by the need for specialized programming and design skills. Existing\\napproaches overcome the programming limitations with no-code authoring through\\ndrag and drop, however they assume that educators possess the necessary skills\\nto design effective and engaging tutor interfaces. To address this assumption\\nwe introduce generative AI capabilities to assist educators in creating tutor\\ninterfaces that meet their needs while adhering to design principles. Our\\napproach leverages Large Language Models (LLMs) and prompt engineering to\\ngenerate tutor layout and contents based on high-level requirements provided by\\neducators as inputs. However, to allow them to actively participate in the\\ndesign process, rather than relying entirely on AI-generated solutions, we\\nallow generation both at the entire interface level and at the individual\\ncomponent level. The former provides educators with a complete interface that\\ncan be refined using direct manipulation, while the latter offers the ability\\nto create specific elements to be added to the tutor interface. A small-scale\\ncomparison shows the potential of our approach to enhance the efficiency of\\ntutor interface design. Moving forward, we raise critical questions for\\nassisting educators with generative AI capabilities to create personalized,\\neffective, and engaging tutors, ultimately enhancing their adoption.', comment=None, journal_ref=None, doi='10.1145/3657604.3664694', primary_category='cs.HC', categories=['cs.HC', 'cs.AI'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/3657604.3664694', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2405.14713v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2405.14713v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses the use of LLMs to enhance educational tools, directly aligning with the topic of using LLMs to improve research and learning capabilities.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2407.15012v1', updated=datetime.datetime(2024, 6, 15, 1, 5, 56, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 6, 15, 1, 5, 56, tzinfo=datetime.timezone.utc), title='Transformative Influence of LLM and AI Tools in Student Social Media Engagement: Analyzing Personalization, Communication Efficiency, and Collaborative Learning', authors=[arxiv.Result.Author('Masoud Bashiri'), arxiv.Result.Author('Kamran Kowsari')], summary=\"The advent of Large Language Models (LLMs) and Artificial Intelligence (AI)\\ntools has revolutionized various facets of our lives, particularly in the realm\\nof social media. For students, these advancements have unlocked unprecedented\\nopportunities for learning, collaboration, and personal growth. AI-driven\\napplications are transforming how students interact with social media, offering\\npersonalized content and recommendations, and enabling smarter, more efficient\\ncommunication. Recent studies utilizing data from UniversityCube underscore the\\nprofound impact of AI tools on students' academic and social experiences. These\\nstudies reveal that students engaging with AI-enhanced social media platforms\\nreport higher academic performance, enhanced critical thinking skills, and\\nincreased engagement in collaborative projects.\\n  Moreover, AI tools assist in filtering out distracting content, allowing\\nstudents to concentrate more on educational materials and pertinent\\ndiscussions. The integration of LLMs in social media has further facilitated\\nimproved peer-to-peer communication and mentorship opportunities. AI algorithms\\neffectively match students based on shared academic interests and career goals,\\nfostering a supportive and intellectually stimulating online community, thereby\\ncontributing to increased student satisfaction and retention rates.\\n  In this article, we delve into the data provided by UniversityCube to explore\\nhow LLMs and AI tools are specifically transforming social media for students.\\nThrough case studies and statistical analyses, we offer a comprehensive\\nunderstanding of the educational and social benefits these technologies offer.\\nOur exploration highlights the potential of AI-driven tools to create a more\\nenriched, efficient, and supportive educational environment for students in the\\ndigital age.\", comment=None, journal_ref=None, doi=None, primary_category='cs.CY', categories=['cs.CY', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2407.15012v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2407.15012v1', title='pdf', rel='related', content_type=None)]),\n",
       "  \"The paper discusses the impact of LLMs and AI tools on students' learning and research capabilities, making it relevant to enhancing human ability to research and learn.\"),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2407.10246v3', updated=datetime.datetime(2024, 7, 29, 23, 1, 18, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 5, 1, 20, 43, 6, tzinfo=datetime.timezone.utc), title='CourseAssist: Pedagogically Appropriate AI Tutor for Computer Science Education', authors=[arxiv.Result.Author('Ty Feng'), arxiv.Result.Author('Sa Liu'), arxiv.Result.Author('Dipak Ghosal')], summary='The growing enrollments in computer science courses and increase in class\\nsizes necessitate scalable, automated tutoring solutions to adequately support\\nstudent learning. While Large Language Models (LLMs) like GPT-4 have\\ndemonstrated potential in assisting students through question-answering,\\neducators express concerns over student overreliance, miscomprehension of\\ngenerated code, and the risk of inaccurate answers. Rather than banning these\\ntools outright, we advocate for a constructive approach that harnesses the\\ncapabilities of AI while mitigating potential risks. This poster introduces\\nCourseAssist, a novel LLM-based tutoring system tailored for computer science\\neducation. Unlike generic LLM systems, CourseAssist uses retrieval-augmented\\ngeneration, user intent classification, and question decomposition to align AI\\nresponses with specific course materials and learning objectives, thereby\\nensuring pedagogical appropriateness of LLMs in educational settings. We\\nevaluated CourseAssist against a baseline of GPT-4 using a dataset of 50\\nquestion-answer pairs from a programming languages course, focusing on the\\ncriteria of usefulness, accuracy, and pedagogical appropriateness. Evaluation\\nresults show that CourseAssist significantly outperforms the baseline,\\ndemonstrating its potential to serve as an effective learning assistant. We\\nhave also deployed CourseAssist in 6 computer science courses at a large public\\nR1 research university reaching over 500 students. Interviews with 20 student\\nusers show that CourseAssist improves computer science instruction by\\nincreasing the accessibility of course-specific tutoring help and shortening\\nthe feedback loop on their programming assignments. Future work will include\\nextensive pilot testing at more universities and exploring better collaborative\\nrelationships between students, educators, and AI that improve computer science\\nlearning experiences.', comment='Accepted to SIGCSE Virtual 2024', journal_ref=None, doi=None, primary_category='cs.CY', categories=['cs.CY', 'cs.AI', 'cs.HC'], links=[arxiv.Result.Link('http://arxiv.org/abs/2407.10246v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2407.10246v3', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses the use of LLMs to enhance tutoring and learning in computer science, directly addressing the topic of LLMs improving human research and learning capabilities.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2407.13787v2', updated=datetime.datetime(2024, 7, 24, 4, 57, 55, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 7, 12, 11, 31, tzinfo=datetime.timezone.utc), title='The Honorific Effect: Exploring the Impact of Japanese Linguistic Formalities on AI-Generated Physics Explanations', authors=[arxiv.Result.Author('Keisuke Sato')], summary=\"This study investigates the influence of Japanese honorifics on the responses\\nof large language models (LLMs) when explaining the law of conservation of\\nmomentum. We analyzed the outputs of six state-of-the-art AI models, including\\nvariations of ChatGPT, Coral, and Gemini, using 14 different honorific forms.\\nOur findings reveal that honorifics significantly affect the quality,\\nconsistency, and formality of AI-generated responses, demonstrating LLMs'\\nability to interpret and adapt to social context cues embedded in language.\\nNotable variations were observed across different models, with some emphasizing\\nhistorical context and derivations, while others focused on intuitive\\nexplanations. The study highlights the potential for using honorifics to adjust\\nthe depth and complexity of AI-generated explanations in educational contexts.\\nFurthermore, the responsiveness of AI models to cultural linguistic elements\\nunderscores the importance of considering cultural factors in AI development\\nfor educational applications. These results open new avenues for research in\\nAI-assisted education and cultural adaptation in AI systems, with significant\\nimplications for personalizing learning experiences and developing culturally\\nsensitive AI tools for global education.\", comment=None, journal_ref=None, doi=None, primary_category='physics.ed-ph', categories=['physics.ed-ph', 'cs.CL'], links=[arxiv.Result.Link('http://arxiv.org/abs/2407.13787v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2407.13787v2', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper explores how LLMs can be influenced by cultural factors like honorifics, which is relevant for enhancing educational applications and learning experiences.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2403.14654v1', updated=datetime.datetime(2024, 2, 26, 2, 59, 7, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 2, 26, 2, 59, 7, tzinfo=datetime.timezone.utc), title='ChatGPT in Veterinary Medicine: A Practical Guidance of Generative Artificial Intelligence in Clinics, Education, and Research', authors=[arxiv.Result.Author('Candice P. Chu')], summary='ChatGPT, the most accessible generative artificial intelligence (AI) tool,\\noffers considerable potential for veterinary medicine, yet a dedicated review\\nof its specific applications is lacking. This review concisely synthesizes the\\nlatest research and practical applications of ChatGPT within the clinical,\\neducational, and research domains of veterinary medicine. It intends to provide\\nspecific guidance and actionable examples of how generative AI can be directly\\nutilized by veterinary professionals without a programming background. For\\npractitioners, ChatGPT can extract patient data, generate progress notes, and\\npotentially assist in diagnosing complex cases. Veterinary educators can create\\ncustom GPTs for student support, while students can utilize ChatGPT for exam\\npreparation. ChatGPT can aid in academic writing tasks in research, but\\nveterinary publishers have set specific requirements for authors to follow.\\nDespite its transformative potential, careful use is essential to avoid\\npitfalls like hallucination. This review addresses ethical considerations,\\nprovides learning resources, and offers tangible examples to guide responsible\\nimplementation. Carefully selected, up-to-date links to platforms that host\\nlarge language models are provided for advanced readers with programming\\ncapability. A table of key takeaways was provided to summarize this review. By\\nhighlighting potential benefits and limitations, this review equips\\nveterinarians, educators, and researchers to harness the power of ChatGPT\\neffectively.', comment=None, journal_ref=None, doi='10.3389/fvets.2024.1395934', primary_category='cs.CY', categories=['cs.CY'], links=[arxiv.Result.Link('http://dx.doi.org/10.3389/fvets.2024.1395934', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2403.14654v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2403.14654v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses specific practical applications of ChatGPT in veterinary medicine, which aligns with the topic of using LLMs to enhance research and learning abilities.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2403.12071v1', updated=datetime.datetime(2024, 2, 12, 17, 30, 5, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 2, 12, 17, 30, 5, tzinfo=datetime.timezone.utc), title='Tailoring Education with GenAI: A New Horizon in Lesson Planning', authors=[arxiv.Result.Author('Kostas Karpouzis'), arxiv.Result.Author('Dimitris Pantazatos'), arxiv.Result.Author('Joanna Taouki'), arxiv.Result.Author('Kalliopi Meli')], summary=\"The advent of Generative AI (GenAI) in education presents a transformative\\napproach to traditional teaching methodologies, which often overlook the\\ndiverse needs of individual students. This study introduces a GenAI tool, based\\non advanced natural language processing, designed as a digital assistant for\\neducators, enabling the creation of customized lesson plans. The tool utilizes\\nan innovative feature termed 'interactive mega-prompt,' a comprehensive query\\nsystem that allows educators to input detailed classroom specifics such as\\nstudent demographics, learning objectives, and preferred teaching styles. This\\ninput is then processed by the GenAI to generate tailored lesson plans. To\\nevaluate the tool's effectiveness, a comprehensive methodology incorporating\\nboth quantitative (i.e., % of time savings) and qualitative (i.e., user\\nsatisfaction) criteria was implemented, spanning various subjects and\\neducational levels, with continuous feedback collected from educators through a\\nstructured evaluation form. Preliminary results show that educators find the\\nGenAI-generated lesson plans effective, significantly reducing lesson planning\\ntime and enhancing the learning experience by accommodating diverse student\\nneeds. This AI-driven approach signifies a paradigm shift in education,\\nsuggesting its potential applicability in broader educational contexts,\\nincluding special education needs (SEN), where individualized attention and\\nspecific learning aids are paramount\", comment='Abstract accepted for EDUCON 2024 (IEEE Global Engineering Education\\n  Conference 2024)', journal_ref=None, doi=None, primary_category='cs.CY', categories=['cs.CY', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2403.12071v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2403.12071v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses a GenAI tool that enhances educational methodologies through advanced natural language processing, directly relating to the use of LLMs for improving research and learning capabilities.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2402.07397v2', updated=datetime.datetime(2024, 4, 26, 23, 31, 4, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 2, 12, 4, 10, 9, tzinfo=datetime.timezone.utc), title='Leveraging AI to Advance Science and Computing Education across Africa: Challenges, Progress and Opportunities', authors=[arxiv.Result.Author('George Boateng')], summary=\"Across the African continent, students grapple with various educational\\nchallenges, including limited access to essential resources such as computers,\\ninternet connectivity, reliable electricity, and a shortage of qualified\\nteachers. Despite these challenges, recent advances in AI such as BERT, and\\nGPT-4 have demonstrated their potential for advancing education. Yet, these AI\\ntools tend to be deployed and evaluated predominantly within the context of\\nWestern educational settings, with limited attention directed towards the\\nunique needs and challenges faced by students in Africa. In this chapter, we\\ndiscuss challenges with using AI to advance education across Africa. Then, we\\ndescribe our work developing and deploying AI in Education tools in Africa for\\nscience and computing education: (1) SuaCode, an AI-powered app that enables\\nAfricans to learn to code using their smartphones, (2) AutoGrad, an automated\\ngrading, and feedback tool for graphical and interactive coding assignments,\\n(3) a tool for code plagiarism detection that shows visual evidence of\\nplagiarism, (4) Kwame, a bilingual AI teaching assistant for coding courses,\\n(5) Kwame for Science, a web-based AI teaching assistant that provides instant\\nanswers to students' science questions and (6) Brilla AI, an AI contestant for\\nthe National Science and Maths Quiz competition. Finally, we discuss potential\\nopportunities to leverage AI to advance education across Africa.\", comment='Book chapter for the book: \"Artificial Intelligence in Education: The\\n  Intersection of Technology and Pedagogy\"', journal_ref=None, doi=None, primary_category='cs.CY', categories=['cs.CY', 'cs.CL', 'cs.HC'], links=[arxiv.Result.Link('http://arxiv.org/abs/2402.07397v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2402.07397v2', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses the application of AI, specifically mentioning advanced models like GPT-4, to enhance education in Africa, which aligns with the research topic of using LLMs to improve learning and research abilities.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2407.16887v1', updated=datetime.datetime(2024, 6, 7, 7, 18, 42, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 6, 7, 7, 18, 42, tzinfo=datetime.timezone.utc), title='Comprehensive AI Assessment Framework: Enhancing Educational Evaluation with Ethical AI Integration', authors=[arxiv.Result.Author('Selçuk Kılınç')], summary='The integration of generative artificial intelligence (GenAI) tools into\\neducation has been a game-changer for teaching and assessment practices,\\nbringing new opportunities, but also novel challenges which need to be dealt\\nwith. This paper presents the Comprehensive AI Assessment Framework (CAIAF), an\\nevolved version of the AI Assessment Scale (AIAS) by Perkins, Furze, Roe, and\\nMacVaugh, targeted toward the ethical integration of AI into educational\\nassessments. This is where the CAIAF differs, as it incorporates stringent\\nethical guidelines, with clear distinctions based on educational levels, and\\nadvanced AI capabilities of real-time interactions and personalized assistance.\\nThe framework developed herein has a very intuitive use, mainly through the use\\nof a color gradient that enhances the user-friendliness of the framework.\\nMethodologically, the framework has been developed through the huge support of\\na thorough literature review and practical insight into the topic, becoming a\\ndynamic tool to be used in different educational settings. The framework will\\nensure better learning outcomes, uphold academic integrity, and promote\\nresponsible use of AI, hence the need for this framework in modern educational\\npractice.', comment='13 Pages, 2 figures, 1 Framework', journal_ref=None, doi=None, primary_category='cs.CY', categories=['cs.CY', 'cs.AI', 'K.4, I.2'], links=[arxiv.Result.Link('http://arxiv.org/abs/2407.16887v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2407.16887v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses the integration of AI tools in education, highlighting their role in enhancing learning, which is directly relevant to researching how LLMs can improve human research and learning abilities.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2403.03920v1', updated=datetime.datetime(2024, 3, 6, 18, 29, 18, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 3, 6, 18, 29, 18, tzinfo=datetime.timezone.utc), title='Enhancing Instructional Quality: Leveraging Computer-Assisted Textual Analysis to Generate In-Depth Insights from Educational Artifacts', authors=[arxiv.Result.Author('Zewei Tian'), arxiv.Result.Author('Min Sun'), arxiv.Result.Author('Alex Liu'), arxiv.Result.Author('Shawon Sarkar'), arxiv.Result.Author('Jing Liu')], summary=\"This paper explores the transformative potential of computer-assisted textual\\nanalysis in enhancing instructional quality through in-depth insights from\\neducational artifacts. We integrate Richard Elmore's Instructional Core\\nFramework to examine how artificial intelligence (AI) and machine learning (ML)\\nmethods, particularly natural language processing (NLP), can analyze\\neducational content, teacher discourse, and student responses to foster\\ninstructional improvement. Through a comprehensive review and case studies\\nwithin the Instructional Core Framework, we identify key areas where AI/ML\\nintegration offers significant advantages, including teacher coaching, student\\nsupport, and content development. We unveil patterns that indicate AI/ML not\\nonly streamlines administrative tasks but also introduces novel pathways for\\npersonalized learning, providing actionable feedback for educators and\\ncontributing to a richer understanding of instructional dynamics. This paper\\nemphasizes the importance of aligning AI/ML technologies with pedagogical goals\\nto realize their full potential in educational settings, advocating for a\\nbalanced approach that considers ethical considerations, data quality, and the\\nintegration of human expertise.\", comment=None, journal_ref=None, doi=None, primary_category='cs.AI', categories=['cs.AI', 'cs.CL', 'cs.HC'], links=[arxiv.Result.Link('http://arxiv.org/abs/2403.03920v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2403.03920v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses the application of AI and machine learning, specifically NLP, to enhance instructional quality and learning processes, which directly aligns with the research topic on LLMs enhancing human learning.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2408.12496v1', updated=datetime.datetime(2024, 8, 22, 15, 41, 58, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 8, 22, 15, 41, 58, tzinfo=datetime.timezone.utc), title='MEDCO: Medical Education Copilots Based on A Multi-Agent Framework', authors=[arxiv.Result.Author('Hao Wei'), arxiv.Result.Author('Jianing Qiu'), arxiv.Result.Author('Haibao Yu'), arxiv.Result.Author('Wu Yuan')], summary='Large language models (LLMs) have had a significant impact on diverse\\nresearch domains, including medicine and healthcare. However, the potential of\\nLLMs as copilots in medical education remains underexplored. Current\\nAI-assisted educational tools are limited by their solitary learning approach\\nand inability to simulate the multi-disciplinary and interactive nature of\\nactual medical training. To address these limitations, we propose MEDCO\\n(Medical EDucation COpilots), a novel multi-agent-based copilot system\\nspecially developed to emulate real-world medical training environments. MEDCO\\nincorporates three primary agents: an agentic patient, an expert doctor, and a\\nradiologist, facilitating a multi-modal and interactive learning environment.\\nOur framework emphasizes the learning of proficient question-asking skills,\\nmulti-disciplinary collaboration, and peer discussions between students. Our\\nexperiments show that simulated virtual students who underwent training with\\nMEDCO not only achieved substantial performance enhancements comparable to\\nthose of advanced models, but also demonstrated human-like learning behaviors\\nand improvements, coupled with an increase in the number of learning samples.\\nThis work contributes to medical education by introducing a copilot that\\nimplements an interactive and collaborative learning approach. It also provides\\nvaluable insights into the effectiveness of AI-integrated training paradigms.', comment=None, journal_ref='ECCV 2024 Workshop', doi=None, primary_category='cs.AI', categories=['cs.AI', 'cs.MA'], links=[arxiv.Result.Link('http://arxiv.org/abs/2408.12496v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2408.12496v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper explores the use of LLMs as interactive copilots in medical education, which directly relates to the enhancement of human research and learning capabilities.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2406.07796v2', updated=datetime.datetime(2024, 6, 22, 1, 2, 54, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 6, 12, 1, 19, 36, tzinfo=datetime.timezone.utc), title='Battling Botpoop using GenAI for Higher Education: A Study of a Retrieval Augmented Generation Chatbots Impact on Learning', authors=[arxiv.Result.Author('Maung Thway'), arxiv.Result.Author('Jose Recatala-Gomez'), arxiv.Result.Author('Fun Siong Lim'), arxiv.Result.Author('Kedar Hippalgaonkar'), arxiv.Result.Author('Leonard W. T. Ng')], summary='Generative artificial intelligence (GenAI) and large language models (LLMs)\\nhave simultaneously opened new avenues for enhancing human learning and\\nincreased the prevalence of poor-quality information in student response -\\ntermed Botpoop. This study introduces Professor Leodar, a custom-built,\\nSinglish-speaking Retrieval Augmented Generation (RAG) chatbot designed to\\nenhance educational while reducing Botpoop. Deployed at Nanyang Technological\\nUniversity, Singapore, Professor Leodar offers a glimpse into the future of\\nAI-assisted learning, offering personalized guidance, 24/7 availability, and\\ncontextually relevant information. Through a mixed-methods approach, we examine\\nthe impact of Professor Leodar on learning, engagement, and exam preparedness,\\nwith 97.1% of participants reporting positive experiences. These findings help\\ndefine possible roles of AI in education and highlight the potential of custom\\nGenAI chatbots. Our combination of chatbot development, in-class deployment and\\noutcomes study offers a benchmark for GenAI educational tools and is a stepping\\nstone for redefining the interplay between AI and human learning.', comment='13 pages, 5 figures, SI with Annexes A, B and C upon request', journal_ref=None, doi=None, primary_category='cs.HC', categories=['cs.HC', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2406.07796v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2406.07796v2', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses the use of large language models to enhance human learning, making it directly relevant to the topic of LLMs for enhancing research and learning abilities.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2405.08008v2', updated=datetime.datetime(2024, 7, 10, 7, 59, 15, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 5, 9, 9, 24, 13, tzinfo=datetime.timezone.utc), title='Iris: An AI-Driven Virtual Tutor For Computer Science Education', authors=[arxiv.Result.Author('Patrick Bassner'), arxiv.Result.Author('Eduard Frankford'), arxiv.Result.Author('Stephan Krusche')], summary=\"Integrating AI-driven tools in higher education is an emerging area with\\ntransformative potential. This paper introduces Iris, a chat-based virtual\\ntutor integrated into the interactive learning platform Artemis that offers\\npersonalized, context-aware assistance in large-scale educational settings.\\nIris supports computer science students by guiding them through programming\\nexercises and is designed to act as a tutor in a didactically meaningful way.\\nIts calibrated assistance avoids revealing complete solutions, offering subtle\\nhints or counter-questions to foster independent problem-solving skills. For\\neach question, it issues multiple prompts in a Chain-of-Thought to\\nGPT-3.5-Turbo. The prompts include a tutor role description and examples of\\nmeaningful answers through few-shot learning. Iris employs contextual awareness\\nby accessing the problem statement, student code, and automated feedback to\\nprovide tailored advice.\\n  An empirical evaluation shows that students perceive Iris as effective\\nbecause it understands their questions, provides relevant support, and\\ncontributes to the learning process. While students consider Iris a valuable\\ntool for programming exercises and homework, they also feel confident solving\\nprogramming tasks in computer-based exams without Iris. The findings underscore\\nstudents' appreciation for Iris' immediate and personalized support, though\\nstudents predominantly view it as a complement to, rather than a replacement\\nfor, human tutors. Nevertheless, Iris creates a space for students to ask\\nquestions without being judged by others.\", comment='Published in Proceedings of the 2024 Innovation and Technology in\\n  Computer Science Education V. 1 (ITiCSE 2024), Pages 534 - 540, July 8--10,\\n  2024, Milan, Italy', journal_ref=None, doi='10.1145/3649217.3653543', primary_category='cs.HC', categories=['cs.HC', 'cs.AI'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/3649217.3653543', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2405.08008v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2405.08008v2', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses an AI-driven tool that enhances the educational experience by promoting independent problem-solving, aligning closely with the research topic of using LLMs to improve learning and research capabilities.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2407.17532v1', updated=datetime.datetime(2024, 7, 24, 3, 33, 47, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 7, 24, 3, 33, 47, tzinfo=datetime.timezone.utc), title='Generative artificial intelligence in dentistry: Current approaches and future challenges', authors=[arxiv.Result.Author('Fabián Villena'), arxiv.Result.Author('Claudia Véliz'), arxiv.Result.Author('Rosario García-Huidobro'), arxiv.Result.Author('Sebastián Aguayo')], summary='Artificial intelligence (AI) has become a commodity for people because of the\\nadvent of generative AI (GenAI) models that bridge the usability gap of AI by\\nproviding a natural language interface to interact with complex models. These\\nGenAI models range from text generation - such as two-way chat systems - to the\\ngeneration of image or video from textual descriptions input by a user. These\\nadvancements in AI have impacted Dentistry in multiple aspects. In dental\\neducation, the student now has the opportunity to solve a plethora of questions\\nby only prompting a GenAI model and have the answer in a matter of seconds.\\nGenAI models can help us deliver better patient healthcare by helping\\npractitioners gather knowledge quickly and efficiently. Finally, GenAI can also\\nbe used in dental research, where the applications range from new drug\\ndiscovery to assistance in academic writing. In this review, we first define\\nGenAI models and describe their multiple generation modalities; then, we\\nexplain and discuss their current and potential applications in Dentistry; and\\nfinally, we describe the challenges these new technologies impose in our area.', comment=None, journal_ref=None, doi=None, primary_category='cs.CL', categories=['cs.CL'], links=[arxiv.Result.Link('http://arxiv.org/abs/2407.17532v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2407.17532v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses the applications of generative AI models in education and research, which aligns with enhancing human abilities to research and learn.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2404.03044v1', updated=datetime.datetime(2024, 4, 3, 20, 8, 15, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 4, 3, 20, 8, 15, tzinfo=datetime.timezone.utc), title='The Artificial Intelligence Ontology: LLM-assisted construction of AI concept hierarchies', authors=[arxiv.Result.Author('Marcin P. Joachimiak'), arxiv.Result.Author('Mark A. Miller'), arxiv.Result.Author('J. Harry Caufield'), arxiv.Result.Author('Ryan Ly'), arxiv.Result.Author('Nomi L. Harris'), arxiv.Result.Author('Andrew Tritt'), arxiv.Result.Author('Christopher J. Mungall'), arxiv.Result.Author('Kristofer E. Bouchard')], summary=\"The Artificial Intelligence Ontology (AIO) is a systematization of artificial\\nintelligence (AI) concepts, methodologies, and their interrelations. Developed\\nvia manual curation, with the additional assistance of large language models\\n(LLMs), AIO aims to address the rapidly evolving landscape of AI by providing a\\ncomprehensive framework that encompasses both technical and ethical aspects of\\nAI technologies. The primary audience for AIO includes AI researchers,\\ndevelopers, and educators seeking standardized terminology and concepts within\\nthe AI domain. The ontology is structured around six top-level branches:\\nNetworks, Layers, Functions, LLMs, Preprocessing, and Bias, each designed to\\nsupport the modular composition of AI methods and facilitate a deeper\\nunderstanding of deep learning architectures and ethical considerations in AI.\\n  AIO's development utilized the Ontology Development Kit (ODK) for its\\ncreation and maintenance, with its content being dynamically updated through\\nAI-driven curation support. This approach not only ensures the ontology's\\nrelevance amidst the fast-paced advancements in AI but also significantly\\nenhances its utility for researchers, developers, and educators by simplifying\\nthe integration of new AI concepts and methodologies.\\n  The ontology's utility is demonstrated through the annotation of AI methods\\ndata in a catalog of AI research publications and the integration into the\\nBioPortal ontology resource, highlighting its potential for cross-disciplinary\\nresearch. The AIO ontology is open source and is available on GitHub\\n(https://github.com/berkeleybop/artificial-intelligence-ontology) and BioPortal\\n(https://bioportal.bioontology.org/ontologies/AIO).\", comment=None, journal_ref=None, doi=None, primary_category='cs.LG', categories=['cs.LG', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2404.03044v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2404.03044v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses the use of LLMs in curating an ontology that enhances understanding and research in AI, which aligns with the research topic of using LLMs to improve learning and research capabilities.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2407.04722v1', updated=datetime.datetime(2024, 6, 21, 12, 16, 1, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 6, 21, 12, 16, 1, tzinfo=datetime.timezone.utc), title='A GPT-based Code Review System for Programming Language Learning', authors=[arxiv.Result.Author('Lee Dong-Kyu')], summary=\"The increasing demand for programming language education and growing class\\nsizes require immediate and personalized feedback. However, traditional code\\nreview methods have limitations in providing this level of feedback. As the\\ncapabilities of Large Language Models (LLMs) like GPT for generating accurate\\nsolutions and timely code reviews are verified, this research proposes a system\\nthat employs GPT-4 to offer learner-friendly code reviews and minimize the risk\\nof AI-assist cheating.\\n  To provide learner-friendly code reviews, a dataset was collected from an\\nonline judge system, and this dataset was utilized to develop and enhance the\\nsystem's prompts. In addition, to minimize AI-assist cheating, the system flow\\nwas designed to provide code reviews only for code submitted by a learner, and\\na feature that highlights code lines to fix was added. After the initial system\\nwas deployed on the web, software education experts conducted usability test.\\nBased on the results, improvement strategies were developed to improve code\\nreview and code correctness check module, thereby enhancing the system.\\n  The improved system underwent evaluation by software education experts based\\non four criteria: strict code correctness checks, response time, lower API call\\ncosts, and the quality of code reviews. The results demonstrated a performance\\nto accurately identify error types, shorten response times, lower API call\\ncosts, and maintain high-quality code reviews without major issues. Feedback\\nfrom participants affirmed the tool's suitability for teaching programming to\\nprimary and secondary school students. Given these benefits, the system is\\nanticipated to be a efficient learning tool in programming language learning\\nfor educational settings.\", comment='11 pages, 14 figures', journal_ref=None, doi=None, primary_category='cs.SE', categories=['cs.SE', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2407.04722v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2407.04722v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses the use of LLMs (like GPT-4) to enhance education and feedback in programming, which aligns with the topic of using LLMs to improve learning and research capabilities.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2407.18310v1', updated=datetime.datetime(2024, 7, 25, 18, 2, 16, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 7, 25, 18, 2, 16, tzinfo=datetime.timezone.utc), title='Revolutionizing Undergraduate Learning: CourseGPT and Its Generative AI Advancements', authors=[arxiv.Result.Author('Ahmad M. Nazar'), arxiv.Result.Author('Mohamed Y. Selim'), arxiv.Result.Author('Ashraf Gaffar'), arxiv.Result.Author('Shakil Ahmed')], summary=\"Integrating Generative AI (GenAI) into educational contexts presents a\\ntransformative potential for enhancing learning experiences. This paper\\nintroduces CourseGPT, a generative AI tool designed to support instructors and\\nenhance the educational experiences of undergraduate students. Built on\\nopen-source Large Language Models (LLMs) from Mistral AI, CourseGPT offers\\ncontinuous instructor support and regular updates to course materials,\\nenriching the learning environment. By utilizing course-specific content, such\\nas slide decks and supplementary readings and references, CourseGPT provides\\nprecise, dynamically generated responses to student inquiries. Unlike generic\\nAI models, CourseGPT allows instructors to manage and control the responses,\\nthus extending the course scope without overwhelming details. The paper\\ndemonstrates the application of CourseGPT using the CPR E 431 - Basics of\\nInformation System Security course as a pilot. This course, with its large\\nenrollments and diverse curriculum, serves as an ideal testbed for CourseGPT.\\nThe tool aims to enhance the learning experience, accelerate feedback\\nprocesses, and streamline administrative tasks. The study evaluates CourseGPT's\\nimpact on student outcomes, focusing on correctness scores, context recall, and\\nfaithfulness of responses. Results indicate that the Mixtral-8x7b model, with a\\nhigher parameter count, outperforms smaller models, achieving an 88.0%\\ncorrectness score and a 66.6% faithfulness score. Additionally, feedback from\\nformer students and teaching assistants on CourseGPT's accuracy, helpfulness,\\nand overall performance was collected. The outcomes revealed that a significant\\nmajority found CourseGPT to be highly accurate and beneficial in addressing\\ntheir queries, with many praising its ability to provide timely and relevant\\ninformation.\", comment='8 pages', journal_ref=None, doi=None, primary_category='cs.CY', categories=['cs.CY', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2407.18310v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2407.18310v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses the use of LLMs like CourseGPT in educational settings to enhance learning experiences, directly aligning with the research topic on LLMs for enhancing research and learning.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2408.11728v1', updated=datetime.datetime(2024, 8, 21, 15, 54, 6, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 8, 21, 15, 54, 6, tzinfo=datetime.timezone.utc), title='AI-assisted Automated Short Answer Grading of Handwritten University Level Mathematics Exams', authors=[arxiv.Result.Author('Tianyi Liu'), arxiv.Result.Author('Julia Chatain'), arxiv.Result.Author('Laura Kobel-Keller'), arxiv.Result.Author('Gerd Kortemeyer'), arxiv.Result.Author('Thomas Willwacher'), arxiv.Result.Author('Mrinmaya Sachan')], summary='Effective and timely feedback in educational assessments is essential but\\nlabor-intensive, especially for complex tasks. Recent developments in automated\\nfeedback systems, ranging from deterministic response grading to the evaluation\\nof semi-open and open-ended essays, have been facilitated by advances in\\nmachine learning. The emergence of pre-trained Large Language Models, such as\\nGPT-4, offers promising new opportunities for efficiently processing diverse\\nresponse types with minimal customization. This study evaluates the\\neffectiveness of a pre-trained GPT-4 model in grading semi-open handwritten\\nresponses in a university-level mathematics exam. Our findings indicate that\\nGPT-4 provides surprisingly reliable and cost-effective initial grading,\\nsubject to subsequent human verification. Future research should focus on\\nrefining grading rules and enhancing the extraction of handwritten responses to\\nfurther leverage these technologies.', comment='17 pages, 12 figures, 4 tables', journal_ref=None, doi=None, primary_category='math.HO', categories=['math.HO', '97-02'], links=[arxiv.Result.Link('http://arxiv.org/abs/2408.11728v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2408.11728v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses the use of pre-trained LLMs like GPT-4 for automated feedback in educational assessments, directly relating to enhancing research and learning capabilities.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2407.05308v1', updated=datetime.datetime(2024, 7, 7, 9, 29, 51, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 7, 7, 9, 29, 51, tzinfo=datetime.timezone.utc), title=\"Exploring the Educational Landscape of AI: Large Language Models' Approaches to Explaining Conservation of Momentum in Physics\", authors=[arxiv.Result.Author('Keisuke Sato')], summary=\"The integration of Large Language Models (LLMs) in education offers both\\nopportunities and challenges, particularly in fields like physics that demand\\nprecise conceptual understanding. This study examines the capabilities of six\\nstate-of-the-art LLMs in explaining the law of conservation of momentum, a\\nfundamental principle in physics. By analyzing responses to a consistent,\\nsimple prompt in Japanese, we assess the models' explanatory approaches, depth\\nof understanding, and adaptability to different educational levels.Our\\ncomprehensive analysis, encompassing text characteristics, response similarity,\\nand keyword usage, unveils significant diversity in explanatory styles across\\nmodels. ChatGPT4.0 and Coral provided more comprehensive and technically\\ndetailed explanations, while Gemini models tended toward more intuitive\\napproaches. Key findings include variations in the treatment of critical\\nconcepts such as net force, and differing emphases on mathematical rigor and\\nreal-world applications.The results indicate that different AI models may be\\nmore suitable for various educational contexts, ranging from introductory to\\nadvanced levels. ChatGPT4.0 and Coral demonstrated potential for advanced\\ndiscussions, while Gemini models appeared more appropriate for introductory\\nexplanations. Importantly, the study underscores the necessity of educator\\nguidance in effectively leveraging these AI tools, as models varied in their\\nability to convey nuanced aspects of physical principles.This research\\nestablishes a foundation for understanding the educational potential of LLMs in\\nphysics, providing insights for educators on integrating these tools into their\\nteaching practices. It also highlights the need for further investigation into\\nAI-assisted learning in STEM fields, paving the way for more sophisticated\\napplications of AI in physics education.\", comment=None, journal_ref=None, doi=None, primary_category='physics.ed-ph', categories=['physics.ed-ph'], links=[arxiv.Result.Link('http://arxiv.org/abs/2407.05308v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2407.05308v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper directly addresses the use of LLMs in education, specifically highlighting their role in enhancing understanding and learning in physics, which aligns well with the research topic on LLMs for enhancing human research and learning capabilities.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2405.00330v1', updated=datetime.datetime(2024, 5, 1, 5, 39, 7, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 5, 1, 5, 39, 7, tzinfo=datetime.timezone.utc), title=\"Integrating A.I. in Higher Education: Protocol for a Pilot Study with 'SAMCares: An Adaptive Learning Hub'\", authors=[arxiv.Result.Author('Syed Hasib Akhter Faruqui'), arxiv.Result.Author('Nazia Tasnim'), arxiv.Result.Author('Iftekhar Ibne Basith'), arxiv.Result.Author('Suleiman Obeidat'), arxiv.Result.Author('Faruk Yildiz')], summary=\"Learning never ends, and there is no age limit to grow yourself. However, the\\neducational landscape may face challenges in effectively catering to students'\\ninclusion and diverse learning needs. These students should have access to\\nstate-of-the-art methods for lecture delivery, online resources, and technology\\nneeds. However, with all the diverse learning sources, it becomes harder for\\nstudents to comprehend a large amount of knowledge in a short period of time.\\nTraditional assistive technologies and learning aids often lack the dynamic\\nadaptability required for individualized education plans. Large Language Models\\n(LLM) have been used in language translation, text summarization, and content\\ngeneration applications. With rapid growth in AI over the past years,\\nAI-powered chatbots and virtual assistants have been developed. This research\\naims to bridge this gap by introducing an innovative study buddy we will be\\ncalling the 'SAMCares'. The system leverages a Large Language Model (LLM) (in\\nour case, LLaMa-2 70B as the base model) and Retriever-Augmented Generation\\n(RAG) to offer real-time, context-aware, and adaptive educational support. The\\ncontext of the model will be limited to the knowledge base of Sam Houston State\\nUniversity (SHSU) course notes. The LLM component enables a chat-like\\nenvironment to interact with it to meet the unique learning requirements of\\neach student. For this, we will build a custom web-based GUI. At the same time,\\nRAG enhances real-time information retrieval and text generation, in turn\\nproviding more accurate and context-specific assistance. An option to upload\\nadditional study materials in the web GUI is added in case additional knowledge\\nsupport is required. The system's efficacy will be evaluated through controlled\\ntrials and iterative feedback mechanisms.\", comment='Accepted in ASEE Annual Conference 2024', journal_ref=None, doi=None, primary_category='cs.CY', categories=['cs.CY', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2405.00330v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2405.00330v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses the use of LLMs to enhance educational support, directly aligning with the topic of enhancing human research and learning abilities.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2403.15472v3', updated=datetime.datetime(2024, 4, 5, 11, 32, 24, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 3, 20, 15, 47, 28, tzinfo=datetime.timezone.utc), title='Enhancing Programming Education with ChatGPT: A Case Study on Student Perceptions and Interactions in a Python Course', authors=[arxiv.Result.Author('Boxaun Ma'), arxiv.Result.Author('Li Chen'), arxiv.Result.Author(\"Shin'ichi Konomi\")], summary=\"The integration of ChatGPT as a supportive tool in education, notably in\\nprogramming courses, addresses the unique challenges of programming education\\nby providing assistance with debugging, code generation, and explanations.\\nDespite existing research validating ChatGPT's effectiveness, its application\\nin university-level programming education and a detailed understanding of\\nstudent interactions and perspectives remain limited. This paper explores\\nChatGPT's impact on learning in a Python programming course tailored for\\nfirst-year students over eight weeks. By analyzing responses from surveys,\\nopen-ended questions, and student-ChatGPT dialog data, we aim to provide a\\ncomprehensive view of ChatGPT's utility and identify both its advantages and\\nlimitations as perceived by students. Our study uncovers a generally positive\\nreception toward ChatGPT and offers insights into its role in enhancing the\\nprogramming education experience. These findings contribute to the broader\\ndiscourse on AI's potential in education, suggesting paths for future research\\nand application.\", comment=None, journal_ref=None, doi=None, primary_category='cs.CY', categories=['cs.CY', 'cs.AI', 'cs.PL'], links=[arxiv.Result.Link('http://arxiv.org/abs/2403.15472v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2403.15472v3', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses the use of ChatGPT in educational settings, specifically in programming courses, which directly relates to how LLMs can enhance research and learning abilities.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2403.19506v2', updated=datetime.datetime(2024, 3, 29, 20, 2, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 3, 28, 15, 37, 10, tzinfo=datetime.timezone.utc), title='LLMs as Academic Reading Companions: Extending HCI Through Synthetic Personae', authors=[arxiv.Result.Author('Celia Chen'), arxiv.Result.Author('Alex Leitch')], summary='This position paper argues that large language models (LLMs) constitute\\npromising yet underutilized academic reading companions capable of enhancing\\nlearning. We detail an exploratory study examining Claude from Anthropic, an\\nLLM-based interactive assistant that helps students comprehend complex\\nqualitative literature content. The study compares quantitative survey data and\\nqualitative interviews assessing outcomes between a control group and an\\nexperimental group leveraging Claude over a semester across two graduate\\ncourses. Initial findings demonstrate tangible improvements in reading\\ncomprehension and engagement among participants using the AI agent versus\\nunsupported independent study. However, there is potential for overreliance and\\nethical considerations that warrant continued investigation. By documenting an\\nearly integration of an LLM reading companion into an educational context, this\\nwork contributes pragmatic insights to guide development of synthetic personae\\nsupporting learning. Broader impacts compel policy and industry actions to\\nuphold responsible design in order to maximize benefits of AI integration while\\nprioritizing student wellbeing.', comment='3 pages, accepted to CHI2024 workshop \"Challenges and Opportunities\\n  of LLM-Based Synthetic Personae and Data in HCI\"', journal_ref=None, doi=None, primary_category='cs.HC', categories=['cs.HC'], links=[arxiv.Result.Link('http://arxiv.org/abs/2403.19506v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2403.19506v2', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper directly addresses the use of LLMs as academic companions to enhance learning and research capabilities, aligning well with the research topic.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2405.13890v1', updated=datetime.datetime(2024, 5, 22, 18, 1, 24, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 5, 22, 18, 1, 24, tzinfo=datetime.timezone.utc), title='An empirical study to understand how students use ChatGPT for writing essays and how it affects their ownership', authors=[arxiv.Result.Author('Andrew Jelson'), arxiv.Result.Author('Sang Won Lee')], summary=\"As large language models (LLMs) become more powerful and ubiquitous, systems\\nlike ChatGPT are increasingly used by students to help them with writing tasks.\\nTo better understand how these tools are used, we investigate how students\\nmight use an LLM for essay writing, for example, to study the queries asked to\\nChatGPT and the responses that ChatGPT gives. To that end, we plan to conduct a\\nuser study that will record the user writing process and present them with the\\nopportunity to use ChatGPT as an AI assistant. This study's findings will help\\nus understand how these tools are used and how practitioners -- such as\\neducators and essay readers -- should consider writing education and evaluation\\nbased on essay writing.\", comment='5 pages, 2 figures, submitted and accepted to ACM CHI Workshop\\n  In2Writing in 2024', journal_ref=None, doi=None, primary_category='cs.HC', categories=['cs.HC'], links=[arxiv.Result.Link('http://arxiv.org/abs/2405.13890v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2405.13890v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper directly explores how LLMs like ChatGPT are utilized by students for writing tasks, which is pertinent to enhancing research and learning capabilities.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2405.05134v1', updated=datetime.datetime(2024, 4, 25, 0, 23, 20, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 4, 25, 0, 23, 20, tzinfo=datetime.timezone.utc), title='Enhancing Deep Knowledge Tracing via Diffusion Models for Personalized Adaptive Learning', authors=[arxiv.Result.Author('Ming Kuo'), arxiv.Result.Author('Shouvon Sarker'), arxiv.Result.Author('Lijun Qian'), arxiv.Result.Author('Yujian Fu'), arxiv.Result.Author('Xiangfang Li'), arxiv.Result.Author('Xishuang Dong')], summary=\"In contrast to pedagogies like evidence-based teaching, personalized adaptive\\nlearning (PAL) distinguishes itself by closely monitoring the progress of\\nindividual students and tailoring the learning path to their unique knowledge\\nand requirements. A crucial technique for effective PAL implementation is\\nknowledge tracing, which models students' evolving knowledge to predict their\\nfuture performance. Based on these predictions, personalized recommendations\\nfor resources and learning paths can be made to meet individual needs. Recent\\nadvancements in deep learning have successfully enhanced knowledge tracking\\nthrough Deep Knowledge Tracing (DKT). This paper introduces generative AI\\nmodels to further enhance DKT. Generative AI models, rooted in deep learning,\\nare trained to generate synthetic data, addressing data scarcity challenges in\\nvarious applications across fields such as natural language processing (NLP)\\nand computer vision (CV). This study aims to tackle data shortage issues in\\nstudent learning records to enhance DKT performance for PAL. Specifically, it\\nemploys TabDDPM, a diffusion model, to generate synthetic educational records\\nto augment training data for enhancing DKT. The proposed method's effectiveness\\nis validated through extensive experiments on ASSISTments datasets. The\\nexperimental results demonstrate that the AI-generated data by TabDDPM\\nsignificantly improves DKT performance, particularly in scenarios with small\\ndata for training and large data for testing.\", comment=None, journal_ref=None, doi=None, primary_category='cs.CY', categories=['cs.CY', 'cs.AI', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2405.05134v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2405.05134v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses the use of generative AI models to enhance personalized adaptive learning through knowledge tracing, directly relating to how LLMs can improve research and learning abilities.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2405.13065v1', updated=datetime.datetime(2024, 5, 20, 15, 43, 4, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 5, 20, 15, 43, 4, tzinfo=datetime.timezone.utc), title=\"Exploring Teachers' Perception of Artificial Intelligence: The Socio-emotional Deficiency as Opportunities and Challenges in Human-AI Complementarity in K-12 Education\", authors=[arxiv.Result.Author('Soon-young Oh'), arxiv.Result.Author('Yongsu Ahn')], summary=\"In schools, teachers play a multitude of roles, serving as educators,\\ncounselors, decision-makers, and members of the school community. With recent\\nadvances in artificial intelligence (AI), there is increasing discussion about\\nhow AI can assist, complement, and collaborate with teachers. To pave the way\\nfor better teacher-AI complementary relationships in schools, our study aims to\\nexpand the discourse on teacher-AI complementarity by seeking educators'\\nperspectives on the potential strengths and limitations of AI across a spectrum\\nof responsibilities. Through a mixed method using a survey with 100 elementary\\nschool teachers in South Korea and in-depth interviews with 12 teachers, our\\nfindings indicate that teachers anticipate AI's potential to complement human\\nteachers by automating administrative tasks and enhancing personalized learning\\nthrough advanced intelligence. Interestingly, the deficit of AI's\\nsocio-emotional capabilities has been perceived as both challenges and\\nopportunities. Overall, our study demonstrates the nuanced perception of\\nteachers and different levels of expectations over their roles, challenging the\\nneed for decisions about AI adoption tailored to educators' preferences and\\nconcerns.\", comment=None, journal_ref=None, doi=None, primary_category='cs.HC', categories=['cs.HC', 'cs.AI', 'cs.CY'], links=[arxiv.Result.Link('http://arxiv.org/abs/2405.13065v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2405.13065v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses the complementarity of AI with educators, which is directly related to enhancing human abilities to research and learn.'),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2407.20792v1', updated=datetime.datetime(2024, 7, 30, 12, 55, 42, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 7, 30, 12, 55, 42, tzinfo=datetime.timezone.utc), title='How Novice Programmers Use and Experience ChatGPT when Solving Programming Exercises in an Introductory Course', authors=[arxiv.Result.Author('Andreas Scholl'), arxiv.Result.Author('Natalie Kiesler')], summary=\"This research paper contributes to the computing education research\\ncommunity's understanding of Generative AI (GenAI) in the context of\\nintroductory programming, and specifically, how students utilize related tools,\\nsuch as ChatGPT. An increased understanding of students' use is mandatory for\\neducators and higher education institutions, as GenAI is here to stay, and its\\nperformance is likely to improve rapidly in the near future. Learning about\\nstudents' use patterns is not only crucial to support their learning, but to\\ndevelop adequate forms of instruction and assessment. With the rapid\\nadvancement of AI, its broad availability, and ubiquitous presence in\\neducational environments, elaborating how AI can enhance learning experiences,\\nespecially in courses such as introductory programming is important. To date,\\nmost studies have focused on the educator's perspective on GenAI, its\\nperformance, characteristics, and limitations. However, the student\\nperspective, and how they actually use GenAI tools in course contexts, has not\\nbeen subject to a great number of studies. Therefore, this study is guided by\\nthe following research questions: (1) What do students report on their use\\npattern of ChatGPT in the context of introductory programming exercises? and\\n(2) How do students perceive ChatGPT in the context of introductory programming\\nexercises? To address these questions, computing students at a large German\\nuniversity were asked to solve programming tasks with the assistance of ChatGPT\\nas part of their introductory programming course. Students (n=298) provided\\ninformation regarding the use of ChatGPT, and their evaluation of the tool via\\nan online survey. This research provides a comprehensive evaluation of\\nChatGPT-3.5's application by novice programmers in a higher education\\ncontext...\", comment='Accepted at 2024 IEEE ASEE Frontiers in Education Conference', journal_ref=None, doi=None, primary_category='cs.AI', categories=['cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2407.20792v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2407.20792v1', title='pdf', rel='related', content_type=None)]),\n",
       "  \"The paper focuses on students' use of Generative AI, specifically ChatGPT, in enhancing their learning experience in introductory programming, which directly relates to LLMs improving research and learning capabilities.\"),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2403.15396v1', updated=datetime.datetime(2024, 2, 16, 8, 10, 41, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 2, 16, 8, 10, 41, tzinfo=datetime.timezone.utc), title='I would love this to be like an assistant, not the teacher: a voice of the customer perspective of what distance learning students want from an Artificial Intelligence Digital Assistant', authors=[arxiv.Result.Author('Bart Rienties'), arxiv.Result.Author('John Domingue'), arxiv.Result.Author('Subby Duttaroy'), arxiv.Result.Author('Christothea Herodotou'), arxiv.Result.Author('Felipe Tessarolo'), arxiv.Result.Author('Denise Whitelock')], summary=\"With the release of Generative AI systems such as ChatGPT, an increasing\\ninterest in using Artificial Intelligence (AI) has been observed across\\ndomains, including higher education. While emerging statistics show the\\npopularity of using AI amongst undergraduate students, little is yet known\\nabout students' perceptions regarding AI including self-reported benefits and\\nconcerns from their actual usage, in particular in distance learning contexts.\\nUsing a two-step, mixed-methods approach, we examined the perceptions of ten\\nonline and distance learning students from diverse disciplines regarding the\\ndesign of a hypothetical AI Digital Assistant (AIDA). In the first step, we\\ncaptured students' perceptions via interviews, while the second step supported\\nthe triangulation of data by enabling students to share, compare, and contrast\\nperceptions with those of peers. All participants agreed on the usefulness of\\nsuch an AI tool while studying and reported benefits from using it for\\nreal-time assistance and query resolution, support for academic tasks,\\npersonalisation and accessibility, together with emotional and social support.\\nStudents' concerns related to the ethical and social implications of\\nimplementing AIDA, data privacy and data use, operational challenges, academic\\nintegrity and misuse, and the future of education. Implications for the design\\nof AI-tailored systems are also discussed.\", comment='23 pages, 1 figure, submitted to Distance Education', journal_ref=None, doi=None, primary_category='cs.CY', categories=['cs.CY', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2403.15396v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2403.15396v1', title='pdf', rel='related', content_type=None)]),\n",
       "  \"The paper discusses students' perceptions and benefits of using AI tools in education, which directly relates to enhancing human research and learning capabilities.\"),\n",
       " (arxiv.Result(entry_id='http://arxiv.org/abs/2404.18518v1', updated=datetime.datetime(2024, 4, 29, 9, 3, 19, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 4, 29, 9, 3, 19, tzinfo=datetime.timezone.utc), title='From ChatGPT, DALL-E 3 to Sora: How has Generative AI Changed Digital Humanities Research and Services?', authors=[arxiv.Result.Author('Jiangfeng Liu'), arxiv.Result.Author('Ziyi Wang'), arxiv.Result.Author('Jing Xie'), arxiv.Result.Author('Lei Pei')], summary='Generative large-scale language models create the fifth paradigm of\\nscientific research, organically combine data science and computational\\nintelligence, transform the research paradigm of natural language processing\\nand multimodal information processing, promote the new trend of AI-enabled\\nsocial science research, and provide new ideas for digital humanities research\\nand application. This article profoundly explores the application of\\nlarge-scale language models in digital humanities research, revealing their\\nsignificant potential in ancient book protection, intelligent processing, and\\nacademic innovation. The article first outlines the importance of ancient book\\nresources and the necessity of digital preservation, followed by a detailed\\nintroduction to developing large-scale language models, such as ChatGPT, and\\ntheir applications in document management, content understanding, and\\ncross-cultural research. Through specific cases, the article demonstrates how\\nAI can assist in the organization, classification, and content generation of\\nancient books. Then, it explores the prospects of AI applications in artistic\\ninnovation and cultural heritage preservation. Finally, the article explores\\nthe challenges and opportunities in the interaction of technology, information,\\nand society in the digital humanities triggered by AI technologies.', comment='21 pages, 3 figures', journal_ref=None, doi=None, primary_category='cs.DL', categories=['cs.DL', 'cs.AI', 'cs.CL', 'cs.CY'], links=[arxiv.Result.Link('http://arxiv.org/abs/2404.18518v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2404.18518v1', title='pdf', rel='related', content_type=None)]),\n",
       "  'The paper discusses the application of LLMs in research and learning contexts, particularly in digital humanities, which aligns well with the topic.')]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_search_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "augment-research-workflows",
   "language": "python",
   "name": "augment-research-workflows"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
