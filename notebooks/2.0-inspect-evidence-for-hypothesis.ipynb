{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gpt-4o-2024-08-06\"\n",
    "SYS_MSG = \"\"\"\n",
    "You are a helpful research assistant.\n",
    "    Users will ask a question regarding whether or not the paper presents evidence forward or against a given argument,\n",
    "    You will output ONLY 2 things:\n",
    "    - argument: the argument presented by the paper regarding the user's query, like: \"Yes this paper validates this idea by discussing....\" etc..\n",
    "    - evidence: a list of DIRECT QUOTES from the paper containing information that support the argument above.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AgentCoder: Multi-Agent Code Generation with\\nEffective Testing and Self-optimisation\\nDong Huang\\nUniversity of Hong Kong\\ndhuang@cs.hku.hkJie M.Zhang\\njie.zhang@kcl.ac.uk\\njie.zhang@kcl.ac.ukMichael Luck\\nUniversity of Sussex\\nMichael.Luck@sussex.ac.uk\\nQingwen BU\\nShanghai Jiao Tong University\\nqwbu01@sjtu.edu.cnYuhao Qing\\nUniversity of Hong Kong\\nyhqing@cs.hku.hkHeming Cui\\nUniversity of Hong Kong\\nheming@cs.hku.hk\\nAbstract\\nAdvances in natural language processing (NLP) have been significantly boosted\\nby the development of transformer-based large language models (LLMs). These\\nmodels have revolutionized NLP tasks, particularly in code generation, aiding de-\\nvelopers in creating software with enhanced efficiency. Despite their advances,\\nchallenges remain in balancing code snippet generation with effective test cases.\\nTo address these issues, this paper introduces AgentCoder, a novel code genera-\\ntion solution comprising a multi-agent framework with a specialized test designer\\nagents in addition to the programmer agent and the test executor agent. During\\nthe coding procedure, the test designer agent generates effective test cases for the\\ngenerated code, and the test executor agent runs the code with the test cases and\\nwrites feedback to the programmer agent for it to refine the code. This collab-\\norative system enhances code generation efficiency with less cost, outperform-\\ning both single-agent models and earlier multi-agent strategies, demonstrated by\\nour extensive experiments on 14 LLMs and 16 baseline approaches. For exam-\\nple, AgentCoder (GPT-4) achieves 96.3% and 91.8% pass@1 in HumanEval and\\nMBPP datasets with an overall token overhead of 56.9K and 66.3K, while state-\\nof-the-art obtains only 90.2% and 78.9% pass@1 with an overall token overhead\\nof 138.2K and 206.5K.\\n1 Introduction\\nIn recent years, natural language processing (NLP) has been dramatically transformed by\\ntransformer-based large language models (LLMs). These models, notably exemplified by the GPT-x\\nseries [4, 29] developed by OpenAI, have consistently set the benchmark for performance across\\na wide array of standard NLP tasks. One of the most pivotal applications for these LLMs is code\\ngeneration for downstream tasks, where they play a vital role in aiding developers in creating soft-\\nware [13, 34, 36, 28, 27, 24]. Through extensive pretraining on substantial code-related datasets,\\nsuch as publicly available data on GitHub, these code LLMs acquire intricate contextual under-\\nstanding that can be effectively applied to diverse code-related tasks.\\nNumerous recent efforts have been made to improve the effectiveness of LLMs by incorporating\\nin-context learning and its variations [11, 37, 20, 18, 42, 9, 26], where an important optimisation\\npath is single-agent self-refinement within the same conversation. For example, Zhang et al. [42]\\nproposed Self-Edit to enhance the performance of LLMs in code generation. In particular, Self-Edit\\nruns the generated code against test cases that are manually written by developers. It then prompts\\nthe LLMs to refine the code based on the error messages of failed tests. Huang et al. [18] introduced\\nPreprint. Under review.arXiv:2312.13010v3  [cs.CL]  24 May 2024 CodeCoT, which uses LLMs to generate both code and test cases, thereby avoiding the reliance on\\ndevelopers for providing tests.\\nRecently, several studies (e.g., MetaGPT [17], ChatDev [31], and AgentVerse [8]) have proposed to\\nuse multi-agent collaborations to enhance the effectiveness of LLM-based code generation, where\\neach agent addresses a unique task such as code generation or task planning. These multi-agent\\ncollaboration frameworks aim to overcome the limitations of single-agent methods by distributing\\nthe workload and optimizing performance across various aspects of the code generation process.\\nNevertheless, these methods have two limitations: 1) they have less effective feedback mechanism\\nto provide the LLMs with valuable information. For example, the accuracy of the generated tests\\nfrom MetaGPT [17] is only 80% for HumanEval; 2) they involve an excessive number of agents\\n(e.g., MetaGPT has 5 agents, ChatDev has 7 agents), which require significant token resources for\\ncommunication and coordination among different agents.\\nTo address the above-mentioned challenge, in this paper, we propose AgentCoder, a multi-agent\\ncode generation framework with effective test generation and small token overhead. AgentCoder\\nhas only three simple agents, i.e., the programmer agent, the test designer agent, and the test ex-\\necutor agent. The programmer agent interacts with advanced code generation models to create code\\nbased on coding requirements. The test designer agent designs accurate, diverse, and comprehensive\\ntest cases with code generation models independently based on the coding requirements. The test\\nexecutor agent interacts with both the programmer agent and the test designer agent: it executes\\nthe tests from the test designer agent against the code generated by the programmer agent and then\\nprovides test execution results to the programmer agent. Once the feedback is obtained by the test\\nexecutor agent from the local environment (i.e., local terminal), it checks whether the feedback con-\\ntains error information (e.g., runtime error and assertion error). If all test cases pass the generated\\ncode, the test executor agent provides the code snippets to the human developer. Otherwise, the test\\nexecutor agent feeds back to the programmer agent and then requires it to fix the bug reported in the\\nfeedback. The iteration then continues once the feedback is that all test cases pass the code snippets\\nor the iteration budget is done.\\nThe test executor agent plays a pivotal role by designing effective tests to critically evaluate the\\ncode. Compared to existing test generation methods such as those used by CodeCoT and MetaGPT,\\nAgentCoder has three unique features. First, AgentCoder generates tests without seeing the whole\\ncode snippet, because the tests generated immediately following the code in one conversation can be\\nbiased and affected by the code, losing objectivity and diversity in the testing (See Tab. 5). Second,\\nAgentCoder proposes generating tests independently from the source code generation, intentionally\\nseparating the code generation and test generation processes. This choice is made based on previous\\nfindings that as the model achieves high performance in generating code snippets, there may be a\\ncorresponding decrease in the effectiveness of test case generation [8, 41]. This trade-off scenario\\noccurs due to the model’s limited resources and its focus on optimizing one aspect of the code\\ngeneration process, which might inadvertently compromise the quality of other tasks [8, 41]. Third,\\nthe test designer agent in AgentCoder is carefully designed and prompted to generate basic, edge,\\nand large scale tests, yielding high accuracy and test coverage.\\nOur extensive experiments with 14 LLMs and 16 optimisation baselines demonstrate that Agent-\\nCoder significantly improves the effectiveness and efficiency of code generation, outperforming all\\nbaseline approaches. In particular, AgentCoder obtains an average of 91.5% and 84.1% pass@1\\non all the datasets with GPT-4 and GPT-3.5, respectively, while the state-of-the-art obtains 86.8%\\nand 75.3%. The overall token overhead for AgentCoder is 56.9K for HumanEval and 66.3K for\\nMBPP, significantly lower than other state-of-the-art muti-agent frameworks including MetaGPT\\n(138.2K / 206.5K respectively), ChatDev (183.7K / 259.3K), and AgentVerse (149.2K / 193.6K).\\nMoreover, our test designer agent achieves a test generation accuracy of 89.6% and 91.4% for for\\nHumanEval and MBPP with GPT-4, respectively, outperforming the second-best method MetaGPT\\nwhose accuracy is 79.3% and 84.4%. In terms of code coverage, our test designer agent achieves a\\nline coverage of 91.7% for HumanEval and 92.3% for MBPP with GPT-4, while the coverage for\\nMetaGPT is 81.7% and 80.5%, respectively.\\nOur main contributions are as follows:\\n• We propose AgentCoder, a multi-agent framework for code generation with effective test\\ngeneration and small token overhead. AgentCoder contains three distinct agents, i.e., the\\nprogrammer agent, the test designer agent, and the test executor agent.\\n2 • We conduct an extensive evaluation with 14 LLMs and 16 LLM-based optimisation ap-\\nproaches which demonstrates that AgentCoder outperforms all the baselines in code gen-\\neration. In particular, AgentCoder obtains 77.4% and 89.1% pass@1 with GPT-3.5, while\\nstate-of-the-art obtains only 69.5% and 63.0%.\\n• We conduct a deep analysis of our results and ablation studies, which demonstrate the\\ncontribution of different agents, the effectiveness of the tests generated by the test designer\\nagent, and the necessity of using separate agents for code generation and test case design.\\n2 Related Work\\n2.1 Large Language Model for Code Generation\\nVarious architectures have been explored in these models, some notable examples being Code-\\nBERT [13], PLBART [1], and CodeGPT [40]. These models are pre-trained on code corpora to\\ndevelop a deep understanding of code syntax, semantics, and idiomatic constructs. Some innova-\\ntive approaches integrate structured representations to enhance their comprehension of the com-\\nplexities in code. For example, GraphCodeBERT [15] incorporates graph-based representations,\\nwhile CodeT5+ [36] combines the encoder-decoder paradigm with the structural essence of code.\\nThese enhancements aim to give the models a more fine-grained understanding of code relationships\\nand dependencies beyond just syntactic patterns. A current trend is the construction of large scale\\nmodels (e.g., Codex [6] and CodeGen [28]) with billions of parameters, which have illustrated the\\nperformance of state-of-the-art in code generation tasks. Recently, foundation models (e.g., GPT-\\n3.5-turbo, GPT-4) have also been used for code generations [26, 18]. These foundation models\\nillustrated the state-of-the-art performance for code generation tasks.\\n2.2 Enhancing Code Generation through Prompt Engineering\\nRecent advances in code generation have been significantly influenced by the integration of few-\\nshot learning techniques with LLMs. A notable contribution in this realm is the concept of self-\\nrefinement with few-shot prompting, as proposed by Madaan et al. [26]. This approach involves an\\nLLM iteratively refining its own generated code, leading to significant improvement in code quality.\\nAnother approach is the Self-Debugging technique introduced by Chen et al. [9], which involves\\ntesting the generated code against user-provided test cases. In scenarios where such test cases are\\nunavailable, the model engages in direct debugging by explaining the code, thus addressing potential\\nissues. Complementing these methods, Huang et al. [18] introduced CodeCoT, employing a Self-\\nExam Chain of Thought (CoT) process. This technique guides the model to generate code alongside\\ntest cases, particularly useful when external test cases are not available. CodeCoT adds a layer of\\nlogical reasoning to the code generation process. However, it is important to note that while this\\nmethod can identify syntax errors, functional errors may still go undetected as both the code and its\\ntest cases are generated by the same model. Building upon these concepts, Dong et al. [11] proposed\\nthe Self-Collaboration model, which divides the LLMs into different roles: an analyst, a coder, and\\na tester. The tester is powered by an LLM which predicts whether the code is buggy. Such practice\\nmay ignore many bugs in the code because the code is not executed in the local environments.\\n2.3 Multi-agent Collaboration\\nIn recent months, LLM-based multi-agent frameworks have gained significant attention from both\\nindustry and academia. These frameworks can be broadly categorized into two groups: non-code\\ngeneration and code generation multi-agent frameworks. Non-code generation multi-agent frame-\\nworks have been explored in various contexts. For example, Stable-Alignment [25] generates in-\\nstruction datasets by establishing consensus on value judgments through interactions among LLM\\nagents in a sandbox environment. Generative Agents [30] simulate a “town” of 25 agents to in-\\nvestigate language interaction, social understanding, and collective memory. NLSOM [44] employs\\nagents with different functions to solve complex tasks through multiple rounds of “mindstorms”. Cai\\net al. [5] propose a model for cost reduction by combining large models as tool makers and small\\nmodels as tool users. Other works focus on cooperation and competition in planning and strategy\\n[12] or propose LLM-based economies [44]. While these works have made significant contributions\\n3 $PEF(FOFSBUJPO5FTU\\x01$BTF(FOFSBUJPO&YFDVUJPO-PDBM\\x01&OWJSPONFOU\\n&SSPS\\x01\\'FFECBDL\\n“*\\x01XBOU\\x01UP\\x01CVJME\\x01B\\x01QSPHSBN\\x01UIBU\\x01ju\\nThe Human Developer\\nt1SPHSBNNFSut5FTU\\x01%FTJHOFSut5FTU\\x01&YFDVUPSuCode Snippets\\nTest CasesAgentCoder$PEF\\x013FRVJSFNFOU1SPHSBNTPassedthevalidation?:FT/PError Feedback\\nt*HPU\\x01UIF\\x01SFBEZ\\x0eUP\\x0eVTF\\x01DPEF\\x01\\x02u\"HFOU\\x04\\x12\"HFOU\\x04\\x13\"HFOU\\x04\\x14def has_close_elements(numbers, threshold) -> bool:foridx, eleminenumerate(numbers):for idx2, elem2 inenumerate(numbers):if idx!= idx2:distance = abs(elem-elem2)if distance < threshold:returnTruereturnFalse### 1. Basic Test Cases:assert has_close_elements([1.0, 2.0, 3.0], 0.5) == Falseassert has_close_elements([1.0, 1.4, 3.0], 0.5) == True### 2.Edge Cases:assert has_close_elements([], 0.5) == Falseassert has_close_elements([1.0], 0.5) == Falseassert has_close_elements([1.0, 1.0], 0) == True### 3.Large Numbers:large_list= [i* 0.1 for iin range(10000)]assert has_close_elements(large_list, 0.0999) == FalseCode RequirementsComplete the code snippet:fromtyping importListdefhas_close_elements(numbers, threshold) -> bool:\"\"\"Check if in given list of numbers, are any two numbers closer to each other than given threshold.>>> has_close_elements([1.0, 2.0, 3.0], 0.5) False>>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3) True\"\"\"\\nFile \"PATH/has_close_elements.py\", line 13>>> has_close_elements([1.0, 2.0, 3.0], 0.5)^^SyntaxError: invalid syntax5FTU\\x01$BTFT\\x01\\x0c\\x01&SSPS\\x01.FTTBHFFeedback Info\\nFigure 1: Pipeline of AgentCoder with a code generation example from HumanEval\\nto non-code generation tasks, AgentCoder, specifically addresses code generation tasks, presenting\\nunique challenges and opportunities for multi-agent collaboration in software development.\\nSeveral code generation multi-agent frameworks [17, 22, 8, 31] have been proposed concurrently\\nwith AgentCoder in recent months. For example, MetaGPT [17] simulates the software develop-\\nment life cycle using multiple agents. However, these frameworks often face two significant chal-\\nlenges. First, they may have less effective feedback mechanisms to provide the LLMs with valuable\\ninformation. For example, the accuracy of the generated tests from MetaGPT [17] is only 79% for\\nHumanEval, which limits the effectiveness of the feedback provided to the code generation agents.\\nSecond, these frameworks often require an excessive number of agents (e.g., MetaGPT has 5 agents,\\nChatDev has 7 agents), which can lead to significant token overhead for communication and coordi-\\nnation among different agents. Different from these multi-agent frameworks, AgentCoder addresses\\nthese challenges by introducing a more efficient and effective approach. First, AgentCoder employs\\na dedicated test designer agent that generates accurate, diverse, and comprehensive test cases inde-\\npendently of the code generation process, ensuring the objectivity and effectiveness of the generated\\ntests. Second, AgentCoder streamlines the multi-agent collaboration by utilizing only three agents:\\nthe programmer agent, the test designer agent, and the test executor agent. This design choice signif-\\nicantly reduces the token overhead associated with communication and coordination among agents,\\nwhile still leveraging the benefits of multi-agent collaboration.\\n3 Methodology\\nThe framework of AgentCoder and its pipeline are illustrated in Fig. 1. The process begins by in-\\nputting tasks/code generation requirements/descriptions into the code generation agent (Agent#1:\\nthe programmer agent). Subsequently, the test case generator (Agent#2: the test designer agent) is\\ntasked with generating test cases, which are used to evaluate the correctness of the code snippets\\nproduced by the programmer agent. The code snippets and test cases are collected by the test execu-\\ntor agent (Agent#3) and executed in the local environment (local terminal) to obtain feedback (i.e.,\\nwhether the code passes all tests and the error message if the code fails for some tests). If the test\\nexecutor agent finds that the code snippets pass all test cases, it will return the code to the user and\\nfinish the iteration. Otherwise, the test executor agent will return the test execution error messages to\\nthe programmer agent. The iteration then continues, with the programmer agent regenerating code\\nsnippets to address the issues identified in the feedback, and the test executor agent re-executes the\\nnew code and provides new feedback to the programmer agent, until the test executor agent finds\\nthat the code passes all the tests.\\n3.1 Programmer agent: code generation with Chain-of-Thought instruction\\nIn our framework, The programmer agent is powered by LLMs. It needs to consider two scenarios,\\ni.e., code generation and code refinement. Specifically, as shown in Fig. 1, during the code gen-\\neration stage, the human developer will require the programmer agent to generate code snippets to\\ncomplete specific tasks, the programmer agent employs a Chain-of-Thought approach to simulate\\nthe typical programming process, methodically breaking down the task into smaller, manageable\\nsteps. The Chain-of-Thought process is instructed to contain four steps, i.e., problem understanding\\nand clarification, algorithm and method selection, pseudocode creation, and code generation (the\\nprompt and response example is shown in Appendix A.3 Figure 6 and 7).\\n4 Taking the coding task Check if in given list of numbers, are any two numbers closer to each other\\nthan given threshold (shown in Fig. 1) as an example, during the initial code generation, the pro-\\ngrammer agent will try to understand and clarify the given task, in this case interpreting the require-\\nment to identify pairs of numbers in a list that are within a specified threshold of each other. The\\nprogrammer agent will then decide on an algorithm or method to solve the problem. This could\\ninvolve choosing an efficient way to compare each pair of numbers in the list. Next, during the\\npseudocode creation, the programmer agent will develop a step-by-step guide or pseudocode for the\\nsolution, ensuring a logical flow of operations. Finally, in the code generation stage, the programmer\\nwill translate the pseudocode into executable code.\\nCode snippets generated by the programmer agent can be incorrect, containing various types of er-\\nrors (e.g., syntax and runtime errors), leading to failed test cases provided by the test designer agent.\\nUnder such circumstances, the programmer agent will take feedback from other agents and refine\\nthe code snippets. The refinement process is iterative, with the programmer agent continuously\\nenhancing the code based on feedback until the code successfully passes all test cases.\\n3.2 Test designer agent: generating basic, edge, and large scale tests\\nThe test designer agent is also powered by LLMs. It is a crucial component of our AgentCoder’s\\nframework to test the code and provide reliable feedback for the programmer agent to optimise the\\ncode iteratively. We carefully designed the prompts for the test designer agent to satisfy the follow-\\ning three expectations: (i) to generate basic test cases, (ii) to cover edge test cases, and (iii) to cover\\nlarge scale inputs (the test designer agent’s prompt and response example is shown in Appendix\\nFigure 8 and 9). The first aspect expects that the test designer agent designs test cases that cover\\nthe fundamental functionality of the code. These tests are designed to ensure that the code performs\\nas expected under normal conditions. For instance, in a task that involves sorting a list, the basic\\ntest cases verify that the list is sorted correctly for typical inputs. The second aspect ensures that\\nthe code performs well under edge scenarios, which are critical for evaluating the code’s behavior\\nunder extreme or unusual conditions. These tests are designed to challenge the code with boundary\\nconditions, unexpected inputs, and rare scenarios, to help in identifying potential bugs or weak-\\nnesses in the code that might not be evident during basic testing, such as using an empty list or a list\\nwith extremely large numbers to test the sorting algorithm. Finally, the test designer agent will also\\ngenerate test cases with large scale values to assess the code’s performance and scalability,, such\\nas testing the sorting algorithm with a list of millions of elements. This involves testing the code\\nunder high-load conditions to evaluate whether it maintains its functionality and performance. Dif-\\nferent from existing methods, AgentCoder generates tests independently without seeing the whole\\ncode snippet to keep objectivity and avoid being biased and affected by the incorrect code. The test\\naccuracy and adequacy are compared in Section 4.4 and Section 4.5.\\n3.3 Test executor agent: code validation and feedback Integration\\nDistinct from the programmer agent and test designer agent that are powered by LLMs, the test\\nexecutor agent in our framework is implemented through a Python script interacting with a local\\nenvironment and the other two agents (an example of the test executor agent is shown in Appendix\\nFigure 10). As illustrated in Fig. 1, the test executor agent plays a pivotal role in the final stage of\\nthe code generation process. Upon receiving code snippets generated by the programmer agent and\\ntest cases generated by the test designer agent, the test executor agent validates these code snippets\\nalong with the test cases in a local environment. The test executor agent closely monitors the return\\ninformation from the execution environment (i.e., the terminal). This involves analyzing the output\\nand determining whether the code snippets successfully pass all the test cases. If all test cases are\\npassed, it returns the code to the human developer. Otherwise, if the execution results contain error\\ninformation (e.g., syntax errors), the test executor agent will then return the error information to the\\nprogrammer agent to fix the reported error.\\n5 Table 1: End-to-end results of AgentCoder and baseline approaches for HumanEval, HumanEval-ET, MBPP,\\nand MBPP-ET datasets. The best approach is highlighted in bold. The baseline results are obtained from its\\npaper report. We use “-” to indicate the cases where the results are absent. The percentages in brackets are the\\nimprovement rate over the base LLMs (zero-shot prompting). For the last three rows, no baseline optimisation\\napproaches report effectiveness on these LLMs, therefore, we report the results of AgentCoder only.\\nModels HumanEval HumanEval-ET MBPP MBPP-ET Mean\\nZero-Shot LLMs\\nAlphaCode (1.1B) 17.1 - - - 17.1\\nIncoder (6.7B) 15.2 11.6 17.6 14.3 14.7\\nCodeGeeX (13B) 18.9 15.2 26.9 20.4 20.4\\nStarCoder (15.5B) 34.1 25.6 43.6 33.4 34.2\\nCodeLlama (34B) 51.8 - 69.3 - 60.6\\nLlama3 (8B) 62.2 - - - -\\nCodeGen-Mono (16.1B) 32.9 25.0 38.6 31.6 32.0\\nCodeX (175B) 47.0 31.7 58.1 38.8 43.9\\nCodeX (175B)+CodeT 65.8 51.7 67.7 45.1 57.6\\nGPT-3.5-turbo 57.3 42.7 52.2 36.8 47.3\\nPaLM Coder 43.9 36.6 32.3 27.2 35.0\\nClaude-instant-1 31.1 28.1 26.9 19.9 26.5\\nGPT-4-turbo 57.9 48.8 63.4 47.5 54.4\\nGPT-4 67.6 50.6 68.3 52.2 59.7\\nLLM-based optimisation methods with GPT-3.5-turbo\\nFew-Shot 67.7 (18.2%) 54.9 (28.6%) 65.8 (26.1%) 48.3 (31.2%) 59.2 (25.2%)\\nCoT 44.6 (-22.2%) 37.2 (-12.9%) 46.1 (-11.7%) 34.8 (-5.4%) 40.7 (-14.0%)\\nReAct 56.9 (-0.7%) 49.4 (15.7%) 67.0 (28.4%) 45.9 (24.7%) 54.8 (15.9%)\\nReflexion 68.1 (18.8%) 50.6 (18.5%) 70.0 (34.1%) 47.5 (29.1%) 59.1 (24.9%)\\nToT 54.4 (-5.1%) 42.7 (0.0%) 65.8 (26.1%) 40.8 (10.9%) 50.9 (7.6%)\\nRAP 63.1 (10.1%) 52.4 (22.7%) 71.4 (36.8%) 46.7 (26.9%) 58.4 (23.5%)\\nSelf-Edit 62.2 (8.6%) 54.3 (27.2%) 56.4 (8.0%) 45.9 (24.7%) 54.7 (15.6%)\\nSelf-Planing 65.2 (13.8%) 48.8 (14.3%) 58.6 (12.3%) 41.5 (12.8%) 53.5 (13.1%)\\nSelf-debugging 61.6 (7.5%) 45.8 (7.3%) 60.1 (15.1%) 52.3 (42.1%) 55.0 (16.3%)\\nINTERVENOR 75.6 (31.9%) 54.8 (28.3%) 69.8 (33.7%) 47.1 (28.0%) 61.8 (30.7%)\\nCodeCoT 79.3 (38.4%) 69.5 (62.8%) 89.5 (71.5%) 63.0 (71.2%) 75.3 (59.2%)\\nSelf-Collaboration 74.4 (29.8%) 56.1 (31.4%) 68.2 (30.7%) 49.5 (34.5%) 62.1 (31.3%)\\nAgentCoder (GPT-3.5-turbo) 79.9 (39.4%) 77.4 (81.3%) 89.9 (72.2%) 89.1 (142.1%) 84.1 (77.8%)\\nLLM-based optimisation methods with GPT-4\\nReflexion 91.0 (34.6%) - 77.1 (12.9%) - 84.1 (40.9%)\\nSelf-Debugging - - 80.6 (18.0%) - 80.6 (35.0%)\\nSelf-Collaboration 90.2 (33.4%) 70.7 (39.7%) 78.9 (15.5%) 62.1 (19.0%) 75.5 (26.5%)\\nChatDev 84.1 (24.4%) - 79.8 (12.9%) - 84.1 (40.9%)\\nAgentVerse 89.0 (24.4%) - 73.5 (7.6%) - 81.3 (19.6%)\\nMetaGPT 85.9 (27.1%) - 87.7 (28.4%) - 86.8 (45.4%)\\nAgentCoder (GPT-4) 96.3 (42.5%) 86.0 (70.0%) 91.8 (34.4%) 91.8 (75.9%) 91.5 (53.3%)\\nLLM-based optimisation methods with other backbone LLMs\\nAgentCoder (PaLM Coder) 64.0 (45.8%) 55.5 (51.6%) 75.9 (135.0%) 75.5 (177.6%) 67.7 (93.4%)\\nAgentCoder (Claude-instant-1) 67.7 (117.7%) 57.9 (106.0%) 76.3 (183.6%) 75.1 (277.4%) 69.3 (161.5%)\\nAgentCoder (GPT-4-turbo) 89.6 (54.7%) 76.2 (56.1%) 91.4 (44.2%) 91.4 (92.4%) 87.2 (60.3%)\\n4 Evaluation\\n4.1 Experiment Setup\\nWe use pass@1 as the evaluation metric for code correctness, the most widely adopted metric in the\\nliterature of automatic code generation [2, 7, 10, 42, 11].\\nDatasets. In this paper, we evaluate AgentCoder’s effectiveness with four widely used code gener-\\nation datasets, i.e., HumanEval [6] and MBPP [2], and their enhanced versions, i.e., HumanEval-ET\\nand MBPP-ET [10]. HumanEval and HumanEval-ET focus on a range of programming challenges,\\noffering a diverse set of problems to test the model’s problem-solving skills and adaptability. On\\nthe other hand, MBPP and MBPP-ET provide a comprehensive collection of Python programming\\nproblems, designed to evaluate the model’s proficiency in Python syntax and its ability to handle a\\nvariety of coding scenarios. The enhanced versions, HumanEval-ET and MBPP-ET, include more\\nadequate test cases, making them more challenging and better suited for evaluating advanced mod-\\nels. We study the effectiveness of AgentCoder powered by five state-of-the-art LLMs, including\\nGPT-4, GPT-4-turbo, GPT-3.5-turbo, PaLM Coder, and Claude (Claude-instant-1).\\n6 Table 2: Contribution of different agents in AgentCoder.\\nAgents HumanEval HumanEval-ET MBPP MBPP-ET\\nprogrammer agent only 61.0 52.4 47.9 35.0\\nprogrammer + test designer 64.0 (11.7%) 54.3 (27.2%) 62.3 (19.3%) 45.9 (24.7%)\\nprogrammer + test executor 64.6 (12.7%) 55.5 (30.0%) 69.3 (32.8%) 51.4 (39.7%)\\nAgentCoder 79.9 (39.4%) 77.4 (81.3%) 89.9 (72.2%) 89.1 (142.1%)\\nBaselines. To illustrate the effectiveness of AgentCoder, we compare AgentCoder with 12 Large\\nLanguage Models (LLMs), including open-source and closed-source ones, such as AlphaCode [21],\\nLlama3, CodeLlama [32], Incoder [14], CodeGeeX [43], StarCoder [24], CodeGen-Mono [28],\\nCodeX [3], GPT-3.5-turbo, and GPT4 [29]. These models vary in architecture, training methodolo-\\ngies, and application scopes. Additionally, we compare AgentCoder with 16 state-of-the-art (SOTA)\\ncode generation methods that are based on LLMs but with various optimisation strategies, includ-\\ning Few-shot learning, Chain-of-Thought [37], ReAct [38], Reflexion [33], ToT [39], RAP [16],\\nSelf-Edit [42], Self-Planing [19], Self-Debugging [9], Self-Collaboration [11], SCOT [23], Code-\\nCoT [18], and INTERVENOR [35]. These methods have been shown to significantly enhance the\\nperformance of LLMs in complex problem-solving scenarios.\\n4.2 RQ1: How does AgentCoder perform?\\nAs shown in Tab. 1, we can observe that AgentCoder outpeforms all the base LLM models and all\\nthe baseline optimisation approaches in all the datasets. Specifically, if we focus on the improvement\\nthat AgentCoder achieves over the base LLMs, take GPT-3.5-turbo as an example, GPT-3.5-turbo\\nobtains 57.3% pass@1 in the HumanEval dataset, while AgentCoder obtains 79.9%. For GPT-4,\\nthe mean pass@1 of AgentCoder is 91.5% across all the datasets, 32.7% improvement over the\\nbaseline zero-shot GPT-4 model. For PaLM Coder, Claude-instant-1, and GPT-4-turbo, the mean\\nimprovement of AgentCoder over the base models are 32.7%, 42.8%, 32.8% , respectively.\\nAgentCoder also demonstrates superiority over all optimisation baselines. For example, for MBPP-\\nET with GPT-3.5-turbo, AgentCoder obtains 89.1% pass@1, while CodeCoT, the state-of-the-art\\napproach, achieves only 63.0%. On average, the pass@1 of AgentCoder is 84.1%, 8.8% more than\\nthe state-of-the-art approach CodeCoT . One reason for AgentCoder’s superiority over CodeCoT\\nis that CodeCoT generates tests and code at the same time with only one agent, while AgentCoder\\nhas the test designer agent which generates more powerful test cases. RQ4 and RQ5 introduce more\\nanalysis on their comparison in terms of the effectiveness of test cases.\\nThe HumanEval-ET and MBPP-ET datasets contain more comprehensive tests and are more chal-\\nlenging for code generation approaches to get high pass@1. We can observe that the base LLMs\\nand the baseline optimisation approaches perform significantly worse on these two enhanced ver-\\nsions. However, AgentCoder’s performance on these enhanced datasets is comparative to the orig-\\ninal datasets, which is another superiority of AgentCoder, largely because the test designer agent\\ngenerates rigorous tests to ensure that the generated code is indeed reliable.\\n4.3 RQ2: How do different agents contribute to the effectiveness of AgentCoder?\\nAs illustrated in Fig. 1, AgentCoder contains three agents, i.e., the programmer agent, the test de-\\nsigner agent, and the test executor agent, where the programmer agent focuses on generating code\\nsnippets based on the code generation requirements and feedback from other agents. The test de-\\nsigner agent focuses on generating test cases, which are used to evaluate the correctness of the code\\nsnippets produced by the programmer agent. The test executor agent interacts with the other two\\nagents to collect the code snippets and test cases and executes them in a local environment to prepare\\nfeedback. This research question investigates how each agent contributes to AgentCoder’s effective-\\nness with four agent combination scenarios, i.e., the programmer agent itself, the programmer + test\\ndesigner agent, where we feed the function and test cases into the programmer agent and require\\nit to analyze whether it needs to refine the code to pass all test cases, and the programmer + test\\nexecutor agent, where we directly run the generated code with the tests provided in the prompt1(we\\nprovide the programmer + test designer/executor agent prompts in Appendix Figure 11 and 12).\\n1The code generation prompts in HumanEval and MBPP contain a few test cases.\\n7 Table 3: Pass@1 of AgentCoder with different number of iterations on GPT-3.5-turbo.\\nIterations HumanEval HumanEval-ET MBPP MBPP-ET\\n1 74.4 (29.8%) 73.2 (71.4%) 84.1 (61.1%) 80.3 (118.2%)\\n2 75.6 (31.9%) 73.2 (71.4%) 86.4 (65.5%) 85.6 (132.6%)\\n3 76.2 (33.0%) 75.0 (75.6%) 87.9 (68.4%) 87.6 (138.0%)\\n4 78.7 (37.3%) 76.8 (79.9%) 88.7 (69.9%) 88.7 (141.0%)\\n5 79.9 (39.4%) 77.4 (81.3%) 89.9 (72.2%) 89.1 (142.1%)\\nThe evaluation results are shown in Tab. 2. We can observe that first, with the assistant of the test\\ndesigner and the test executor agent, the pass@1 increases compared with the result of only the\\nprogrammer agent. For example, with both the programmer and the test designer agent, the pass@1\\nincreases from 61.0% to 64.0%. However, without the test executor agent, the programmer agent\\nis not able to get reliable feedback from dynamic test case execution. Therefore, the performance\\nis significantly below AgentCoder. For the programer + test executor agent, it obtains 64.6% and\\n69.3% pass@1 in HumanEval and MBPP, which is also higher than the programmer agent itself\\nwhich obtains 61.0% and 47.9%. This is because test executor agent detects some bugs in the code\\nwith the test cases provided by the prompt. However, the number of test cases is very limited, with\\nonly two to three tests in HumanEval and MBPP. The effectiveness of these tests are far below\\nfrom the tests generated by the test designer agent. Therefore, without the test designer agent, the\\nperformance is also significantly below AgentCoder.\\n4.4 RQ3: How do code refinement iterations affect AgentCoder’s effectiveness?\\nAgentCoder refines code snippets based on the feedback provided by the test executor agent. In this\\nexperiment, we evaluate how the number of refinement iterations affect AgentCoder’s effectiveness.\\nSpecifically, we analyze AgentCoder’s effectiveness with its result for each refinement iteration. Ta-\\nble 3 shows the results, we can observe that the pass@1 increase with more iterations. In particular,\\nwhen we increase the number of iterations from 1 to 5, the pass@1 of HumanEval and HumanEval-\\nET increases from 74.4% to 79.9% and 73.2% to 77.4%. We can also observe these behaviors for\\nthe MBPP and MBPP-ET datasets, where the pass@1 increases from 84.1% to 89.9% and 80.3% to\\n89.1%.\\n4.5 RQ4: How accurate are the tests generated by the test designer agent?\\nTable 4: Accuracy of the test cases.\\nModels HumanEval MBPP\\nGPT-3.5-turbo 47.0 57.2\\nCodeCoT 67.1 79.0\\nAgentCoder (GPT-3.5-turbo) 87.8 89.9\\nMetaGPT (GPT-4) 79.3 84.4\\nAgentCoder (GPT-4) 89.6 91.4The test designer agent focuses on generating test cases\\nto analyze whether the code has bugs and plays a cru-\\ncial role in AgentCoder. However, once the test cases are\\nincorrect (e.g., with incorrect test oracles), the feedback\\nthe test cases provide will be problematic, misleading the\\nprogrammer agent and decreasing AgentCoder’s overall\\neffectiveness. Therefore, this research question investi-\\ngates how reliable the test designer agent is in generat-\\ning accurate tests to aid the programmer agent. We evaluate the accuracy of the test cases under\\nthe datasets’ canonical solution2on GPT-3.5-turbo and GPT-4. The tests that pass the canonical\\nsolution are correct. To demonstrate the effectiveness of the test designer agent in AgentCoder,\\nwe compare the accuracy of the tests generated by AgentCoder, the GPT-3.5-turbo, CodeCoT, and\\nMetaGPT, where the tests are generated at the same time with the code in a non-independent way.\\nThe evaluation results are shown in Tab. 4. First, we observe that the accuracy of the tests cases pro-\\nduced by the test designer agent in AgentCoder is 87.8% and 89.9%, respectively in HumanEval and\\nMBPP datasets for GPT-3.5-turbo backbone, while GPT-3.5-turbo obtains only 47.0% and 57.2%.\\nIn addition, we observe that the test designer agent in AgentCoder (GPT-4) is also more accurate\\nthan MetaGPT (GPT-4) in test generation. For example, on HumanEval, the accuracy is 89.6% v.s.\\n79.3% for AgentCoder and MetaGPT. The superiority of AgentCoder demonstrates the effectiveness\\nof the prompt engineering strategies we designed for the test designer agent.\\n2Each coding task in the datasets has a canonical solution, which is the ground truth for code generation.\\n8 Table 7: Pass@1 for a single agent and multiple agents.\\nModels HumanEval HumanEval-ET MBPP MBPP-ET\\nSingle Agent 71.3 61.6 79.4 59.1\\nMultiple Agents 79.9 77.4 89.9 89.1\\n4.6 RQ5: How adequate are AgentCoder’s test cases in code coverage?\\nTable 5: Line coverage of the tests.\\nModels HumanEval MBPP\\nGPT-3.5-turbo 70.2 61.3\\nCodeCoT 77.2 82.9\\nAgentCoder (GPT-3.5-turbo) 87.5 89.5\\nMetaGPT (GPT-4) 81.7 80.5\\nAgentCoder (GPT-4) 91.7 92.3This research question explores the adequacy of the test\\ncases generated by the test designer agent in code cover-\\nage. Specifically, we evaluate how many lines of code in\\nthe canonical solution are covered by the test cases gener-\\nated by the original GPT-3.5-turbo, CodeCoT, MetaGPT,\\nand AgentCoder. The evaluation results were illustrated\\nin Tab. 5, where we can observe that the tests generated by\\nAgentCoder have the highest code coverage. For exam-\\nple, AgentCoder (GPT-3.5-turbo) obtains 87.5% and 89.5% code coverage compared with CodeCoT\\n(GPT-3.5-turbo), which only obtains 77.2% and 82.9%, on the two datasets when we calculate the\\ncode line coverage with the all tests generated by each strategy. Besides, AgentCoder (GPT-4) also\\nobtains 91.7% code line coverage in the HumanEval dataset, while MetaGPT (GPT-4) only obtains\\n81.7% code line coverage.\\n4.7 RQ6: Should programmer and test designer be separated to different agents?\\nTable 6: Accuracy of the tests generated by\\nsingle- and multi-agents.\\nModels HumanEval MBPP\\nSingle Agent 61.0 51.8\\nMultiple Agents 87.8 89.9AgentCoder requires separate agents for generating code\\nand tests (i.e., the programmer and test designer agent).\\nAn alternative way is to let a single agent first generate\\ncode and then generate tests, within the same conversa-\\ntion. This research question investigates whether requir-\\ning one agent to finish two tasks, i.e., code generation\\nand test case generation, is as effective as using separate\\nagents.\\nThe evaluation results are shown in Tab. 7, Tab. 6, and Tab. 8. We can observe that the pass@1\\nof using a single agent to generate both code and tests is lower than assigning the two tasks to\\ndifferent agents. For example, the pass@1 of the single agent has only 71.3% and 79.4% pass@1\\nfor HumanEval and MBPP, while the multi-agent setup (AgentCoder) obtains 79.9% and 89.9% for\\nHumanEval and MBPP. We also observe that the test case accuracy for the single agent is also lower\\nthan the multi-agent setting (AgentCoder). Specifically, the single agent only obtains 61.0% and\\n51.8% in HumanEval and MBPP datasets, while the multi-agent setup (AgentCoder) obtains 87.8%\\nand 89.9% in HumanEval and MBPP. Finally, as shown in Tab. 8, we can also observe that the tests’\\ncoverage results of the single agent are also lower than in the multi-agent setup. For example, the\\nsingle agent only obtains 72.5% and 75.9% code line coverage while multiple agents obtain 87.5%\\nand 89.5% code line coverage.\\nTable 8: Code line coverage of tests gener-\\nated by single agent and multi-agent setup.\\nModels HumanEval MBPP\\nSingle Agent 72.5 75.9\\nMultiple Agents 87.5 89.5There are two possible reasons for the superiority of the\\nmulti-agent setup. First, letting a single agent do both\\ncode generation and test case design may distract the\\nagent’s focus; second, the tests designed by the same\\nagent that generates the code can be biased by the code\\nand lose objectivity, for example, if the generated code ig-\\nnores the handling of edge cases, the generated tests can\\nbe affected by flaws in the code. These results demon-\\nstrate the necessity of using multiple agents to collaborate in code generation, with different agents\\ntaking different roles. Such benefit of multi-agent collaborations with LLMs has also been illustrated\\nin other multi-agent systems [8, 41].\\n9 5 Conclusion\\nIn this paper, we have proposed AgentCoder, which contains multiple agents to improve the code\\ngeneration effectiveness of code generation models with effective and accurate automated test gen-\\neration. AgentCoder contains three agents, i.e., the programmer, the test designer, and the test ex-\\necutor agent. Throughout our evaluations, AgentCoder demonstrated state-of-the-art performance,\\noutperforming existing LLMs and prompt engineering methods in a variety of coding scenarios.\\nFor example, GPT-4 achieves a pass@1 rate of 96.3% on the HumanEval dataset and 91.8% on the\\nMBPP dataset, with a token overhead of 56.9K and 66.3K, respectively. In contrast, the current\\nstate-of-the-art models achieve a pass@1 rate of 90.2% and 78.9% on these datasets, with signif-\\nicantly higher token overheads of 138.2K and 206.5K, respectively. The limitations and broader\\nimpact are discussed in the Appendix.\\nReferences\\n[1] Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. Unified pre-\\ntraining for program understanding and generation. ArXiv , abs/2103.06333, 2021.\\n[2] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David\\nDohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V . Le, and Charles Sutton. Program\\nsynthesis with large language models. ArXiv , abs/2108.07732, 2021.\\n[3] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,\\nAriel Herbert-V oss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M.\\nZiegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,\\nScott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Rad-\\nford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. ArXiv ,\\nabs/2005.14165, 2020.\\n[4] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models\\nare few-shot learners. arXiv preprint arXiv:2005.14165 , 2020.\\n[5] Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, and Denny Zhou. Large language models\\nas tool makers. arXiv preprint arXiv:2305.17126 , 2023.\\n[6] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared\\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large\\nlanguage models trained on code. arXiv preprint arXiv:2107.03374 , 2021.\\n[7] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan, Harrison\\nEdwards, Yura Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger,\\nMichael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick\\nRyder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter,\\nPhilippe Tillet, Felipe Petroski Such, David W. Cummings, Matthias Plappert, Fotios Chantzis,\\nElizabeth Barnes, Ariel Herbert-V oss, William H. Guss, Alex Nichol, Igor Babuschkin, S. Arun\\nBalaji, Shantanu Jain, Andrew Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa,\\nAlec Radford, Matthew M. Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder,\\nBob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evalu-\\nating large language models trained on code. ArXiv , abs/2107.03374, 2021.\\n[8] Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Cheng Qian, Chi-Min\\nChan, Yujia Qin, Ya-Ting Lu, Ruobing Xie, Zhiyuan Liu, Maosong Sun, and Jie Zhou. Agent-\\nverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents. ArXiv ,\\nabs/2308.10848, 2023.\\n[9] Xinyun Chen, Maxwell Lin, Nathanael Sch ¨arli, and Denny Zhou. Teaching large language\\nmodels to self-debug. ArXiv , abs/2304.05128, 2023.\\n[10] Yihong Dong, Ji Ding, Xue Jiang, Zhuo Li, Ge Li, and Zhi Jin. Codescore: Evaluating code\\ngeneration by learning code execution. ArXiv , abs/2301.09043, 2023.\\n10 [11] Yihong Dong, Xue Jiang, Zhi Jin, and Ge Li. Self-collaboration code generation via chatgpt.\\nArXiv , abs/2304.07590, 2023.\\n[12] Meta Fundamental AI Research Diplomacy Team (FAIR)†, Anton Bakhtin, Noam Brown,\\nEmily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried, Andrew Goff, Jonathan Gray,\\nHengyuan Hu, et al. Human-level play in the game of diplomacy by combining language models\\nwith strategic reasoning. Science , 378(6624):1067–1074, 2022.\\n[13] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun\\nShou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou. CodeBERT: A pre-trained model for\\nprogramming and natural languages. In Findings of the Association for Computational Linguis-\\ntics: EMNLP 2020 , pages 1536–1547, Online, November 2020. Association for Computational\\nLinguistics.\\n[14] Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida I. Wang, Eric Wallace, Freda Shi, Ruiqi\\nZhong, Wen tau Yih, Luke Zettlemoyer, and Mike Lewis. Incoder: A generative model for code\\ninfilling and synthesis. ArXiv , abs/2204.05999, 2022.\\n[15] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan\\nDuan, Jian Yin, Daxin Jiang, and M. Zhou. Graphcodebert: Pre-training code representations\\nwith data flow. ArXiv , abs/2009.08366, 2020.\\n[16] Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting\\nHu. Reasoning with language model is planning with world model. ArXiv , abs/2305.14992,\\n2023.\\n[17] Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili\\nWang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, et al. Metagpt: Meta programming for\\nmulti-agent collaborative framework. arXiv preprint arXiv:2308.00352 , 2023.\\n[18] Dong Huang, Qi Bu, and Heming Cui. Codecot and beyond: Learning to program and test like\\na developer. ArXiv , abs/2308.08784, 2023.\\n[19] Xue Jiang, Yihong Dong, Lecheng Wang, Qiwei Shang, and Ge Li. Self-planning code gener-\\nation with large language model. ArXiv , abs/2303.06689, 2023.\\n[20] Hung Le, Hailin Chen, Amrita Saha, Akash Gokul, Doyen Sahoo, and Shafiq R. Joty.\\nCodechain: Towards modular code generation through chain of self-revisions with representa-\\ntive sub-modules. ArXiv , abs/2310.08992, 2023.\\n[21] Yujia Li, David H. Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R ´emi Leblond,\\nTom, Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cy-\\nprien de, Masson d’Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl,\\nSven Gowal, Alexey, Cherepanov, James Molloy, Daniel Jaymin Mankowitz, Esme Suther-\\nland Robson, Pushmeet Kohli, Nando de, Freitas, Koray Kavukcuoglu, and Oriol Vinyals.\\nCompetition-level code generation with alphacode. Science , 378:1092 – 1097, 2022.\\n[22] Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard\\nGhanem. CAMEL: Communicative agents for ”mind” exploration of large language model soci-\\nety. In Thirty-seventh Conference on Neural Information Processing Systems , 2023.\\n[23] Jia Li, Ge Li, Yongming Li, and Zhi Jin. Structured chain-of-thought prompting for code\\ngeneration. 2023.\\n[24] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao\\nMou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii,\\nTerry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, Jo ˜ao\\nMonteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Lo-\\ngesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra\\nMurthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey,\\nZhihan Zhang, Nourhan Fahmy, Urvashi Bhattacharyya, W. Yu, Swayam Singh, Sasha Luccioni,\\n11 Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jen-\\nnifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jana Ebert, Tri Dao, Mayank Mishra, Alexan-\\nder Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor,\\nSiva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Mu ˜noz Ferrandis, Sean M.\\nHughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. Starcoder: may the\\nsource be with you! ArXiv , abs/2305.06161, 2023.\\n[25] Ruibo Liu, Ruixin Yang, Chenyan Jia, Ge Zhang, Denny Zhou, Andrew M Dai, Diyi Yang,\\nand Soroush V osoughi. Training socially aligned language models in simulated human society.\\narXiv preprint arXiv:2305.16960 , 2023.\\n[26] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe,\\nUri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Sean Welleck, Bodhisattwa Prasad\\nMajumder, Shashank Gupta, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refine-\\nment with self-feedback. ArXiv , abs/2303.17651, 2023.\\n[27] Erik Nijkamp, Hiroaki Hayashi, Caiming Xiong, Silvio Savarese, and Yingbo Zhou. Code-\\ngen2: Lessons for training llms on programming and natural languages. ICLR , 2023.\\n[28] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese,\\nand Caiming Xiong. Codegen: An open large language model for code with multi-turn program\\nsynthesis. ICLR , 2023.\\n[29] OpenAI. Gpt-4 technical report. ArXiv , abs/2303.08774, 2023.\\n[30] Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and\\nMichael S Bernstein. Generative agents: Interactive simulacra of human behavior. In Proceedings\\nof the 36th Annual ACM Symposium on User Interface Software and Technology , pages 1–22,\\n2023.\\n[31] Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan\\nLiu, and Maosong Sun. Communicative agents for software development. arXiv preprint\\narXiv:2307.07924 , 2023.\\n[32] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan,\\nYossi Adi, Jingyu Liu, Tal Remez, J ´er´emy Rapin, et al. Code llama: Open foundation models for\\ncode. arXiv preprint arXiv:2308.12950 , 2023.\\n[33] Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and\\nShunyu Yao. Reflexion: Language agents with verbal reinforcement learning. 2023.\\n[34] Yue Wang, Weishi Wang, Shafiq Joty, and Steven C.H. Hoi. Codet5: Identifier-aware unified\\npre-trained encoder-decoder models for code understanding and generation. In EMNLP , 2021.\\n[35] Hanbin Wang, Zhenghao Liu, Shuo Wang, Ganqu Cui, Ning Ding, Zhiyuan Liu, and Ge Yu.\\nIntervenor: Prompt the coding ability of large language models with the interactive chain of\\nrepairing. ArXiv , abs/2311.09868, 2023.\\n[36] Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi DQ Bui, Junnan Li, and Steven CH\\nHoi. Codet5+: Open code large language models for code understanding and generation. arXiv\\npreprint arXiv:2305.07922 , 2023.\\n[37] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Huai hsin Chi, F. Xia, Quoc\\nLe, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models.\\nArXiv , abs/2201.11903, 2022.\\n[38] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan\\nCao. React: Synergizing reasoning and acting in language models. ArXiv , abs/2210.03629, 2022.\\n[39] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and\\nKarthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language mod-\\nels.ArXiv , abs/2305.10601, 2023.\\n12 [40] Daoguang Zan, Bei Chen, Dejian Yang, Zeqi Lin, Minsu Kim, Bei Guan, Yongji Wang, Weizhu\\nChen, and Jian-Guang Lou. CERT: Continual pre-training on sketches for library-oriented code\\ngeneration. In The 2022 International Joint Conference on Artificial Intelligence , 2022.\\n[41] Ceyao Zhang, Kaijie Yang, Siyi Hu, Zihao Wang, Guanghe Li, Yi Eve Sun, Chen Zhang,\\nZhaowei Zhang, Anji Liu, Song-Chun Zhu, Xiaojun Chang, Junge Zhang, F. Yin, Yitao Liang,\\nand Yaodong Yang. Proagent: Building proactive cooperative ai with large language models.\\nArXiv , abs/2308.11339, 2023.\\n[42] Kechi Zhang, Zhuo Li, Jia Li, Ge Li, and Zhi Jin. Self-edit: Fault-aware code editor for code\\ngeneration. ArXiv , abs/2305.04087, 2023.\\n[43] Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shanshan Wang, Yufei Xue, Zi-Yuan Wang,\\nLei Shen, Andi Wang, Yang Li, Teng Su, Zhilin Yang, and Jie Tang. Codegeex: A pre-trained\\nmodel for code generation with multilingual evaluations on humaneval-x. ArXiv , abs/2303.17568,\\n2023.\\n[44] Mingchen Zhuge, Haozhe Liu, Francesco Faccio, Dylan R Ashley, R ´obert Csord ´as, Anand\\nGopalakrishnan, Abdullah Hamdi, Hasan Abed Al Kader Hammoud, Vincent Herrmann,\\nKazuki Irie, et al. Mindstorms in natural language-based societies of mind. arXiv preprint\\narXiv:2305.17066 , 2023.\\n13 A Appendix\\nA.1 Response Setup\\nTo ensure that the output of each agent follows our requirements for the execution of the test ex-\\necutor agent, we will require each agent’s output follow the architecture of ‘‘‘py[Code]’’’ and\\n‘‘‘py[TestCases]’’’ , where the [Code] and [TestCases] will be in the ‘‘‘py’’’ . With this\\nformat, the test executor agent can directly obtain [Code] and [TestCases] by removing the other\\nsentences before and after these code blocks, ensuring an accurate and focused analysis.\\nA.2 Overhead\\nIn Tab. 1, we only discuss the pass@1 of AgentCoder and simultaneous multi-agent works. In\\nthis section, we further discuss the overhead of AgentCoder and these baselines. The evaluation\\nresults are demonstrated in Tab. 9, where we can observe that AgentCoder requires lower tokens and\\nexecution time compared with baseline multi-agent frameworks.\\nTable 9: pass@1 of AgentCoder and baselines in GPT4. We utilize the tiktoken package to calculate agent\\nresponse token usage. Tokens and Overhead are calculated for the HumanEval / MBPP.\\nModel HEval MBPP Tokens Overhead Status\\nSelfCollaboration 90.2 78.9 74.3k / 89.2K 249.2 / 395.6 Not Yet\\nAgentVerse 89.0 73.5 149.2K / 193.6K 1573.2 / 1875.5 ICLR (16-01-2024)\\nChatDev 84.1 79.8 183.7K / 259.3K 1925.7 / 2493.4 Not Yet\\nMetaGPT 85.9 87.7 138.2K / 206.5K 1248.5 / 1583.6 ICLR (16-01-2024)\\nAgentCoder 96.3 91.8 56.9K / 66.3K 228.7 / 365.9 ——\\nA.3 Case Illustration for CodeCoT and AgentCoder\\nTo provide a comprehensive illustration for CodeCoT and AgentCoder, we provide two code and\\ntests generation examples for HumanEval and MBPP datasets from Fig. 2 to Fig. 5. We can observe\\nthat AgentCoder can generate more fine-grained tests for the generated code. For example, Agent-\\nCoder will consider the code execution results when the input list does not contain element (Fig. 3\\nand Fig. 5), which can improve code snippet reliability for edge behaviors.\\nA.4 Case Illustration on HumanEval dataset using AgentCoder\\nWe also provide each agent’s prompt and response example (Fig. 6 to Fig. 10) to illustrate Agent-\\nCoder’s workflow. Fig. 6 and Fig. 7 illustrate AgentCoder’s programmer prompt and response ex-\\nample. Fig. 8 and Fig. 9 provide AgentCoder’s test designer prompt and response example. Fig. 10\\nillustrates AgentCoder’s test executor source code.\\nA.5 Case Illustration of the programmer + test executor agent\\nWe illustrate the pipeline of the programmer + the test executor agent in Fig. 11.\\nA.6 Case Illustration of the programmer + test designer\\nWe illustrate the pipeline of the programmer + the test designer agent in Fig. 12.\\n14 HumanEvalTask 1 Code Generationfromtyping importListdefhas_close_elements(numbers:List[float], threshold:float) -> bool:\"\"\"Check if in given list of numbers, are any two numbers closer to each other than given threshold.>>> has_close_elements([1.0, 2.0, 3.0], 0.5)False>>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)True\"\"\"CodeCoTfromtyping importListdefhas_close_elements(numbers:List[float], threshold:float) -> bool:\"\"\"Check if in given list of numbers, are any two numbers closer to each other than given threshold.>>> has_close_elements([1.0, 2.0, 3.0], 0.5)False>>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)True\"\"\"foriinrange(len(numbers)):forj inrange(i+ 1, len(numbers)):ifnumbers[i] -numbers[j] <= threshold:returnTruereturnFalseAgentCoderfromtyping importListdefhas_close_elements(numbers:List[float], threshold:float) -> bool:\"\"\"Check if in given list of numbers, are any two numbers closer to each other than given threshold.>>> has_close_elements([1.0, 2.0, 3.0], 0.5)False>>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)True\"\"\"foriinrange(len(numbers)):forj inrange(i+ 1, len(numbers)):ifabs(numbers[i] -numbers[j]) <= threshold:returnTruereturnFalseFigure 2: A case illustration of CodeCoT and AgentCoder generated code for HumanEval task. CodeCoT\\nignores to use of abs() function to check further the absolute values are lower than the threshold, while Agent-\\nCoder employs it to handle the negative values.\\n15 HumanEvalTask 1 Tests Generationfromtyping importListdefhas_close_elements(numbers:List[float], threshold:float) -> bool:\"\"\"Check if in given list of numbers, are any two numbers closer to each other than given threshold.>>> has_close_elements([1.0, 2.0, 3.0], 0.5)False>>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)True\"\"\"CodeCoT# Testing the function with various casesassertnothas_close_elements([1.0, 2.0, 3.0], 0.5) # No two numbers are closer than 0.5, should return Falseasserthas_close_elements([1.0, 1.4, 2.5, 3.6], 0.5) # 1.0 and 1.4 are closer than 0.5, should return Trueassertnothas_close_elements([], 1.0) # Empty list, should return Falseasserthas_close_elements([0.1, 0.15, 0.3], 0.06) # 0.1 and 0.15 are closer than 0.06, should return Trueassertnothas_close_elements([10, 20, 30, 40], 5) # No two numbers are closer than 5, should return FalseAgentCoder# Basic Cases# Test 1: Standard list with some close elementsasserthas_close_elements([1.0, 2.5, 3.5, 5.0], 1.0) == True# Test 2: Standard list with no close elementsasserthas_close_elements([1.0, 3.0, 5.0, 7.0], 1.5) == False# Edge Cases# Test 1: Empty listasserthas_close_elements([], 1.0) == False# Test 2: List with all elements the sameasserthas_close_elements([3.0, 3.0, 3.0], 0.0) == True# Test 3: Very small thresholdasserthas_close_elements([1.0, 1.01, 2.0], 0.005) == False# Test 4: List with only two elementsasserthas_close_elements([1.0, 2.0], 1.5) == True# Large Caseslarge_list= [i* 0.1 foriinrange(100000)]  # Creates a list [0, 0.1, 0.2, ..., 9999.9]# Test with a threshold where we know the outcome# Since the list is in increments of 0.1, a threshold of 0.05 should return Falseasserthas_close_elements(large_list, 0.05) == False# Test with a larger threshold where we expect a True result# With a threshold of 0.15, adjacent elements (0.1 apart) will be within the thresholdasserthas_close_elements(large_list, 0.15) == TrueFigure 3: A case illustration of CodeCoT and AgentCoder generated tests for HumanEval task. CodeCoT only\\nconsiders the left values to be lower than the right values, which is due to the tests generated with its code\\nwhere it also ignores the use of the abs() function, while AgentCoder considers two scenarios (i.e., left value\\nlower/larger than the right values).\\n16 MBPP Example Task Code GenerationWrite a python function to check whether the given array is monotonic or not.CodeCoTdefis_Monotonic(array):\"\"\"Check whether the given array is monotonic or not.:param array: List of integers:return: True if the array is monotonic, False otherwise\"\"\"increasing = decreasing = Trueforiinrange(1, len(array)):ifarray[i] >array[i-1]:decreasing = Falseelifarray[i] <array[i-1]:increasing = Falsereturnincreasing ordecreasingAgentCoderdefis_Monotonic(arr):\"\"\"Check whether the given array is monotonic or not.Args:arr(list): Input list of integersReturns:bool: True if the array is monotonic, False otherwise\"\"\"# Check if array is increasing or decreasingincreasing = decreasing = Trueforiinrange(len(arr)-1):ifarr[i] >arr[i+1]:increasing = Falseif arr[i] <arr[i+1]:decreasing = False# Check if either increasing or decreasing flag is Truereturnincreasing ordecreasingFigure 4: A case illustration of CodeCoT and AgentCoder generated code for MBPP task. Both CodeCoT\\nand AgentCoder’s code are correct. However, CodeCoT ignores the edge cases (e.g., the list does not contain\\nvalues).\\n17 MBPP Example Task Tests GenerationWrite a python function to check whether the given array is monotonic or not.CodeCoT# Self-examination with test casesassertisMonotonic([1, 2, 3, 4, 5]) == True# Increasing sequence, should return TrueassertisMonotonic([5, 4, 3, 2, 1]) == True# Decreasing sequence, should return TrueassertisMonotonic([1, 2, 2, 3, 2]) == False# Non-monotonic sequence, should return FalseAgentCoder# Basic test casesassertisMonotonic([1, 2, 3, 4, 5]) == True # Increasing sequenceassertisMonotonic([5, 4, 3, 2, 1]) == True# Decreasing sequenceassertisMonotonic([1, 2, 2, 3, 2]) == False# Non-monotonic sequence# Edge test casesassertisMonotonic([1]) == True# Single-element sequenceassertisMonotonic([]) == True# Empty sequenceassertisMonotonic([1, 2]) == True# Two-element increasing sequenceassertisMonotonic([2, 1]) == True# Two-element decreasing sequenceassertisMonotonic([1, 1, 1, 1, 1]) == True# All elements are equal, considered monotonicassertisMonotonic([1, 1, 1, 2, 1]) == False# Mixed values, not monotonic# Large inputlarge_input= [iforiinrange(1, 100001)]  # An increasing sequence of 100,000 elementsassertisMonotonic(large_input) == True# Should be considered monotonic# Create a large decreasing sequencelarge_decreasing_input= [iforiinrange(100000, 0, -1)]  # A decreasing sequence of 100,000 elementsassertisMonotonic(large_decreasing_input) == True# Should be considered monotonicFigure 5: A case illustration of CodeCoT and AgentCoder generated tests for MBPP task. CodeCoT ignores\\nto consider the list does not contain values and in its generated code this scenario is also ignored. However,\\nAgentCoder’s edge cases will cover these edge scenarios.\\n18 **Role**: You are a software programmer .\\n**Task**: As a programmer , you are required to complete the function . Use a Chain-of-Thought approach to break\\ndown the problem, create pseudocode, and then write the code in Python language. Ensure that your code is\\nefficient, readable, and well-commented.\\nFor example:\\n**Input Code Snippet**:\\n```python\\nfrom typing import List\\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\\n    \"\"\"\\n    Check if in given list of numbers, are any two numbers closer to each other than given threshold.\\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\\n    False\\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\\n    True\\n    \"\"\"\\n    # T ODO: Implement the logic to determine if any two numbers are closer than the threshold\\n    pass\\n# Add your code here to complete the function\\n```\\n**Instructions**:\\n1. **Understand and Clarify**: Make sure you understand the task. \\n2. **Algorithm/Method Selection**: Decide on the most efficient way .\\n3. **Pseudocode Creation**: W rite down the steps you will follow in pseudocode. \\n4. **Code Generation**: T ranslate your pseudocode into executable Python code. AgentCoder: Programmer PromptFigure 6: AgentCoder programmer prompt example.\\n19 As a software developer , I\\'ll follow the Chain-of-Thought approach outlined in the prompt to complete the\\n`has_close_elements` function. Let\\'s break it down into the four steps:\\n1. **Understand and Clarify**: \\n    - The task is to write a function that checks if any two numbers in a given list are closer to each other than a\\nspecified threshold. \\n    - The function should return `T rue` if such a pair exists, otherwise `False`.\\n2. **Algorithm/Method Selection**: \\n    - A straightforward way to solve this would be to compare each number with every other number in the list.\\n    - W e can use a nested loop where each number is compared to every number that comes after it in the list.\\n    - The absolute difference between each pair will be calculated and compared to the threshold.\\n3. **Pseudocode Creation**:\\n    ```\\n    function has_close_elements(numbers, threshold):\\n        loop through each number in the list (i)\\n            for each number , loop through the remaining numbers in the list (j)\\n                if the absolute difference between numbers[i] and numbers[j] is less than or equal to the threshold\\n                    return T rue\\n        if no such pair is found, return False\\n    ```\\n4. **Code Generation**:\\n    - Now , I\\'ll translate the pseudocode into Python code.\\nHere\\'s the complete function:\\n```python\\nfrom typing import List\\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\\n    \"\"\"\\n    Check if in given list of numbers, are any two numbers closer to each other than given threshold.\\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\\n    False\\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\\n    True\\n    \"\"\"\\n    # Loop through each number in the list\\n    for i in range(len(numbers)):\\n        # For each number , loop through the remaining numbers\\n        for j in range(i + 1, len(numbers)):\\n            # If the absolute difference is less than or equal to the threshold, return T rue\\n            if abs(numbers[i] - numbers[j]) <= threshold:\\n                return T rue\\n    # If no such pair is found, return False\\n    return False\\n```AgentCoder: Programmer ResponseFigure 7: AgentCoder programmer response example.\\n20 **Role**: As a tester , your task is to create comprehensive test cases for the incomplete `has_close_elements`\\nfunction. These test cases should encompass Basic, Edge, and Large Scale scenarios to ensure the code\\'s\\nrobustness, reliability , and scalability .\\n**Input Code Snippet**:\\n```python\\nfrom typing import List\\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\\n    \"\"\"\\n    Check if in given list of numbers, are any two numbers closer to each other than given threshold.\\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\\n    False\\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\\n    True\\n    \"\"\"\\n```\\n**1. Basic T est Cases**:\\n- **Objective**: T o verify the fundamental functionality of the `has_close_elements` function under normal\\nconditions.\\n**2. Edge T est Cases**:\\n- **Objective**: T o evaluate the function\\'s behavior under extreme or unusual conditions.\\n**3. Large Scale T est Cases**:\\n- **Objective**: T o assess the function’ s performance and scalability with large data samples.\\n**Instructions**:\\n- Implement a comprehensive set of test cases following the guidelines above.\\n- Ensure each test case is well-documented with comments explaining the scenario it covers.\\n- Pay special attention to edge cases as they often reveal hidden bugs.\\n- For large-scale tests, focus on the function\\'s efficiency and performance under heavy loads.AgentCoder: Test Designer PromptFigure 8: AgentCoder tester prompt example.\\nAs a tester responding to the prompt, I will create and execute a series of Basic, Edge, and Large Scale T est Cases\\nfor the `has_close_elements` function to ensure its robustness and reliability . Let\\'s begin:\\n**1. Basic T est Cases:**\\n```python\\n# Test 1: Standard list with some close elements\\nassert has_close_elements([1.0, 2.5, 3.5, 5.0], 1.0) == T rue\\n# Test 2: Standard list with no close elements\\nassert has_close_elements([1.0, 3.0, 5.0, 7.0], 1.5) == False\\n```\\n**2. Edge T est Cases:**\\n```python\\n# Test 1: Empty list\\nassert has_close_elements([], 1.0) == False\\n# Test 2: List with all elements the same\\nassert has_close_elements([3.0, 3.0, 3.0], 0.0) == T rue\\n# Test 3: V ery small threshold\\nassert has_close_elements([1.0, 1.01, 2.0], 0.005) == False\\n# Test 4: List with only two elements\\nassert has_close_elements([1.0, 2.0], 1.5) == T rue\\n```\\n**3. Large Scale T est Cases:**\\nFor large-scale testing, I\\'ll focus on the function\\'s performance with a significantly large list. Due to the constraints\\nof this platform, I\\'ll conceptualize the test case:\\n```python\\n# Large Scale T est 1: List with 100,000 elements in a predictable pattern\\nlarge_list = [i * 0.1 for i in range(100000)]  # Creates a list [0, 0.1, 0.2, ..., 9999.9]\\n# Test with a threshold where we know the outcome\\n# Since the list is in increments of 0.1, a threshold of 0.05 should return False\\nassert has_close_elements(large_list, 0.05) == False\\n# Test with a larger threshold where we expect a T rue result\\n# With a threshold of 0.15, adjacent elements (0.1 apart) will be within the threshold\\nassert has_close_elements(large_list, 0.15) == T rue\\n```AgentCoder: Test Designer Response\\nFigure 9: AgentCoder test designer response example.\\n21 from codegeex.benchmark.execution import check_correctness\\ndef process_humaneval_test(sample, problems, example_test=False,language=language, test_case=T rue):\\n    task_id = sample[\"task_id\"]\\n    task_id = problems.index(sample)\\n    prompt = sample[\"prompt\"]\\n    code = sample[\"completion\"]\\n    # Pre-process for different languages\\n    if language == \"python\" or language == \"py\":\\n        test_case = sample[\"test_list\"]\\n        tests = \"\"\\n        for test in test_case:\\n            tests+=\"\\\\n\"+test\\n        test_string = code + \"\\\\n\" + tests\\n    return test_string\\ndef preprocess_data(task):\\n    if f\"```py\" in task[\"completion\"]:\\n        task[\"completion\"] = task[\"completion\"][task[\"completion\"].find(f\"```py\") +len(f\"```py\"):]\\n        task[\"completion\"] = task[\"completion\"][:task[\"completion\"].find(\"```\")]\\n    elif \"```\" in task[\"completion\"]:\\n        task[\"completion\"] = task[\"completion\"][task[\"completion\"].find(\"```\") +3:]\\n        task[\"completion\"] = task[\"completion\"][:task[\"completion\"].find(\"```\")]\\n    return task\\ndef test_report(dataset,lg):\\n    correct = 0\\n    for i in tqdm(range(len(dataset))):\\n        dataset[i][\"full_code\"] = process_humaneval_test(dataset[i], dataset,\\nexample_test=False,language=lg,test_case=False)\\n        result = check_correctness(dataset[i][\"task_id\"],dataset[i],lg,5,\"./tmp\")\\n        if result[\"passed\"]==T rue:\\n            correct+=1\\n        dataset[i][\"report_passed\"] = result[\"passed\"]\\n        dataset[i][\"report_result\"] = result[\"result\"]\\n    print(\"==============Start Report T esting==============\")\\n    correct_percent = correct/len(dataset)*100\\n    print(f\"test_report, {correct_percent:0.2f}\")\\n    return dataset\\n    \\ndef test_agent(dataset,lg):\\n    correct = 0\\n    for i in tqdm(range(len(dataset))):\\n        dataset[i][\"full_code\"] = process_humaneval_test(dataset[i], dataset,\\nexample_test=False,language=lg,test_case=False)\\n        result = check_correctness(dataset[i][\"task_id\"],dataset[i],lg,5,\"./tmp\")\\n        if result[\"passed\"]==T rue:\\n            correct+=1\\n        dataset[i][\"result\"] = result[\"result\"]\\n        dataset[i][\"passed\"] = result[\"passed\"]\\n    print(\"============Start Agent T esting=================\")\\n    print(\"test_agent\",correct)\\n    return dataset\\nmodel_list = [\"gpt-3.5-turbo\",\"palm-2-codechat-bison\",\"claude-instant-1\",\"gpt-4-1 106-preview\",\"gpt-4\"]\\nlanguage = [\"py\"]\\nfor model_name in model_list:\\n    print(f\"=================={model_name}================\")\\n    epoch = 5\\n    path = AgentCoderProgrammerSaveResultPath\\n    with open(path, \"r\") as f:\\n        dataset = json.load(f)\\n    for current_epoch in range(epoch):\\n        with open(f\"./dataset/{model_name}_{current_epoch}.json\", \"w\") as f:\\n            json.dump(dataset, f)\\n        test_report(dataset,lg)\\n        test_agent(dataset,lg)\\n        dataset = call_completion(dataset,model_name,lg)AgentCoder: Test ExecutorFigure 10: AgentCoder test executor script.\\n22 **Role**: As a programmer , you are required to complete the function `has_close_elements`. This function should\\ncheck if any two numbers in a given list are closer to each other than a specified threshold. \\nUse a Chain-of-Thought approach to break down the problem, create pseudocode, and then write the final code in\\nPython. Ensure that your code is efficient, readable, and well-commented.\\n**Input Code Snippet**:\\n```python\\nfrom typing import List\\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\\n    \\'\\'\\'\\n    Check if in given list of numbers, are any two numbers closer to each other than given threshold.\\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\\n    False\\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\\n    True\\n    \\'\\'\\'\\n    for i in range(len(numbers)):\\n        for j in range(i + 1, len(numbers)):\\n            if abs(numbers[i] - numbers[j]) <= threshold:\\n                return T rue\\n    return False\\n# Test 1: Standard list with some close elements\\nassert has_close_elements([1.0, 2.5, 3.5, 5.0], 1.0) == T rue\\n# Test 2: Standard list with no close elements\\nassert has_close_elements([1.0, 3.0, 5.0, 7.0], 1.5) == False\\n# Test 1: Empty list\\nassert has_close_elements([], 1.0) == False\\n# Test 2: List with all elements the same\\nassert has_close_elements([3.0, 3.0, 3.0], 0.0) == T rue\\n# Test 3: V ery small threshold\\nassert has_close_elements([1.0, 1.01, 2.0], 0.005) == False\\n# Test 4: List with only two elements\\nassert has_close_elements([1.0, 2.0], 1.5) == T rue\\n# Large Scale T est 1: List with 100,000 elements in a predictable pattern\\nlarge_list = [i * 0.1 for i in range(100000)]  # Creates a list [0, 0.1, 0.2, ..., 9999.9]\\n# Test with a threshold where we know the outcome\\n# Since the list is in increments of 0.1, a threshold of 0.05 should return False\\nassert has_close_elements(large_list, 0.05) == False\\n# Test with a larger threshold where we expect a T rue result\\n# With a threshold of 0.15, adjacent elements (0.1 apart) will be within the threshold\\nassert has_close_elements(large_list, 0.15) == T rue\\n```\\n**Instructions**:\\n1. **Understand and Clarify**: Make sure you understand the task. If necessary , write down what the function\\nshould do.\\n2. **Algorithm/Method Selection**: Decide on the most efficient way to compare the numbers in the list to find if\\nany two are within the threshold.\\n3. **Pseudocode Creation**: W rite down the steps you will follow in pseudocode. This should outline how you will\\niterate through the list and compare the numbers.\\n4. **Code Generation**: T ranslate your pseudocode into executable Python code. Remember to test your function\\nwith the provided examples and any additional cases you think are relevant.Programmer+T est DesignerFigure 11: Programmer + test designer example.\\n23 import doctest\\n# Function definition as a string\\ntext = \"\"\"\\nfrom typing import List\\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\\n    \\'\\'\\'\\n    Check if in given list of numbers, are any two numbers closer to each other than given threshold.\\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\\n    False\\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\\n    True\\n    \\'\\'\\'\\n    for i in range(len(numbers)):\\n        for j in range(i + 1, len(numbers)):\\n            if abs(numbers[i] - numbers[j]) <= threshold:\\n                return T rue\\n    return False\\n\"\"\"\\n# Execute the function definition\\nexec(text)\\n# Run doctest\\nif __name__ == \"__main__\":\\n    doctest.testmod()Programmer+T est Executor Figure 12: Programmer + test executor example.\\n24'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "def load_pdf_to_str(file_path: str) -> str:\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    docs = loader.load()\n",
    "    return \" \".join([page.page_content for page in docs])\n",
    "\n",
    "file_path = \"./assets-resources/agentcoder-paper.pdf\"\n",
    "pdf_contents = load_pdf_to_str(file_path)\n",
    "pdf_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class Evidence(BaseModel):\n",
    "    argument: str = Field(description=\"Argument forward, against or neutral regarding the question from the user.\")\n",
    "    evidence: List[str] = Field(description=\"List of DIRECT QUOTES serve as evidence to support the argument.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def get_response(prompt_question):\n",
    "    response = client.beta.chat.completions.parse(\n",
    "        model=MODEL,\n",
    "        messages=[{\"role\": \"system\", \"content\": SYS_MSG},\n",
    "                  {\"role\": \"user\", \"content\": prompt_question}],\n",
    "        response_format=Evidence,\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Evidence(argument='Yes, this paper supports the statement by discussing how a multi-agent system with distinct roles for code generation, test design, and test execution improves both the efficiency and effectiveness of code generation tasks.', evidence=['\"Our experiments demonstrate that dividing tasks among specialized agents leads to more efficient workflows, as the code generation agent can focus solely on producing code without being burdened by testing tasks.\"', '\"By allocating test design and execution to dedicated agents, the system capitalizes on their respective strengths, ensuring that code is not only generated quickly but also meets higher quality standards.\"', '\"In comparison to single-agent models, the multi-agent approach significantly reduced code generation time while simultaneously increasing the quality of the produced code as verified by test agents.\"', '\"The collaboration between agents in our multi-agent system fosters an environment where code generation is not only faster but also more robust, as evidenced by higher success rates in code validation tasks compared to single-agent baseline systems.\"]}]}The results indicate that efficiency in code generation can be substantially improved.\"'])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Does this paper: \n",
    "{pdf_contents}\\n\\n\n",
    "presents evidence for the following statement:\n",
    "\"The use of a multi-agent system, with roles for code generation, test design, and test execution can lead to improved efficiency and effectiveness in code generation tasks compared to single-agent models.\"\n",
    "\"\"\"\n",
    "\n",
    "output = get_response(prompt)\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, this paper supports the statement by discussing how a multi-agent system with distinct roles for code generation, test design, and test execution improves both the efficiency and effectiveness of code generation tasks.'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"Our experiments demonstrate that dividing tasks among specialized agents leads to more efficient workflows, as the code generation agent can focus solely on producing code without being burdened by testing tasks.\"',\n",
       " '\"By allocating test design and execution to dedicated agents, the system capitalizes on their respective strengths, ensuring that code is not only generated quickly but also meets higher quality standards.\"',\n",
       " '\"In comparison to single-agent models, the multi-agent approach significantly reduced code generation time while simultaneously increasing the quality of the produced code as verified by test agents.\"',\n",
       " '\"The collaboration between agents in our multi-agent system fosters an environment where code generation is not only faster but also more robust, as evidenced by higher success rates in code validation tasks compared to single-agent baseline systems.\"]}]}The results indicate that efficiency in code generation can be substantially improved.\"']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Yes this paper validates this idea by discussing the advantages of multi-agent systems over single-agent models."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "\n",
    "Markdown(output.argument)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The implementation of a multi-agent system allows for distinct roles such as code generation, test design, and test execution, enabling parallel processing and specialization, which significantly enhances overall efficiency.\"\n",
      "\"Research indicates that multi-agent systems outperform single-agent models in collaborative environments, particularly in tasks requiring simultaneous processing of multiple aspects of software development.\"\n",
      "\"By distributing responsibilities among multiple agents, the system improves the effectiveness of testing and code generation, as agents can focus on their specific tasks without the bottleneck associated with single-agent approaches.\"\n"
     ]
    }
   ],
   "source": [
    "for evidence in output.evidence:\n",
    "    print(evidence)\n",
    "    # if evidence in pdf_contents:\n",
    "    #     print(evidence)\n",
    "    # else:\n",
    "    #     print(\"The quote was not found in the paper\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOT WORKING YET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "augment-research-workflows",
   "language": "python",
   "name": "augment-research-workflows"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
